[
    {
        "cluster_id": 63,
        "text": "arXiv:2501.09223v2  [cs.CL]  15 Jun 2025\nFoundations of\nLarge Language Models\nTong Xiao and Jingbo Zhu\nJune 17, 2025\nNLP Lab, Northeastern University & NiuTrans Research\nThis book is a selection of chapters from an introductory NLP resource\navailable at https://github.com/NiuTrans/NLPBook\nCopyright © 2021-2025 Tong Xiao and Jingbo Zhu\nNATURAL LANGUAGE PROCESSING LAB, NORTHEASTERN UNIVERSITY\n&\nNIUTRANS RESEARCH\nLicensed under the Creative Commons Attribution-NonCommercial 4.0 Unported License (the\n“License”). You may not use this file except in compliance with the License. You may ob-\ntain a copy of the License at http://creativecommons.org/licenses/by-nc/4.0. Unless\nrequired by applicable law or agreed to in writing, software distributed under the License is dis-\ntributed on an “ AS IS ” BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND , either\nexpress or implied. See the License for the specific language governing permissions and limita-\ntions under the License.\nJune 17, 2025\nPreface\nLarge language models originated from natural language processing, but they have undoubtedly\nbecome one of the most revolutionary technological advancements in the field of artificial intelli-\ngence in recent years. An important insight brought by large language models is that knowledge\nof the world and languages can be acquired through large-scale language modeling tasks, and\nin this way, we can create a universal model that handles diverse problems. This discovery has\nprofoundly impacted the research methodologies in natural language processing and many related\ndisciplines. We have shifted from training specialized systems from scratch using a large amount\nof labeled data to a new paradigm of using large-scale pre-training to obtain foundation models,\nwhich are then fine-tuned, aligned, and prompted.\nThis book aims to outline the basic concepts of large language models and introduce the\nrelated techniques. As the title suggests, the book focuses more on the foundational aspects of\nlarge language models rather than providing comprehensive coverage of all cutting-edge methods.\n• Chapter 2 introduces generative models, which are the large language models we commonly\nrefer to today. After presenting the basic process of building these models, we will also\nexplore how to scale up model training and handle long texts.\n• Chapter 3 introduces prompting methods for large language models. We will discuss var-\nious prompting strategies, along with more advanced methods such as chain-of-thought\nreasoning and automatic prompt design.\nNote that this chapter is mostly introductory and cannot cover all aspects of pre-training. For\nexample, there are many methods to fine-tune a pre-trained model, offering different ways to better\nadapt the model to diverse situations. Moreover, large language models, which are considered one\nof the most significant achievements in AI in recent years, are skipped in this section. We leave\nthe discussion of these topics to the following chapters.\nThe concept of language modeling or probabilistic language modeling dates back to early ex-\nperiments conducted by Shannon [1951]. In his work, a language model was designed to estimate\nthe predictability of English — how well can the next letter of a text be predicted when the pre-\nceding N letters are known. Although Shannon’s experiments were preliminary, the fundamental\ngoals and methods of language modeling have remained largely unchanged over the decades since\nthen. For quite a long period, particularly before 2010, the dominant approach to language mod-\neling was the n-gram approach [Jurafsky and Martin, 2008]. In n-gram language modeling, we\nestimate the probability of a word given its preceding n−1 words, and thus the probability of a\nsequence can be approximated by the product of a series of n-gram probabilities. These proba-\nbilities are typically estimated by collecting smoothed relative counts of n-grams in text. While\nsuch an approach is straightforward and simple, it has been extensively used in NLP. For example,\nthe success of modern statistical speech recognition and machine translation systems has largely\ndepended on the utilization of n-gram language models [Jelinek, 1998; Koehn, 2010].\nIndeed, training language models on large-scale data has led NLP research to exciting times.\nsurprisingly, better results were continuously yielded as language models were evolved into more\ncomputationally intensive models and trained on larger datasets [Kaplan et al., 2020]. These\nsuccesses have led NLP researchers to continue increasing both the training data and model size\nin order to build more powerful language models.\n[Almazrouei et al., 2023] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cap-\npelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin\nMalartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon\nseries of open language models. arXiv preprint arXiv:2311.16867, 2023.\nNayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault Févry, Zaid Alyafeai, Manan Dey,\nAndrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan\nFries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang,\nDragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated de-\nvelopment environment and repository for natural language prompts. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations, pages 93–104, 2022.\n[Besta et al., 2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten\nHoefler. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 38, pages 17682–17690, 2024.\n[Chang et al., 2024] Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, and\nJingbo Zhu. Efficient prompting methods for large language models: A survey. arXiv preprint\narXiv:2404.01077, 2024.\n[Chen et al., 2023] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. Unleashing\nthe potential of prompt engineering in large language models: a comprehensive review. arXiv preprint\narXiv:2310.14735, 2023a.\n[Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending\ncontext window of large language models via positional interpolation.arXiv preprint arXiv:2306.15595,\n2023c.\n[Chevalier et al., 2023] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting\nlanguage models to compress contexts. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 3829–3846, 2023.\n[Cui et al., 2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan\nNi, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. ULTRAFEEDBACK:\n5.4 Summary 251\nBoosting language models with scaled AI feedback. InProceedings of the 41st International Conference\non Machine Learning, volume 235, pages 9722–9744, 2024.\n[Deletang et al., 2024] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim\nGenewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent\nOrseau, Marcus Hutter, and Joel Veness. Language modeling is compression. In The Twelfth Interna-\ntional Conference on Learning Representations, 2024.\n5.4 Summary 253\n[Gao et al., 2023] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie\nCallan, and Graham Neubig. Pal: Program-aided language models. In International Conference on\nMachine Learning, pages 10764–10799. PMLR, 2023b.\n[Guo et al., 2024] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\nprompt optimizers. In The Twelfth International Conference on Learning Representations, 2024.\n[Hoffmann et al., 2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Si-\nmon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training\ncompute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n[Hu et al., 2022] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International\nConference on Learning Representations, 2022.\n[Jiang et al., 2023] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua:\nCompressing prompts for accelerated inference of large language models. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing, pages 13358–13376, 2023b.\n[Jiang et al., 2020] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what\nlanguage models know? Transactions of the Association for Computational Linguistics , 8:423–438,\n2020.\n[Khandelwal et al., 2020] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike\nLewis. Generalization through memorization: Nearest neighbor language models. In International\nConference on Learning Representations, 2020.\n[Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. Large language models are zero-shot reasoners. Advances in neural information processing\nsystems, 35:22199–22213, 2022.\n258 Inference\n[Liu et al., 2023] Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang Dong, Peng Di, Wenhai\nWang, and Dongxia Wang. Prompting frameworks for large language models: A survey. arXiv preprint\narXiv:2311.12785, 2023c.\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation\nof word representations in vector space. In Proceedings of the International Conference on Learning\nRepresentations (ICLR 2013), 2013a.\n[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Dis-\ntributed representations of words and phrases and their compositionality. In Proceedings of the 26th In-\nternational Conference on Neural Information Processing Systems - Volume 2, pages 3111–3119, 2013b.\n[Minaee et al., 2024] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard\nSocher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint\narXiv:2402.06196, 2024.\n[Narayanan et al., 2021] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,\nMostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan\nCatanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on\ngpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, pages 1–15, 2021.\n[Ouyang et al., 2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan\nLeike, and Ryan Lowe. Training language models to follow instructions with human feedback.Advances\nin Neural Information Processing Systems, 35:27730–27744, 2022.\n[Peng et al., 2024] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient con-\ntext window extension of large language models. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[Pennington et al., 2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global\nvectors for word representation. In Proceedings of Empirical Methods in Natural Language Processing\n(EMNLP), pages 1532–1543, 2014.\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the\n2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers), 2018.\n[Prasad et al., 2023] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-\nbased instruction search for prompting large language models. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Computational Linguistics, pages 3845–3864, 2023.\nMeasuring and narrowing the compositionality gap in language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages 5687–5711, 2023.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8), 2019.\n262 Inference\n[Schick et al., 2023] Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izac-\nard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. PEER: A collaborative\nlanguage model. In Proceedings of The Eleventh International Conference on Learning Representations,\n2023.\n[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. arXiv preprint arXiv:1909.08053, 2019.\nGemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023a.\nRationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022a.\n[Wei et al., 2022] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proceedings\nof International Conference on Learning Representations, 2022a.\n[Wei et al., 2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models.arXiv\npreprint arXiv:2206.07682, 2022b.\n[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,\nEd H. Chi, Quoc V . Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. Advances in Neural Information Processing Systems, 35:24824–24837, 2022c.\n[Wu et al., 2023] Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu, Yuanhang Sun,\nGang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models.\n[Wu et al., 2024] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan for future\ntokens? arXiv preprint arXiv:2404.00859, 2024.\n[Wu et al., 2023] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu,\nNoah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. In Thirty-seventh Conference on Neural Information Processing\nSystems, 2023b.\n[Xu et al., 2024] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,\nQingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow\ncomplex instructions. In The Twelfth International Conference on Learning Representations, 2024.\n[Zhang et al., 2023] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought\nprompting in large language models. In The Eleventh International Conference on Learning Represen-\ntations, 2023b.\nA survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\n[Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc V . Le, and Ed H. Chi. Least-to-most prompting enables\ncomplex reasoning in large language models. In Proceedings of The Eleventh International Conference\non Learning Representations, 2023b.\n[Zhou et al., 2023] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris\nChan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh\nInternational Conference on Learning Representations, 2023c"
    },
    {
        "cluster_id": 65,
        "text": "ii\nNotation\na variable\na row vector or matrix\nf(a) function of a\nmax f(a) maximum value of f(a)\narg maxaf(a) value of athat maximizes f(a)\nx input token sequence to a model\nxj input token at position j\ny output token sequence produced by a model\nyi output token at position i\nθ model parameters\nPr(a) probability of a\nPr(a|b) conditional probability of agiven b\nPr(·|b) probability distribution of a variable given b\nPrθ(a) probability of aas parameterized by θ\nht hidden state at time step tin sequential models\nH matrix of all hidden states over time in a sequence\nQ, K, V query, key, and value matrices in attention mechanisms\nSoftmax(A) Softmax function that normalizes the input vector or matrix A\nL loss function\nD dataset used for training or fine-tuning a model\n∂L\n∂θ gradient of the loss function Lwith respect to the parameters θ\nKL(p||q) KL divergence between distributions pand q\niii\nContents\n1 Pre-training 1\n1.1 Pre-training NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-training . . . . . . . . 2\n1.1.2 Adapting Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Self-supervised Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2.1 Decoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2.2 Encoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2.3 Encoder-Decoder Pre-training . . . . . . . . . . . . . . . . . . . . . . . 15\n1.2.4 Comparison of Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . 20\n1.3 Example: BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.3.1 The Standard Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.3.2 More Training and Larger Models . . . . . . . . . . . . . . . . . . . . . 27\n1.3.3 More Efficient Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1.3.4 Multi-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.4 Applying BERT Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2 Generative Models 36\n2.1 A Brief Introduction to LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2.1.1 Decoder-only Transformers . . . . . . . . . . . . . . . . . . . . . . . . 38\n2.1.2 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n2.1.3 Fine-tuning LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n2.1.4 Aligning LLMs with the World . . . . . . . . . . . . . . . . . . . . . . 46\n2.1.5 Prompting LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n2.2 Training at Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.2.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n2.2.2 Model Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n2.2.3 Distributed Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n2.2.4 Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n2.3 Long Sequence Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n2.3.1 Optimization from HPC Perspectives . . . . . . . . . . . . . . . . . . . 67\n2.3.2 Efficient Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n2.3.3 Cache and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n2.3.4 Sharing across Heads and Layers . . . . . . . . . . . . . . . . . . . . . 79\niv\nv\n2.3.5 Position Extrapolation and Interpolation . . . . . . . . . . . . . . . . . . 81\n2.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n3 Prompting 96\n3.1 General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.1.2 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n3.1.3 Prompt Engineering Strategies . . . . . . . . . . . . . . . . . . . . . . . 101\n3.1.4 More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n3.2 Advanced Prompting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n3.2.1 Chain of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n3.2.2 Problem Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n3.2.3 Self-refinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n3.2.4 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n3.2.5 RAG and Tool Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n3.3 Learning to Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n3.3.1 Prompt Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n3.3.2 Soft Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n3.3.3 Prompt Length Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 152\n3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n4 Alignment 155\n4.1 An Overview of LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n4.2 Instruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.1 Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n4.2.2 Fine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . 161\n4.2.3 Fine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . . 166\n4.2.4 Instruction Generalization . . . . . . . . . . . . . . . . . . . . . . . . . 167\n4.2.5 Using Weak Models to Improve Strong Models . . . . . . . . . . . . . . 169\n4.3 Human Preference Alignment: RLHF . . . . . . . . . . . . . . . . . . . . . . . 172\n4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173\n4.3.2 Training Reward Models . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n4.3.3 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n4.4 Improved Human Preference Alignment . . . . . . . . . . . . . . . . . . . . . . 187\n4.4.1 Better Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . 187\nvi Notation\n4.4.2 Direct Preference Optimization . . . . . . . . . . . . . . . . . . . . . . 193\n4.4.3 Automatic Preference Data Generation . . . . . . . . . . . . . . . . . . 196\n4.4.4 Step-by-step Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n4.4.5 Inference-time Alignment . . . . . . . . . . . . . . . . . . . . . . . . . 200\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n5 Inference 203\n5.1 Prefilling and Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n5.1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n5.1.2 A Two-phase Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n5.1.3 Decoding Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n5.1.4 Evaluation Metrics for LLM Inference . . . . . . . . . . . . . . . . . . . 221\n5.2 Efficient Inference Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n5.2.1 More Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n5.2.2 Batching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n5.2.3 Parallelization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n5.2.4 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n5.3 Inference-time Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n5.3.1 Context Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n5.3.2 Search Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236\n5.3.3 Output Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n5.3.4 Generating and Verifying Thinking Paths . . . . . . . . . . . . . . . . . 238\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\nBibliography 247\nhttps://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nCHAPTER 1\nPre-training\nThe development of neural sequence models, such as Transformers [Vaswani et al., 2017], along\nwith the improvements in large-scale self-supervised learning, has opened the door to universal\nlanguage understanding and generation. This achievement is largely motivated by pre-training:\nwe separate common components from many neural network-based systems, and then train them\non huge amounts of unlabeled data using self-supervision. These pre-trained models serve as\nfoundation models that can be easily adapted to different tasks via fine-tuning or prompting. As a\nresult, the paradigm of NLP has been enormously changed. In many cases, large-scale supervised\nlearning for specific tasks is no longer required, and instead, we only need to adapt pre-trained\nfoundation models.\nWhile pre-training has gained popularity in recent NLP research, this concept dates back\ndecades to the early days of deep learning. For example, early attempts to pre-train deep learning\nsystems include unsupervised learning for RNNs, deep feedforward networks, autoencoders, and\nothers [Schmidhuber, 2015]. In the modern era of deep learning, we experienced a resurgence of\npre-training, caused in part by the large-scale unsupervised learning of various word embedding\nmodels [Mikolov et al., 2013b; Pennington et al., 2014]. During the same period, pre-training\nalso attracted significant interest in computer vision, where the backbone models were trained on\nrelatively large labeled datasets such as ImageNet, and then applied to different downstream tasks\n[He et al., 2019; Zoph et al., 2020]. Large-scale research on pre-training in NLP began with the\ndevelopment of language models using self-supervised learning. This family of models covers\nseveral well-known examples like BERT [Devlin et al., 2019] and GPT [Brown et al., 2020], all\nwith a similar idea that general language understanding and generation can be achieved by train-\ning the models to predict masked words in a huge amount of text. Despite the simple nature of\nthis approach, the resulting models show remarkable capability in modeling linguistic structure,\nthough they are not explicitly trained to achieve this. The generality of the pre-training tasks\nleads to systems that exhibit strong performance in a large variety of NLP problems, even outper-\nforming previously well-developed supervised systems. More recently, pre-trained large language\nmodels have achieved greater success, showing the exciting prospects for more general artificial\nintelligence [Bubeck et al., 2023].\nThis chapter discusses the concept of pre-training in the context of NLP. It begins with a gen-\neral introduction to pre-training methods and their applications. BERT is then used as an example\nto illustrate how a sequence model is trained via a self-supervised task, called masked language\nmodeling. This is followed by a discussion of methods for adapting pre-trained sequence mod-\nels for various NLP tasks. Note that in this chapter, we will focus primarily on the pre-training\nparadigm in NLP, and therefore, we do not intend to cover details about generative large language\nmodels. A detailed discussion of these models will be left to subsequent chapters.\n1.1 Pre-training NLP Models\nThe discussion of pre-training issues in NLP typically involves two types of problems: sequence\nmodeling (or sequence encoding) and sequence generation. While these problems have different\n2 Pre-training\nforms, for simplicity, we describe them using a single model defined as follows:\no = g(x0,x1,...,x m; θ)\n= gθ(x0,x1,...,x m) (1.1)\nwhere {x0,x1,...,x m}denotes a sequence of input tokens 1, x0 denotes a special symbol ( ⟨s⟩or\n[CLS]) attached to the beginning of a sequence, g(·; θ) (also written as gθ(·)) denotes a neural\nnetwork with parameters θ, and o denotes the output of the neural network. Different problems\ncan vary based on the form of the output o. For example, in token prediction problems (as in\nlanguage modeling), o is a distribution over a vocabulary; in sequence encoding problems, o is a\nrepresentation of the input sequence, often expressed as a real-valued vector sequence.\nA third approach to pre-training is self-supervised learning. In this approach, a neural net-\nwork is trained using the supervision signals generated by itself, rather than those provided by\nhumans. This is generally done by constructing its own training tasks directly from unlabeled\ndata, such as having the system create pseudo labels. While self-supervised learning has recently\nemerged as a very popular method in NLP, it is not a new concept. In machine learning, a related\nconcept is self-training where a model is iteratively improved by learning from the pseudo labels\nassigned to a dataset. To do this, we need some seed data to build an initial model. This model\nthen generates pseudo labels for unlabeled data, and these pseudo labels are subsequently used to\niteratively refine and bootstrap the model itself. Such a method has been successfully used in sev-\neral NLP areas, such as word sense disambiguation [Yarowsky, 1995] and document classification\n[Blum and Mitchell, 1998]. Unlike the standard self-training method, self-supervised pre-training\nin NLP does not rely on an initial model for annotating the data. Instead, all the supervision sig-\nnals are created from the text, and the entire model is trained from scratch. A well-known example\nof this is training sequence models by successively predicting a masked word given its preceding\nor surrounding words in a text. This enables large-scale self-supervised learning for deep neural\nnetworks, leading to the success of pre-training in many understanding, writing, and reasoning\ntasks.\nFigure 1.1 shows a comparison of the above three pre-training approaches. Self-supervised\npre-training is so successful that most current state-of-the-art NLP models are based on this\nparadigm. Therefore, in this chapter and throughout this book, we will focus on self-supervised\npre-training. We will show how sequence models are pre-trained via self-supervision and how the\npre-trained models are applied.\n1.2 Self-supervised Pre-training Tasks\nIn this section, we consider self-supervised pre-training approaches for different neural architec-\ntures, including decoder-only, encoder-only, and encoder-decoder architectures. We restrict our\ndiscussion to Transformers since they form the basis of most pre-trained models in NLP. How-\never, pre-training is a broad concept, and so we just give a brief introduction to basic approaches\nin order to make this section concise.\n1.2.1 Decoder-only Pre-training\nThe decoder-only architecture has been widely used in developing language models [Radford\net al., 2018]. For example, we can use a Transformer decoder as a language model by simply\nremoving cross-attention sub-layers from it. Such a model predicts the distribution of tokens at\na position given its preceding tokens, and the output is the token with the maximum probability.\n1.2.2.1 Masked Language Modeling\nOne of the most popular methods of encoder pre-training is masked language modeling, which\nforms the basis of the well-known BERT model [Devlin et al., 2019]. The idea of masked lan-\nguage modeling is to create prediction challenges by masking out some of the tokens in the input\nsequence and training a model to predict the masked tokens. In this sense, the conventional lan-\nguage modeling problem, which is sometimes called causal language modeling, is a special case\nof masked language modeling: at each position, we mask the tokens in the right-context, and\npredict the token at this position using its left-context. However, in causal language modeling we\nonly make use of the left-context in word prediction, while the prediction may depend on tokens\nin the right-context. By contrast, in masked language modeling, all the unmasked tokens are used\nfor word prediction, leading to a bidirectional model that makes predictions based on both left and\nright-contexts.\n1.2.2.2 Permuted Language Modeling\nWhile masked language modeling is simple and widely applied, it introduces new issues. One\ndrawback is the use of a special token, [MASK], which is employed only during training but not\n1.2 Self-supervised Pre-training Tasks 11\nat test time. This leads to a discrepancy between training and inference. Moreover, the auto-\nencoding process overlooks the dependencies between masked tokens. For example, in the above\nexample, the prediction of x2 (i.e., the first masked token) is made independently of x6 (i.e., the\nsecond masked token), though x6 should be considered in the context of x2.\n1.2 Self-supervised Pre-training Tasks 15\n1.2.3 Encoder-Decoder Pre-training\nIn NLP, encoder-decoder architectures are often used to model sequence-to-sequence problems,\nsuch as machine translation and question answering. In addition to these typical sequence-to-\nsequence problems in NLP, encoder-decoder models can be extended to deal with many other\nproblems. A simple idea is to consider text as both the input and output of a problem, and so\nwe can directly apply encoder-decoder models. For example, given a text, we can ask a model to\noutput a text describing the sentiment of the input text, such as positive, negative, and neutral.\nThere have been several powerful methods of self-supervised learning for either Transformer\nencoders or decoders. Applying these methods to pre-train encoder-decoder models is relatively\nstraightforward. One common choice is to train encoder-decoder models as language models. For\nexample, the encoder receives a sequence prefix, while the decoder generates the remaining se-\nquence. However, this differs from standard causal language modeling, where the entire sequence\nis autoregressively generated from the first token. In our case, the encoder processes the prefix at\nonce, and then the decoder predicts subsequent tokens in the manner of causal language modeling.\nA second approach to pre-training encoder-decoder models is masked language modeling. In\nthis approach, as discussed in Section 1.2.2, tokens in a sequence are randomly replaced with a\nmask symbol, and the model is then trained to predict these masked tokens based on the entire\nmasked sequence.\n1.2.4 Comparison of Pre-training Tasks\nSo far, we have discussed a number of pre-training tasks. Since the same training objective can\napply to different architectures (e.g., using masked language modeling for both encoder-only and\nencoder-decoder pre-training), categorizing pre-training tasks based solely on model architecture\ndoes not seem ideal. Instead, we summarize these tasks based on the training objectives.\nLet A(x) be the set of selected positions of a given token sequence x, and ¯x be the modified\nsequence of x. The loss function of masked language modeling can be defined as\nLossMLM = −\n∑\ni∈A(x)\nlog Pri(xi|¯x) (1.18)\nwhere Pri(xi|¯x) is the probability of predicting xi at the position igiven ¯x. Figure 1.5 shows a\nrunning example of computing LossMLM.\nAn improvement to multi-lingual pre-trained models like mBERT is to introduce bilingual data\ninto pre-training. Rather than training solely on monolingual data from multiple languages, bilin-\ngual training explicitly models the relationship between tokens in two languages. The resulting\nmodel will have innate cross-lingual transfer abilities, and thus can be easily adapted to different\nlanguages. Lample and Conneau [2019] propose an approach to pre-training cross-lingual lan-\nguage models (XLMs). In their work, a cross-lingual language model can be trained in either the\ncausal language modeling or masked language modeling manner. For masked language modeling\n6https://github.com/google-research/bert/\n1.3 Example: BERT 29\npre-training, the model is treated as an encoder. The training objective is the same as BERT: we\nmaximize the probabilities of some randomly selected tokens which are either masked, replaced\nwith random tokens, or kept unchanged in the input. If we consider bilingual data in pre-training,\nwe sample a pair of aligned sentences each time. Then, the two sentences are packed together to\nform a single sequence used for training. For example, consider an English-Chinese sentence pair\n鲸鱼 是 哺乳 动物 。 ↔ Whales are mammals.\nThe result of multi-lingual pre-training is influenced by several factors. Given that the model\narchitecture is fixed, one needs to specify the size of the shared vocabulary, the number (or per-\ncentage) of samples in each language, the size of the model, and so on. Conneau et al. [2020]\npoint out several interesting issues regarding large-scale multi-lingual pre-training for XLM-like\nmodels. First, as the number of supported languages increases, a larger model is needed to handle\nthese languages. Second, a larger shared vocabulary is helpful for modeling the increased diver-\nsity in languages. Third, low-resource languages more easily benefit from cross-lingual transfer\nfrom high-resource languages, particularly when similar high-resource languages are involved in\npre-training. However, interference may occur if the model is trained for an extended period,\n30 Pre-training\n[CLS] [MASK] 是 [MASK] 动物 。 [SEP] Whales [MASK] [MASK] . [SEP]\n(zh) (zh) (zh) (zh) (zh) (zh) (zh) (en) (en) (en) (en) (en)\ne0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11\nh0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11\n鲸鱼 哺乳 are mammals\nTransformer Encoder\nFig. 1.7: An illustration of translation language modeling. For ease of understanding, we present a simple example\nwhere all the selected tokens are masked. The model is trained to predict these masked tokens. As the sequence\ncontains tokens in two languages, predicting a token in one language allows access to tokens in the other language,\nthereby enabling cross-lingual modeling. In Lample and Conneau [2019]’s work, an input embedding (i.e., ei) is the\nsum of the token embedding, positional embedding, and language embedding. This requires that each token is assigned\nwith a language label. Thus we can distinguish tokens in different languages. In multi-lingual pre-training, particularly\nin work using shared vocabularies, specifying the language to which a token belongs is not necessary. The use of\nlanguage embeddings in turn makes it difficult to handle code-switching. Therefore, we assume here that all token\nrepresentations are language-independent.\n1.5 Summary\nIn this chapter we have discussed the general idea of pre-training in NLP. In particular, we have dis-\ncussed self-supervised pre-training and its application to encoder-only, decoder-only, and encoder-\ndecoder architectures. Moreover, we have presented and compared a variety of pre-training tasks\nfor these architectures. As an example, BERT is used to illustrate how sequence models are pre-\ntrained via masked language modeling and applied to different downstream tasks.\nPr(x1|x0)\nPr(x2|x0x1)\nPr(xm|x0x1...xm−1)\nx1 x2 ... xm\nLanguage Model\nz0 z1 ... zm−1\nPost-norm or Pre-norm\nPost-norm or Pre-norm\nSelf-attention\nFFN\nLBlocks\nFig. 2.1: The Transformer-decoder architecture for language modeling. The central components are Lstacked Trans-\nformer blocks, each comprising a self-attention sub-layer and an FFN sub-layer. To prevent the model from accessing\nthe right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to\ngenerate a probability distribution for the next token, given the sequence of previous tokens. During inference, the\nmodel takes the previously predicted token to predict the next one, repeating this process until the end of the sequence\nis reached. {z0,..., zm−1}denote the inputs of a Transformer block, and {hL\n0 ,..., hL\nm−1}denote the outputs of the\nlast Transformer block.\n[Lample and Conneau, 2019] Guillaume Lample and Alexis Conneau. Cross-lingual language model\npretraining. arXiv preprint arXiv:1901.07291, 2019"
    },
    {
        "cluster_id": 7,
        "text": "In this section, we discuss the basic ideas in addressing these issues.\nFinally, we give a summary of these discussions.\nThe concept of RAG has been mentioned several times in the previous sections and chapters.\n136 Prompting\nYour task is to answer the following question. To help you with this, relevant\ntexts are provided. Please base your answer on these texts.\nYour task is to answer the following question. To help you with this, relevant\ntexts are provided. Please base your answer on these texts.\nFor more detailed discussions of these topics, interested readers can refer to Chapter 2"
    },
    {
        "cluster_id": 35,
        "text": "1.1.1 Unsupervised, Supervised and Self-supervised Pre-training\nIn deep learning, pre-training refers to the process of optimizing a neural network before it is\nfurther trained/tuned and applied to the tasks of interest. This approach is based on an assumption\nthat a model pre-trained on one task can be adapted to perform another task. As a result, we do\nnot need to train a deep, complex neural network from scratch on tasks with limited labeled data.\nDuring the resurgence of neural networks through deep learning, many early attempts to\nachieve pre-training were focused on unsupervised learning. In these methods, the parame-\nters of a neural network are optimized using a criterion that is not directly related to specific tasks.\nFor example, we can minimize the reconstruction cross-entropy of the input vector for each layer\n[Bengio et al., 2006]. Unsupervised pre-training is commonly employed as a preliminary step\nbefore supervised learning, offering several advantages, such as aiding in the discovery of better\nlocal minima and adding a regularization effect to the training process [Erhan et al., 2010]. These\nbenefits make the subsequent supervised learning phase easier and more stable.\n4 Pre-training\nUnsupervised Supervised\nPre-training Training\nUnlabeled\nData\nLabeled\nData\n(a) Unsupervised Pre-training\nSupervised Supervised\nPre-training Tuning\nLabeled\nData\nTask 1\nLabeled\nData\nTask 2\n(b) Supervised Pre-training\nSelf-\nSupervised Supervised\nZero/Few\nShot Learning\nPre-training Tuning\nPrompting\nUnlabeled\nData\nLabeled\nData\n(c) Self-supervised Pre-training\nFig. 1.1: Illustration of unsupervised, supervised, and self-supervised pre-training. In unsupervised pre-training, the\npre-training is performed on large-scale unlabeled data. It can be viewed as a preliminary step to have a good starting\npoint for the subsequent optimization process, though considerable effort is still required to further train the model\nwith labeled data after pre-training. In supervised pre-training, the underlying assumption is that different (supervised)\nlearning tasks are related. So we can first train the model on one task, and transfer the resulting model to another task\nwith some training or tuning effort. In self-supervised pre-training, a model is pre-trained on large-scale unlabeled data\nvia self-supervision. The model can be well trained in this way, and we can efficiently adapt it to new tasks through\nfine-tuning or prompting.\nAs we enter the era dominated by deep learning methods, models based on deep neural net-\nworks have become extremely powerful. Even with very simple search algorithms, these models\ncan achieve excellent results. In this context, inference no longer seems as “important” as it once\nwas, and research attention has gradually shifted toward model architectures, training methods,\nand scaling up models.\n[Erhan et al., 2010] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does\nunsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference\non artificial intelligence and statistics, pages 201–208, 2010.\n[Harlap et al., 2018] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Deva-\nnur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training. arXiv\npreprint arXiv:1806.03377, 2018.\n[He et al., 2019] Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training. In\n254 Inference\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 4918–4927, 2019.\n[Huang et al., 2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient\ntraining of giant neural networks using pipeline parallelism. Advances in neural information processing\nsystems, 32, 2019"
    },
    {
        "cluster_id": 11,
        "text": "Assume that the polarity of a text is a label chosen from {positive, negative,\nneutral}. Identify the polarity of the input.\nPolarity: Negative\nInput: The weather here is wonderful.\nGiven the following text snippets, classify their sentiment as Positive, Negative,\nor Neutral.\n3.1.4.1 Text Classification\nText classification is perhaps one of the most common problems in NLP. Many tasks can be\nbroadly categorized as assigning pre-defined labels to a given text. Here we consider the polarity\nclassification problem in sentiment analysis. We choose polarity classification for illustration be-\ncause it is one of the most popular and well-defined text classification tasks. In a general setup of\npolarity classification, we are required to categorize a given text into one of three categories: neg-\native, positive, or neutral. Below is a simple prompt for doing this (for easy reading, we highlight\nthe task description in the prompt).\nAnalyze the polarity of the following text and classify it as positive, negative, or\nneutral.\nThe polarity of the text can be classified as negative.\nAnalyze the polarity of the following text and classify it as positive, negative, or\nneutral.\nThe polarity of the text is negative\n3.1 General Prompt Design 107\nWe can use LLMs to complete the text and fill the blank with the most appropriate word. Ide-\nally, we wish the filled word would be positive, negative, or neutral. However, LLMs are not\nguaranteed to generate these label words. One method to address this problem is to constrain the\nprediction to the set of label words and select the one with the highest probability. Then, the output\nlabel is given by\nlabel = arg max\ny∈Y\nPr(y|x) (3.1)\nwhere ydenotes the word filled in the blank, and Y denotes the set of label words\n{positive,negative,neutral}.\nAnalyze the polarity of the following text and classify it as positive, negative, or\nneutral.\nWhat is the polarity of the text?\nJust answer: positive, negative, or neutral.\nNegative\nSentiment analysis is a common NLP problem that has probably been well understood by\nLLMs through pre-training or fine-tuning. Thus we can prompt LLMs using simple instructions\nto perform the task. However, for new classification problems, it may be necessary to provide\nadditional details about the task, such as the classification standards, so that the LLMs can perform\ncorrectly. To do this, we can add a more detailed description of the task and/or demonstrate\nclassification examples in the prompts. To illustrate, consider the following example.\n108 Prompting\nAnalyze the polarity of the following text and classify it as positive, negative, or\nneutral. Here’s what each category represents:\nPositive: This indicates that the text conveys a positive emotion or attitude. For\nexample, texts expressing happiness, satisfaction, excitement, or admiration are\nconsidered positive.\nNegative: This refers to a text that expresses a negative emotion or attitude. It\nencompasses feelings of sadness, anger, frustration, or criticism.\nNeutral: Neutral sentiment is used to describe texts that do not exhibit clear posi-\ntive or negative emotions but instead convey informational, factual, or indifferent\ntones.\nWhat is the polarity of the text?\nNegative\nWhile it seems straightforward to use LLMs for classification problems, there are still issues\nthat have not been well addressed. For example, when dealing with a large number of categories,\nit remains challenging to effectively prompt LLMs. Note that if we face a very difficult classifica-\ntion problem and have a certain amount of labeled data, fine-tuning LLMs or adopting “BERT +\nclassifier”-like architectures is also desirable"
    },
    {
        "cluster_id": 43,
        "text": "Pr(x) = Pr( x0) ·Pr(x1|x0) ·Pr(x2|x0,x1) ·Pr(x3|x0,x1,x2) ·\nPr(x4|x0,x1,x2,x3)\n= Pr( x0) ·Pr(x1|e0) ·Pr(x2|e0,e1) ·Pr(x3|e0,e1,e2) ·\nPr(x4|e0,e1,e2,e3) (1.13)\nNow, let us consider a different order for token prediction: x0 →x4 →x2 →x1 →x3. The\nsequence generation process can then be expressed as follows:\nPr(x) = Pr( x0) ·Pr(x4|e0) ·Pr(x2|e0,e4) ·Pr(x1|e0,e4,e2) ·\nPr(x3|e0,e4,e2,e1) (1.14)\nThis new prediction order allows for the generation of some tokens to be conditioned on a\nbroader context, rather than being limited to just the preceding tokens as in standard language\nmodels. For example, in generating x3, the model considers both its left-context (i.e., e0,e1,e2)\nand right-context (i.e., e4). The embeddings e0,e1,e2,e4 incorporate the positional information\nof x0,x1,x2,x4, preserving the original order of the tokens. As a result, this approach is somewhat\nakin to masked language modeling: we mask out x3 and use its surrounding tokens x0,x1,x2,x4\nto predict this token.\n38 Generative Models\nContext Predict Decision Rule Sequence Probability\n⟨s⟩a b arg maxx2∈V Pr(x2|⟨s⟩a) Pr(⟨s⟩) ·Pr(a|⟨s⟩)· Pr(b|⟨s⟩a)\n⟨s⟩a b c arg maxx3∈V Pr(x3|⟨s⟩ab) Pr(⟨s⟩) ·Pr(a|⟨s⟩) ·Pr(b|⟨s⟩a)·\nPr(c|⟨s⟩ab)\n⟨s⟩a b c d arg maxx4∈V Pr(x4|⟨s⟩abc ) Pr(⟨s⟩) ·Pr(a|⟨s⟩) ·Pr(b|⟨s⟩a)·\nPr(c|⟨s⟩ab)· Pr(d|⟨s⟩abc )\nTable 2.1: Illustration of generating the three tokens b c dgiven the prefix ⟨s⟩ avia a language model. In each step,\nthe model picks a token xi from V so that Pr(xi|x0,...,x i−1) is maximized. This token is then appended to the end\nof the context sequence. In the next step, we repeat the same process, but based on the new context.\nWhen applying a trained language model, a common task is to find the most likely token given\nits previous context tokens. This token prediction task can be described as\nˆxi = arg max\nxi∈V\nPr(xi|x0,...,x i−1) (2.3)\nWe can perform word prediction multiple times to generate a continuous text: each time we\npredict the best token ˆxi, and then add this predicted token to the context for predicting the next\ntoken ˆxi+1. This results in a left-to-right generation process implementing Eqs. (2.1) and (2.2). To\nillustrate, consider the generation of the following three words given the prefix ‘⟨s⟩ a’, as shown\nin Table 2.1. Now we discuss how LLMs are constructed, trained, and applied.\nmodel follows an autoregressive process. Each time the language model takes a token xi−1 as\ninput and predicts a token xi that maximizes the probability Pr(xi|x0,...,x i−1). It is important\nto note that, despite different implementation details, many LLMs share the same architecture\ndescribed above. These models are called large because both their depth and width are significant.\nMore formally, let x = x0...xm denote a token sequence of context given by users, and\ny = y1...yn denote a token sequence following the context. Then, the inference of LLMs can be\ndefined as a problem of finding the most likely sequence y based on x:\nˆy = arg max\ny\nlog Pr(y|x)\n= arg max\ny\nn∑\ni=1\nlog Pr(yi|x0,...,x m,y1,...,y i−1) (2.15)\nHere ∑n\ni=1 log Pr(yi|x0,...,x m,y1,...,y i−1) essentially expresses the same thing as the right-\nhand side of Eq. (2.2). It models the log probability of predicting tokens from position m+ 1,\nrather than position 0. Throughout this chapter and subsequent ones, we will employ separate\nvariables x and y to distinguish the input and output of an LLM, though they can be seen as sub-\nsequences from the same sequence. By adopting such notation, we see that the form of the above\nequation closely resembles those used in other text generation models in NLP, such as neural\nmachine translation models.\n[2023]’s work. In this method, the draft model is a small language model, denoted byPrq(yi|x,y<i),\nwhile the verification model is a normal LLM, denoted byPrp(yi|x,y<i). The goal is that, given a\nprefix, we use the draft model to autoregressively predict up toτ tokens. The verification model is\nthen employed to generate the last token at the point where errors begin to occur in the speculative\npredictions. Figure 5.7 illustrates one step in this decoding process.\n• Given na, we keep the speculated tokens {ˆyi+1,..., ˆyi+na}. We then use the verification\nmodel to make a new prediction at i+ na + 1\n¯yi+na+1 = arg max\nyi+ns+1\nPrp(yi+ns+1|x,y≤i,ˆyi+1...ˆyi+ns) (5.36)\n220 Inference\nContext (x,y<i) ˆyi+1 ˆyi+2 ˆyi+3 ˆyi+4 ˆyi+5\nDraft Model Prq(·)\npredict\n(a) Predict the next τ tokens given the context using the draft model (τ = 5)\nContext (x,y<i) ˆyi+1 ˆyi+2 ˆyi+3 ˆyi+4 ˆyi+5\nDraft Model Prq(·)\nEvaluation Model Prp(·)\nevaluate\n(b) Evaluate the predicted tokens using the evaluation model\nContext (x,y<i) ˆyi+1 ˆyi+2 ˆyi+3 ˆyi+4 ˆyi+5\nDraft Model Prq(·)\nEvaluation Model Prp(·)\naccepted rejected\n(c) Determine the number of accepted tokens\nContext (x,y<i) ˆyi+1 ˆyi+2 ˆyi+3 ¯yi+4\nDraft Model Prq(·)\nEvaluation Model Prp(·)\n(d) Predict a new token following the accepted tokens using the evaluation model\nFig. 5.7: Illustration of one step of speculative decoding. The goal is to predict as many next tokens as possible using\nthe draft model. There are four sub-steps. Given the context, we first use the draft model to predict the next τ tokens.\n• Above, we have described one step of speculative decoding. The result sequence (including\nboth the context and predicted tokens) is illustrated as follows\n5.1 Prefilling and Decoding 221\n[x, y<i] ˆyi+1...ˆyi+na ¯yi+na+1\nContext na tokens\npredicted using\nthe draft model\nOne token\npredicted using\nthe verification model\nOnce we have finished this step, we add the predicted tokens {ˆyi+1,..., ˆyi+na,¯yi+na+1}to\nthe context, and repeat the above process"
    },
    {
        "cluster_id": 47,
        "text": "[CLS] The puppies are[MASK] outside [MASK] house.\n→ ⟨s⟩The puppies are frolicking outside the house.\n[CLS] The puppies are[MASK] outside [MASK] [MASK].\n→ ⟨s⟩The puppies are frolicking outside the house.\n→ ⟨s⟩[X] frolicking [Y] the house[Z]\n18 Pre-training\n[CLS] The puppies are [M] in [M] house.\nOriginal (x): The puppies are frolicking outside the house.\nToken Masking (xnoise): The puppies are [MASK] outside [MASK] house.\nSpan Masking (xnoise): The [MASK] puppies are [MASK] house"
    },
    {
        "cluster_id": 41,
        "text": "1.2.3.2 Denoising Training\nIf we view the problem of training encoder-decoder models as a problem of training denoising\nautoencoders, there will typically be many different methods for introducing input corruption and\nreconstructing the input. For instance, beyond randomly masking tokens, we can also alter some\nof them or rearrange their order.\nFor pre-training, we can apply multiple corruption methods to learn robust models, for ex-\nample, we randomly choose one of them for each training sample. In practice, the outcome of\nencoder-decoder pre-training depends heavily on the input corruption methods used, and so we\ntypically need to choose appropriate training objectives through careful experimentation.\n• Denoising Autoencoding. This approach is applied to the pre-training of encoder-decoder\nmodels. The input is a corrupted sequence and the encoder-decoder models are trained to\nreconstruct the original sequence"
    },
    {
        "cluster_id": 38,
        "text": "• Token Masking. This is the same masking method that we used in masked language mod-\neling. The tokens in the input sequence are randomly selected and masked.\n• Token Deletion. This method is similar to token masking. However, rather than replacing\nthe selected tokens with a special symbol [MASK], these tokens are removed from the\nsequence. See the following example for a comparison of the token masking and token\ndeletion methods.\nWe can pack them to obtain a sequence, like this\n[CLS] 鲸鱼 是 哺乳 动物 。 [SEP] Whales are mammals . [SEP]\nWe then select a certain percentage of the tokens and replace them with [MASK]"
    },
    {
        "cluster_id": 5,
        "text": "We can reorder the two sentences to have a corrupted input sequence\nSuccess brings happiness . Hard work leads to success.\nInput: There is many reasons to celebrate.\nOutput: There are many reasons to celebrate.\nInput: There is many reasons to celebrate.\nOutput: There are many reasons to celebrate.\nOutput: My friend and I go to the gym every day"
    },
    {
        "cluster_id": 64,
        "text": "1.3.1 The Standard Model\nThe standard BERT model, which is proposed in Devlin et al. [2019]’s work, is a Transformer\nencoder trained using both masked language modeling and next sentence prediction tasks. The\nloss used in training this model is a sum of the loss of the two tasks.\n1.3.1.1 Loss Functions\nIn general, BERT models are used to represent a single sentence or a pair of sentences, and thus\ncan handle various downstream language understanding problems. In this section we assume that\nthe input representation is a sequence containing two sentences SentA and SentB, expressed as\n[CLS] Sent A [SEP] Sent B [SEP]\nHere we follow the notation in BERT’s paper and use[SEP] to denote the separator.\n1.3.2 More Training and Larger Models\nBERT is a milestone model in NLP, sparking many subsequent efforts to improve it. One direction\nis to scale up the model itself, including increasing training data and developing larger models.\nRoBERTa, an extension of the standard BERT model, is an example of such efforts [Liu et al.,\n2019]. It introduces two major improvements. First, simply using more training data and more\ncompute can improve BERT models without need of changing the model architectures. Second,\nremoving the NSP loss does not decrease the performance on downstream tasks if the training is\nscaled up. These findings suggest exploring a general direction of pre-training: we can continue\nto improve pre-training by scaling it up on simple pre-training tasks.\nA second approach to improving BERT models is to increase the number of model parame-\nters. For example, in He et al. [2021]’s work, a 1.5 billion-parameter BERT-like model is built by\nincreasing both the model depth and hidden size. However, scaling up BERT and various other\npre-trained models introduces new challenges in training, for example, training very large models\noften becomes unstable and difficult to converge. This makes the problem more complicated, and\nrequires careful consideration of various aspects, including model architecture, parallel computa-\ntion, parameter initialization, and so on. In another example, Shoeybi et al. [2019] successfully\ntrained a 3.9 billion-parameter BERT-like model, where hundreds of GPUs were used to manage\nthe increased computational demands.\nSeveral threads of research are of interest to NLP researchers in developing efficient BERT\nmodels. First, work on knowledge distillation, such as training student models with the output\nof well-trained teacher models, shows that smaller BERT models can be obtained by transferring\nknowledge from larger BERT models. Given that BERT models are multi-layer networks with\nseveral different types of layers, knowledge distillation can be applied at different levels of repre-\nsentation. For example, beyond distilling knowledge from the output layers, it is also possible to\nincorporate training loss that measures the difference in output of hidden layers between teacher\nmodels and student models [Sun et al., 2020; Jiao et al., 2020]. Indeed, knowledge distillation has\nbeen one of the most widely-used techniques for learning small pre-trained models.\n[2019] show that removing some of the heads does not significantly decrease the performance of\nBERT models, but speeds up the inference of these models. Another approach to compressing\nBERT models is quantization [Shen et al., 2020]. By representing model parameters as low-\nprecision numbers, the models can be greatly compressed. While this method is not specific to\nBERT models, it proves effective for large Transformer-based architectures.\n1.4 Applying BERT Models\nOnce a BERT model is pre-trained, it can then be used to solve NLP problems. But BERT models\nare not immediately ready for performing specific downstream tasks. In general, additional fine-\ntuning work is required to make them adapt. As a first step, we need a predictor to align the\noutput of the model with the problem of interest. Let BERTˆθ(·) be a BERT model with pre-\ntrained parameters ˆθ, and Predictω(·) be a prediction network with parameters ω. By integrating\nthe prediction network with the output of the BERT model, we develop a model to tackle the\ndownstream tasks. This model can be expressed as\ny = Predict ω(BERTˆθ(x)) (1.21)\nwhere x is the input and y is the output that fits the problem. For example, in classification\nproblems, the model outputs a probability distribution over labels.\nWith the fine-tuned parameters ˜ωand ˜θ, we can apply the modelPredict˜ω(BERT˜θ(·)) to new\ndata of the same tasks for which the model was fine-tuned. The form of the downstream tasks\ndetermines the input and output formats of the model, as well as the architecture of the prediction\nnetwork. In the following we list some tasks to which BERT models are generally suited.\nThis problem can be framed as a sequence labeling-like task in which we predict a label for\neach yj to indicate the beginning or ending of the span. Following Seo et al. [2017], we\nadd two networks on top of the BERT output for yj: one for generating the probability of\nyj being the beginning of the span (denoted bypbeg\nj ), and one for generating the probability\nof yj being the ending of the span (denoted by pend\nj ). The resulting model architecture is\nshown as follows\n[CLS] x1 x2 ... xm [SEP] y1 y2 ... yn [SEP]\nQuery Context Text\necls e1 e2 ... em em+1 em+2 em+3 ... elen−1 elen\nhcls h1 h2 ... hm hm+1 hm+2 hm+3 ... hlen−1 hlen\n(pbeg\n1 )\nBeg\n(pend\n1 )\nEnd\n(pbeg\n2 )\nBeg\n(pend\n2 )\nEnd\n(pbeg\nn )\nBeg\n(pend\nn )\nEnd\nBERT\n34 Pre-training\nWe pack the query and context text together to obtain the input sequence. The prediction\nnetworks are only applied to outputs for the context text, generating the probabilities pbeg\nj\nand pend\nj at each position. The loss can be computed by summing the negative log likeli-\nhoods of the two models across the entire context text.\nLoss = −1\nn\nn∑\nj=1\n(\nlog pbeg\nj + logpend\nj\n)\n(1.24)\nAt test time, we search for the best span by\n(ˆj1,ˆj2) = arg max\n1≤j1≤j2≤n\n(\nlog pbeg\nj1 + logpend\nj2\n)\n(1.25)\n• Encoding for Encoder-Decoder Models . While our focus in this section has been pri-\nmarily on language understanding problems, it is worth noting that BERT models can be\napplied to a broader range of NLP tasks. In fact, BERT models can be used in all the scenar-\nios where we need to encode a piece of text. One application that we have not mentioned is\ntext generation which includes a range of tasks such as machine translation, summarization,\nquestion answering, and dialogue generation. These tasks can be formulated as sequence-\nto-sequence problems: we use an encoder to represent the source text, and a decoder to\ngenerate the corresponding target text. A straightforward method to apply BERT models\nis to consider them as encoders. Before fine-tuning, we can initialize the parameters of the\nencoder with those from a pre-trained BERT model. Then, the encoder-decoder model can\nbe fine-tuned on pairs of texts as usual. The following shows the architecture of a neural\nmachine translation system where a BERT model is applied on the source side.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[Xin et al., 2020] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early\nexiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2246–2251, 2020"
    },
    {
        "cluster_id": 51,
        "text": "The input is a sequence of embeddings, each being the sum of the token embedding, the positional\nembedding, and the segment embedding.\ne = x + epos + eseg (1.20)\nBoth the token embedding ( x) and positional embedding ( epos) are regular, as in Transformer\nmodels. The segment embedding (eseg) is a new type of embedding that indicates whether a token\nbelongs to SentA or SentB. This can be illustrated by the following example.\nIn this case, the embedding at position ican be expressed as\nei = xi + PE(i) (2.74)\n82 Generative Models\nwhere xi ∈Rd denotes the token embedding, and PE(i) ∈Rd denotes the positional embedding.\n2.3.5.3 Rotary Positional Embedding\nAs with sinusoidal embeddings, rotary positional embeddings are based on hard-coded values for\nall dimensions of an embedding [Su et al., 2024]. Recall that in the sinusoidal embedding model,\npositions are represented as combinations of sine and cosine functions with different frequencies.\nThese embeddings are then added to token embeddings to form the inputs to the Transformer\nlayer stack. Rotary positional embeddings instead model positional context as rotations to token\nembeddings in a complex space. This leads to a model expressed in the form of multiplicative\nembeddings\nei = xiR(i) (2.85)\nwhere R(i) ∈ Rd×d is the rotation matrix representing the rotations performed on the token\nembedding xi ∈Rd.\nThe degree of rotation is usually defined by a specific angle, denoted by θ. The rotation can be\n88 Generative Models\nx1\nx2\nθ\nvector x\nxRθ\nrotated vector\n(a) Single-step Rotation\nx1\nx2\nθ\nθ\nθ\nxxRθ\nxR2θ\nxR3θ\n(b) Multi-step Rotation\nx1\nx2\n7θ\n7θ\nsleeping4\nsleeping11\ncat2\ncat9\nThe1 cat2 is3 sleeping4 peacefully5\nin6 the7 warm8 sunlight9 .10\nEvery1 afternoon2 ,3 you4 ’ll5 find6 that7\nthe8 cat9 is10 sleeping11 on12 my13 bed14 .15\n(c) Angles between embeddings of two tokens at different positions\nFig. 2.12: Illustrations of vector rotations in a plane. Sub-figures (a) and (b) show rotations of a vector in a single\nstep and multiple steps, respectively. Sub-figure (c) shows the embeddings of tokens cat and sleeping in two different\nsentences. We show these sentences with a subscript affixed to each token to indicate its position. If we represent\ntokens as vectors, we can add positional information by rotating these vectors. This rotation preserves the “distances”\nbetween the vectors. For example, given that the distance between cat and sleeping is the same in both sentences, the\nangle between their embeddings also remains the same during rotation.\nexpressed mathematically in the form\nRo(x,θ) = xRθ\n=\n[\nx1 x2\n][\ncos θ sin θ\n−sin θ cos θ\n]\n=\n[\ncos θ·x1 −sin θ·x2 sin θ·x1 + cosθ·x2\n]\n(2.86)\nwhere Rθ =\n[\ncos θ sin θ\n−sin θ cos θ\n]\nis the rotation matrix. If two or more rotations are performed on the\nsame vector, we can rotate the vector further. This follows from the fact that the composition of\nsuccessive rotations is itself a rotation. More formally, rotating a vector by an angle θfor ttimes\n2.3 Long Sequence Modeling 89\ncan be expressed as\nRo(x,tθ) = xRtθ\n=\n[\ncos tθ·x1 −sin tθ·x2 sin tθ·x1 + costθ·x2\n]\n(2.87)\nIf we interpret tas the position of a token represented by x in a sequence, then we will find\nthat the above equation defines a simple positional embedding model. As shown in Figure 2.12\n(b), we start moving the token from position 0. Each time we move one step forward, the vector\nis rotated by the angle θ. Upon arriving at the position t, the representation of the token with\npositional context is given by Ro(x,iθ). As the rotations do not change the magnitude of the\nembedding, the original “meaning” of the token is retained. The positional information is injected\ninto the embedding, when it gets rotated.\nThis rotary positional embedding can be extended to multi-dimensional embeddings. For\na d-dimensional token embedding x =\n[\nx1 x2 ... xd\n]\n, we can treat it as a d\n2 -dimensional\ncomplex vector x′ =\n[\nx′\n1 x′\n2 ... x′\nd/2\n]\n=\n[\nx1 + ix2 x3 + ix4 ... xd−1 + ixd\n]\n, where\neach consecutive pair of items forms a complex number. Then, the rotary positional embedding in\n90 Generative Models\nthe complex space is given by\nC(x,tθ) =\nd/2∑\nk=1\nx′\nkeitθk⃗ ek (2.91)\nwhere ⃗ ek is the standard basis vector with a single non-zero value in the k-th coordinate and 0’s\nelsewhere [Biderman et al., 2021].\nTo illustrate, consider the rotary positional embedding model described above. The embedding\nof each token is described by a model Ro(xi,iθ) in which θ =\n[\nθ1,...,θ d/2\n]\nare the parameters.\n2.3 Long Sequence Modeling 91\n(2.93))\ncos iθ =\n[\ncos iθ1 ... cos iθd/2\n]\n(2.95)\nsin iθ =\n[\nsin iθ1 ... sin iθd/2\n]\n(2.96)\nθk is a exponential function of kand takes the form\nθk = b−2(k−1)\nd (2.97)\nwhere bis the base. The period of cos iθk and sin iθk is\nTk = 2 π·b\n2(k−1)\nd (2.98)\nThe key idea behind position interpolation is to adjust this period so that the new positions can\nbe encoded within the range [0,ml]. One way to achieve this is to scale up Tk by m\nml\n, given by\nT′\nk = m\nml\n·2π·b\n2(k−1)\nd (2.99)\nHence all points in [0,m] are compressed into [0,ml]. This linear scaling can be easily realized\nby modifying the input to the embedding model [Chen et al., 2023c]. The new model with linear\npositional interpolation is given by\nRo′(xi,iθ) = Ro( xi,ml\nmiθ) (2.100)\nAnother method of positional interpolation is to scale the base 17. Suppose that the base bis\nscaled by λ. We wish the period of this new model in the last dimension of θ(i.e., dimension d\n2 )\nto be equal to that of the linear positional interpolation model. This can be expressed as\n2π·(λb)\n2( d\n2 −1)\nd = m\nml\n·2π·b\n2( d\n2 −1)\nd (2.101)\nSolving this equation, we obtain\nλ =\n(m\nml\n) d\n2( d\n2 −1)\n=\n(m\nml\n)d\nd−2 (2.102)\nThis gives an embedding model\nRo′(xi,iθ) = Ro( xi,iθ′) (2.103)\n17This method was first proposed in https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/\nntkaware_scaled_rope_allows_llama_models_to_have/\n92 Generative Models\nwhere\nθ′=\n[\n(λb)−0\nd,(λb)−2\nd,..., (λb)−d−2\nd\n]\n(2.104)\nNote that scaling the base provides a non-uniform method for scaling the periods across dif-\nferent dimensions of θ. This method has been found to be helpful for extending LLMs to longer\nsequences, and several improvements have been developed [Peng et al., 2024; Ding et al., 2024]"
    },
    {
        "cluster_id": 40,
        "text": "Also, one may use methods specialized to catastrophic forgetting, such as experience replay [Rol-\nnick et al., 2019] and elastic weight consolidation [Kirkpatrick et al., 2017]. The interested reader\ncan refer to some surveys for more detailed discussions of this issue in continual learning [Parisi\net al., 2019; Wang et al., 2023a;e].\n[Kirkpatrick et al., 2017] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic\nforgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526,\n2017.\nContinual lifelong learning with neural networks: A review. Neural networks, 113:54–71, 2019.\n[Rolnick et al., 2019] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory\nWayne. Experience replay for continual learning. Advances in Neural Information Processing Systems,\n32, 2019.\n[Wang et al., 2023] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of\ncontinual learning: Theory, method and application. arXiv preprint arXiv:2302.00487, 2023a.\n[Wang et al., 2023] Zhenyi Wang, Enneng Yang, Li Shen, and Heng Huang. A comprehensive survey of\nforgetting in deep learning beyond continual learning. arXiv preprint arXiv:2307.09218, 2023e"
    },
    {
        "cluster_id": 60,
        "text": "The idea of learning word representations through neural language models inspired subsequent\nresearch in representation learning in NLP. However, this approach did not attract significant in-\nterest in developing NLP systems in the first few years after its proposal. Starting in about 2012,\nthough, advances were made in learning word embeddings from large-scale text via simple word\nprediction tasks. Several methods, such as Word2Vec, were proposed to effectively learn such\nembeddings, which were then successfully applied in a variety of NLP systems [Mikolov et al.,\n2013a;b]. As a result of these advances, researchers began to think of learning representations of\nsequences using more powerful language models, such as LSTM-based models [Sutskever et al.,\n2014; Peters et al., 2018]. And further progress and interest in sequence representation exploded\nafter Transformer was proposed. Alongside the rise of Transformer, the concept of language mod-\neling was generalized to encompass models that learn to predict words in various ways. Many\n2.1 A Brief Introduction to LLMs 37\npowerful Transformer-based models were pre-trained using these word prediction tasks, and suc-\ncessfully applied to a variety of downstream tasks [Devlin et al., 2019].\nFor Transformers, dealing with long sequences is computationally expensive, as the computa-\ntional cost of self-attention grows quadratically with the sequence length. This makes it infeasible\nto train and deploy such models for very long inputs. Two strands of research have tried to adapt\nTransformers to long-context language modeling.\nRecurrent models can be used in real-time learning scenarios where data arrives in a stream and\npredictions can be made at any time step. In NLP, applying recurrent models to language mod-\neling is one of the earliest successful attempts to learn representations of sequences. Although\nTransformer has been used as the foundational architecture in LLMs, recurrent models are still\npowerful models, especially for developing efficient LLMs. More recently, recurrent models have\nstarted their resurgence in language modeling and have been reconsidered as a promising alterna-\ntive to Transformers [Gu and Dao, 2023]. Figure 2.5 shows a comparison of the models discussed\nin this subsection.\n[Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceed-\nings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2978–2988,\n2019"
    },
    {
        "cluster_id": 71,
        "text": "A second method, which has been a de facto standard in recent research, is to adapt LLMs\nvia fine-tuning. As such, the token prediction ability learned in the pre-training phase can be\ngeneralized to accomplish new tasks. The idea behind fine-tuning is that some general knowledge\nof language has been acquired in pre-training, but we need a mechanism to activate this knowledge\nfor applying it to new tasks. To achieve this, we can slightly fine-tune the model parameters using\ninstruction-following data. This approach is called instruction fine-tuning.\nTo increase the diversity in the fine-tuning data and make LLMs generalize, we can define more\ntasks using instructions.\nThis data may include diverse instructions and possible responses. It has been found that scaling\nthe number of fine-tuning tasks is beneficial for improving the performance of LLMs [Chung\net al., 2022]. Note that although more fine-tuning data is favorable, the amount of this data is\ngenerally orders of magnitude smaller than that of the pre-training data. For example, LLMs can\nbe fine-tuned with tens or hundreds of thousands of samples, or even fewer if these samples are\nof high quality [Zhou et al., 2023a; Chen et al., 2023b], whereas pre-training such models may\nrequire billions or trillions of tokens, resulting in significantly larger computational demands and\nlonger training times [Touvron et al., 2023a].\nIt is also worth noting that we should not expect the fine-tuning data to cover all the down-\nstream tasks to which we intend to apply LLMs. A common understanding of how the pre-training\n+ fine-tuning approach works is that LLMs have gained knowledge for understanding instructions\nand generating responses in the pre-training phase. However, these abilities are not fully activated\nuntil we introduce some form of supervision. The general instruction-following behavior emerges\nas we fine-tune the models with a relatively small amount of labeled data. As a result, we can\nachieve some level of zero-shot learning: the fine-tuned models can handle new tasks that they\nhave not been explicitly trained or fine-tuned for [Sanh et al., 2022; Wei et al., 2022a]. This zero-\nshot learning ability distinguishes generative LLMs from earlier pre-trained models like BERT,\nwhich are primarily fine-tuned for specific tasks.\n• Supervised Fine-tuning (SFT). This involves continuing the training of pre-trained LLMs\non new, task-oriented, labelled data. A commonly used SFT technique is instruction fine-\ntuning. As described in the previous subsection, by learning from instruction-response\nannotated data, LLMs can align with the intended behaviors for following instructions,\nthereby becoming capable of performing various instruction-described tasks. Supervised\nfine-tuning can be seen as following the pre-training + fine-tuning paradigm, and offers a\nrelatively straightforward method to adapt LLMs.\nBoth RAG and fine-tuning are common methods for adapting LLMs using task-specific data.\n3.3.2.1 Adapting LLMs with Less Prompting\nOne obvious way to adapt an LLM for a particular task is to simply fine-tune the model using\nlabeled data. This leads to a variety of LLM alignment methods, such as supervised fine-tuning,\nwhich update the model parameters by aligning the responses to given prompts with supervision\nsignals. Fine-tuned LLMs embed task-related information in model parameters, and thus these\n144 Prompting\nmodels can respond correctly when dealing with similar prompts with those in fine-tuning.\n3.3.2.2 Learning Soft Prompts for Parameter-efficient Fine-tuning\nUpdating all parameters is a common method for adapting LLMs to tasks of interest. Although\nfine-tuning is considered computationally cheaper than pre-training, it is still costly to apply in\npractice. This issue motivates the development of parameter-efficient fine-tuning methods, which\naim to minimize the number of parameters that need to be updated.\n• SFT is still computationally expensive for LLMs due to their large size. As a result, main-\ntaining and updating such models is resource-intensive. For example, applying gradient up-\ndates to billions of parameters within an LLM requires significant computational power and\nmemory. This often requires high-performance computing environments, which are costly\nto operate. To address these challenges, various optimization strategies, such as pruning,\nquantization, and the use of more efficient training algorithms, have been explored. In par-\nticular, there has been significant interest in parameter-efficient fine-tuning methods which\nare designed to maintain state-of-the-art performance without the need for extensive compu-\ntational resources. We have seen in Chapter 3 that applying techniques like soft prompts can\n4.2 Instruction Alignment 161\nUser: I’ve been feeling very tired lately.\n4.2.2 Fine-tuning Data Acquisition\nFine-tuning data is so important that much recent work in LLM has focused on developing various\ndatasets for instruction fine-tuning. As with most work in machine learning, there are generally\ntwo approaches to data acquisition — manual data generation and automatic data generation.\nAn issue related to the construction of the fine-tuning datasets is that we usually want the\ndata to be as diverse as possible. Many studies have found that increasing the diversity of fine-\ntuning data can improve the robustness and generalization ability of LLMs. For this reason, there\nhas been considerable interest in involving more diverse prompts and tasks in LLM fine-tunining\ndatasets. We will provide further discussion on the generalization of fine-tuning in Section 4.2.4.\nThey showed that the LLaMa 65B model fine-tuned with this dataset could be competitive with\nor even better than models with much more fine-tuning effort. This suggests that LLMs can\nbe adapted to respond to diverse tasks without necessarily needing fine-tuning on all types of\ninstruction-following data. Chen et al. [2024a] developed a system based on the GPT-3.5 model\nto assess the quality of each instruction-following sample. Therefore, they could select high-\nquality samples from existing datasets, showing better fine-tuning performance with fewer fine-\ntuning samples. Researchers have also developed methods to either select or filter out data using\n4.2 Instruction Alignment 167\nheuristics [Zhao et al., 2024; Ge et al., 2024], or to prioritize data that more significantly influences\nthe fine-tuning process [Xia et al., 2024]. In fact, most of these methods can be seen as instances\nof larger families of data selection and filtering methods. And it is often the case that using higher\nquality (but maybe less) data is beneficial for training NLP models.\nThe discoveries in instruction fine-tuning somewhat differ from traditional views in NLP: the\nability of models to handle complex problems can be activated with a small amount of annotated\ndata, rather than requiring massive amounts of supervised data for extensive training. One possible\nexplanation is that the ability of generating correct responses given instructions has been learned\nduring pre-training, but such instruction-response mappings are not with high probabilities during\ninference. Fine-tuning can slightly adjust the models to get them to follow instructions, requiring\nsignificantly less training effort than pre-training. This is closely related to what is known as\nthe superficial alignment hypothesis, which suggests that learning primarily occurs during pre-\ntraining, and the subsequent fine-tuning or alignment phase does not significantly contribute to the\nunderlying knowledge base of an LLM [Zhou et al., 2023a]. Since the core abilities and knowledge\nof the model are already established from pre-training, effective fine-tuning for alignment with\nuser needs can be achieved with relatively small training fine-tuning effort. This implies the\npossibility of fine-tuning LLMs with very little data. In another direction, it may not be necessary\nto restrict fine-tuning to paired instruction-response data. For example, Hewitt et al. [2024] found\nthat instruction-following can be implicitly achieved by fine-tuning LLMs only on responses,\nwithout corresponding instructions.\nOne way to generalize instruction fine-tuning is to increase the diversity of the fine-tuning\ndata. In earlier studies on instruction fine-tuning, researchers developed many datasets, covering\na wide variety of NLP tasks and different instructions for each task [Wang et al., 2022b; Sanh\net al., 2022; Longpre et al., 2023]. By transforming these tasks into a unified format, one can fine-\ntune an LLM with a sufficiently large number of samples, for example, there have been several\ninstruction fine-tuning datasets that involve over 100 NLP tasks and 1M samples. However, these\nearly datasets mostly focus on existing academic problems, but not those that users want to deal\nwith in real-world applications. Much recent work has shifted focus to addressing new and more\npractical problems. For example, there has been considerable interest in constructing datasets\nthat contain large and complicated demonstrations and responses from SOTA models to real user\nqueries [Wang et al., 2023c; Teknium, 2023].\nAnother view is that some instruction-response mappings may already be learned during pre-\ntraining, and so the pre-trained LLMs have encoded such mappings. However, since we often do\nnot know exactly what data is used in the pre-training, it is hard to judge whether we need to learn\nsuch mappings in the fine-tuning. A related question is whether out-of-distribution generalization\n4.2 Instruction Alignment 169\nis primarily achieved during pre-training or fine-tuning. While directly answering this question is\nbeyond the scope of this chapter, it has been shown that pre-training on large and diverse datasets\nis effective in improving out-of-distribution performance [Hendrycks et al., 2020; Radford et al.,\n2021; Gunasekar et al., 2023]. This raises an interesting problem: if an LLM has been well pre-\ntrained at scale, fine-tuning may not be as essential for out-of-distribution generalization, since the\nmodel may have already encountered sufficient distributional variation. This prompts researchers\nto fine-tune LLMs with modest effort or to explore new methods to achieve instruction-following"
    },
    {
        "cluster_id": 28,
        "text": "LLMs are powerful models but are expensive to build.\nYes\nLLMs are powerful models but are expensive to build.\nLLMs are powerful models but are expensive to build.\nGiven their amazing power, LLMs have attracted considerable interest, both in terms of tech-\nniques and applications. As a result, the explosion of research interest in LLMs has also led to a\n2.4 Summary 95\nvast number of new techniques and models. However, we do not attempt to provide a comprehen-\nsive literature review on all aspects of LLMs, given the rapid evolution of the field. Nevertheless,\none can still gain knowledge about LLMs from general reviews [Zhao et al., 2023; Minaee et al.,\n2024] or more focused discussions on specific topics [Ruan et al., 2024].\nIn addition to these metrics, energy efficiency and cost efficiency are practical considerations\nfor deploying LLMs at scale. Energy efficiency measures the amount of electrical power con-\nsumed by the model during inference. Cost efficiency, on the other hand, evaluates the total\nexpenses related to deploying and maintaining the model.\nWhile quality-focused metrics are essential for assessing LLMs, efficiency metrics are equally\ncrucial for their effective deployment in real-world applications. A comprehensive evaluation\nframework should include both sets of metrics to accurately estimate an LLM’s performance and\npracticality"
    },
    {
        "cluster_id": 34,
        "text": "Character 1: A boy whose name is Andy.\nCharacter 2: Andy’s dog.\nIn the heart of a bustling city, Andy, his loyal dog Rocket, and his friend\nJane discovered a hidden garden beneath the glow of twilight. Surrounded by\nancient trees and whispering winds, they shared dreams and secrets, forging an\nunbreakable bond. As the night deepened, they made a pact under the stars to\nalways find magic in the mundane. Together,they realized that true adventure lay\nin the moments spent with friends, and every day was a treasure waiting to be\ndiscovered.\nStart of the story:\nOne sunny afternoon, Lily discovered a tiny, golden key hidden under a rock in\nher backyard. Curious, she picked it up and wondered what it might unlock. Just\nthen, her cat, Whiskers, began to meow excitedly, leading her towards the old\noak tree at the edge of the garden.\nLily followed Whiskers, her heart pounding with excitement and a touch of\nnervousness. The old oak tree had always seemed mysterious to her, its gnarled\nbranches casting long shadows even on the brightest days. As they reached the\ntree, Whiskers darted ahead and started pawing at something buried beneath a\npile of autumn leaves ...\nText:\nThe quick brown fox jumps over the lazy dog"
    },
    {
        "cluster_id": 30,
        "text": "Ingredients:\n2 cups heavy cream; 1 cup milk; 3/4 cup sugar; 2 tsp vanilla extract;\nPinch of salt.\nSteps:\n1. Whisk all ingredients until sugar dissolves.\n2. Chill mixture for 2 hours in the fridge.\n3. Freeze in a shallow dish, stirring every 30 minutes, 3-4 times.\n4. Freeze solid for 2 hours"
    },
    {
        "cluster_id": 26,
        "text": "If you buy 5 apples and each apple costs $1.20, how much do you spend in total?\n$6.00\nWrite a Python program to calculate the sum of squares of the following numbers.\nA: Calculate the square of each number: 12 = 1, 32 = 9, 52 = 25, and 72 = 49.\nSum the squares, 1 + 9 + 25 + 49 = 84. There are 4 numbers in total. Divide\nthe sum by the number of items, 84/4 = 21. The answer is 21"
    },
    {
        "cluster_id": 58,
        "text": "2.1.4 Aligning LLMs with the World\nInstruction fine-tuning provides a simple way to adapt LLMs to tasks that can be well defined. This\nproblem can broadly be categorized as an alignment problem. Here, alignment is referred to as a\nprocess of guiding LLMs to behave in ways that align with human intentions. The guidance can\ncome from labeled data, human feedback, or any other form of human preferences. For example,\n2.1 A Brief Introduction to LLMs 47\nwe want LLMs not only to be accurate in following instructions, but also to be unbiased, truthful,\nand harmless. So we need to supervise the models towards human values and expectations. A\ncommon example is that when we ask an LLM how to build a weapon, it may provide a list of key\nsteps to do so if it is not carefully aligned. However, a responsible model should recognize and\navoid responding to requests for harmful or illegal information. Alignment in this case is crucial\nfor ensuring that LLMs act responsibly and in accordance with ethical guidelines.\nA related concept to alignment is AI safety. One ultimate goal of AI is to build intelligent\nsystems that are safe and socially beneficial. To achieve this goal we should keep these systems\nrobust, secure, and subjective, in any conditions of real-world use, even in conditions of misuse\nor adverse use. For LLMs, the safety can be increased by aligning them with appropriate human\nguidance, such as human labeled data and interactions with users during application.\nAlignment is difficult as human values and expectations are diverse and shifting. Sometimes,\nit is hard to describe precisely what humans want, unless we see the response of LLMs to user\nrequests. This makes alignment no longer a problem of tuning LLMs on predefined tasks, but a\nbigger problem of training them with the interactions with the real world.\nAs a result of the concerns with controlling AI systems, there has been a surge in research\non the alignment issue for LLMs. Typically, two alignment steps are adopted after LLMs are\npre-trained on large-scale unlabeled data.\n• Learning from Human Feedback. After an LLM finishes pre-training and supervised fine-\ntuning, it can be used to respond to user requests if appropriately prompted. But this model\nmay generate content that is unfactual, biased, or harmful. To make the LLM more aligned\nwith the users, one simple approach is to directly learn from human feedback. For example,\ngiven some instructions and inputs provided by the users, experts are asked to evaluate how\nwell the model responds in accordance with their preferences and interests. This feedback\nis then used to further train the LLM for better alignment.\nFor example, pre-trained LLMs may not be able to follow user instructions because they were\nnot trained to do so. Another example is that LLMs may generate harmful content or perpetuate\nbiases inherent in their training data. This poses new challenges in ensuring that LLM outputs are\nnot only accurate and relevant, but also ethically sound and non-discriminatory.\nIn this chapter, we will focus on alignment methods for LLMs. We will begin by discussing the\ngeneral alignment tasks. Then we will consider two widely-used approaches, known as instruc-\ntion alignment and human preference alignment, respectively. The former resorts to supervised\nfine-tuning techniques and guides the LLMs to generate outputs that adhere closely to user instruc-\ntions. On the other hand, the latter typically relies on reinforcement learning techniques, where\nthe LLMs are trained based on feedback from humans. While these methods are motivated by\ndifferent goals, they are commonly used together to develop well-aligned LLMs.\nThe second approach is to fine-tune LLMs using reward models. One difficulty in alignment\nis that human values and expectations are complex and hard to describe. In many cases, even for\nhumans themselves, articulating what is ethically correct or culturally appropriate can be challeng-\ning. As a result, collecting or annotating fine-tuning data is not as straightforward as it is with SFT.\nMoreover, aligning LLMs is not just a task of fitting data, or in other words, the limited samples\nannotated by humans are often insufficient to comprehensively describe these behaviors. What we\nreally need here is to teach the model how to determine which outputs are more in line with human\npreferences, for example, we not only want the outputs to be technically accurate but also to align\nwith human expectations and values. One idea is to develop a reward model analogous to a human\nexpert. This reward model would work by rewarding the LLM whenever it generates responses\nthat align more closely with human preferences, much like how a teacher provides feedback to a\nstudent. To obtain such a reward model, we can train a scoring function from human preference\ndata. The trained reward model is then used as a guide to adjust and refine the LLM. This frames\nthe LLM alignment task as a reinforcement learning task. The resulting methods, such as rein-\nforcement learning from human feedback (RLHF), have been demonstrated to be particularly\nsuccessful in adapting LLMs to follow the subtleties of human behavior and social norms.\nThe third approach is to perform alignment during inference rather than during training or\nfine-tuning. From this perspective, prompting in LLMs can also be seen as a form of alignment,\nbut it does not involve training or fine-tuning. So we can dynamically adapt an LLM to various\ntasks at minimal cost. Another method to do alignment at inference time is to rescore the outputs\nof an LLM. For example, we could develop a scoring system to simulate human feedback on the\noutputs of the LLM (like a reward model) and prioritize those that receive more positive feedback.\nmodels (see Figure 4.6). To do this, we can recruit human experts who indicate their preferences\nbetween pairs of outputs generated by the LLM. This preference data is used to train a reward\nmodel that can predict the perceived quality of LLM outputs. Once trained, the reward model\nprovides feedback by assigning scores to new outputs that the LLM generates in response to the\ninputs. The LLM uses these scores to update its parameters through reinforcement learning algo-\nrithms. In the rest of this section, we will first introduce the basic knowledge of reinforcement\nlearning to facilitate the discussion, and then discuss methods for training reward models and\naligning LLMs with these models.\nTo train the reward model, the first step is to collect human feedback on a set of generated\noutputs. Given an input x, we use the LLM to produce multiple candidate outputs {y1,..., yN}.\n4.4.3 Automatic Preference Data Generation\nAlthough learning from human preferences is an effective and popular method for aligning LLMs,\nannotating preference data is costly. Using human feedback does not only faces the problem of\nlimited scalability, but it may also introduce bias because human feedback is inherently subjective.\n4.4.5 Inference-time Alignment\nIn this section we explored a variety of methods to align models with human preferences and an-\nnotations. However, one of the significant limitations of many such methods is that LLMs must\nbe fine-tuned. For RLHF and its variants, training LLMs with reward models can be computa-\ntionally expensive and unstable, leading to increased complexity and costs when applying these\napproaches. In this case, we can consider aligning models at inference time, thus avoiding the\nadditional complexity and effort involved.\n4.5 Summary\nIn this chapter, we have explored a range of techniques for aligning LLMs. In particular, we\nhave discussed fine-tuning methods that enable LLMs to follow instructions and align them with\nhuman preferences. One of the benefits of fine-tuning LLMs is computation efficiency. Unlike\npre-training based on large-scale neural network optimization, fine-tuning is a post-training step\nand so is less computationally expensive. Moreover, it is better suited to address problems that are\nnot easily solved in pre-training, such as human value alignment. The widespread attention to the\nalignment issue has also led to a surge of research papers on this topic, which has posed challenges\nin writing this chapter, as it is difficult to cover all the latest techniques. However, we have tried\nto provide a relatively detailed introduction to the fundamental approaches to alignment, such as\nissues are not often discussed in the context of LLMs. For example, suppose we have an old model and a new, more\npowerful model. We can use the new model to select the best output from theN-best list of the old model as the oracle\noutput. The performance difference between the oracle output and the top-1 output of the original N-best list reflects\nthe performance gain brought by the new model. If the performance gain is significant, we can say that the old model\nhas more model errors. If the gain is small, it may indicate that the issue lies in search errors, as the best candidates\nwere not found.\nWhile we have focused on LLM alignment techniques in this chapter, the term AI alignment\nis a wide-ranging concept. It generally refers to the process of ensuring that the behavior of an AI\nsystem aligns with human values, goals, and expectations. The idea of AI alignment can be traced\nback to the early days of AI. A widely cited description of AI alignment comes from an article by\nthe mathematician and computer scientist Norbert Wiener [Wiener, 1960]. The quote is as follows\nIf we use, to achieve our purposes, a mechanical agency with whose operation\nwe cannot efficiently interfere ... we had better be quite sure that the purpose\nput into the machine is the purpose which we really desire.\nThese challenges have motivated and are motivating AI research towards more aligned sys-\ntems, either through developing new mechanisms for perceiving the world or more efficient and\ngeneralizable methods to adapt these systems to given tasks. More importantly, as AI systems\nbecome more powerful and intelligent, especially given that recent advances in LLMs have shown\nremarkable capabilities in dealing with many challenging problems, the need for AI alignment\nhas become more urgent. Researchers have started to be concerned with AI safety and warn the\ncommunity that they need to develop and release AI systems with great caution to prevent these\nsystems from being misaligned [Russell, 2019; Bengio et al., 2024]"
    },
    {
        "cluster_id": 56,
        "text": "Predicting\nComparisons\ny1 ≻y4 ≻y2 ≻y3\nAnnotating Data with Human Preferences\n(b) Annotating Data with Human Preferences\nReward Model\nComparison Data\n{(x,yk1 ≻yk2 )}\nTraining\n(c) Training the Reward Model\nLLM\n(Policy)\nDataset D\nx ∼D\nInput-output Pairs\n{x,y}\nSampling y via the Policy Pr(y|x)\nReward Model\nReward Scores\n{r(x,y)}\nEvaluate the Input-output Pairs\n(d) Training/Fine-tuning the Policy\nRL Fine-tuning\nFig. 2.2: An overview of RLHF. There are 4 key steps involved: a) training an initial LLM (i.e., policy) using pre-\ntraining and supervised fine-tuning; b) collecting human preference data by ranking the outputs of the LLM; c) training\na reward model using the ranking results; d) RL fine-tuning of the policy based on the reward model. Double line\narrows mean training or fine-tuning.\nWe train this reward model using ranking loss. For example, a pair-wise ranking loss function\ncan be written in the form\nLossω(Dr) = −E(x,yk1 ,yk2 )∼Dr log(Sigmoid(Rω(x,yk1 ) −Rω(x,yk2 ))) (2.19)\nwhere ω represents the parameters of the reward model, and Dr represents a set of tuples of an\ninput and a pair of outputs. (x,yk1 ,yk2 ) ∼Dr is a sampling operation which draws a sample\n(x,yk1 ,yk2 ) from Dr with some probability. As an example, suppose we first draw a model\ninput x with a uniform distribution and then draw a pair of model outputs with a probability of\nyk1 ≻yk2 given x (denoted by Pr(yk1 ≻yk2 |x)). The corresponding loss function is given by\nLossω(Dr)\n= −\n∑\nPr(x) ·Pr(yk1 ≻yk2 |x) ·log(Sigmoid(Rω(x,yk1 ) −Rω(x,yk2 )))\n= −1\nK\n∑\nPr(yk1 ≻yk2 |x) ·log(Sigmoid(Rω(x,yk1 ) −Rω(x,yk2 ))) (2.20)\nwhere K represents the number of model inputs involved in sampling. While the form of these\nfunctions may seem complex, their idea is simple: we penalize the model if the predicted ranking\nof two outputs differs from the human-labeled ranking. By contrast, the model receives a bonus,\nif the predicted ranking matches the human-labeled ranking.\nWe can train the reward model by minimizing the above ranking loss\nˆω = arg min\nω\nLossω(Dr) (2.21)\nThe resulting model Rˆω(·) can be employed to evaluate any given pair of input and output. Note\nthat although the reward model is trained using a ranking-based objective, it is used for scoring.\nIn practice, more advanced reinforcement learning algorithms, such as proximal policy opti-\nmization (PPO), are often used for achieving more stable training, as well as better performance.\nAn interesting question arises here: why not consider learning from human preferences as\na standard supervised learning problem? This question is closely related to our aforementioned\ndiscussion on the difficulty of data annotation. Often, describing human values and goals is chal-\nlenging, and it is even more difficult for humans to provide outputs that are well aligned. As an\nalternative, annotating the preferences of a given list of model outputs offers a simpler task. By\ndoing so, we can create a model that understands human preferences, which can then be used as\na reward model for training policies. From the perspective of machine learning, RLHF is par-\nticularly useful for scenarios where the desired behavior of an agent is difficult to demonstrate\nbut can be easily recognized by humans. Another advantage of RLHF is its ability to explore the\nsample space. By employing sampling techniques, models trained with reinforcement learning\ncan venture beyond the annotated data set to explore additional samples. This exploratory ability\nallows RLHF to discover potentially beneficial policies that are not immediately apparent from\nthe labeled data alone.\n• Value Function (V and Q). A state-value function (or value function, for short) assesses\nthe expected discounted return (i.e., accumulated rewards) for an agent starting from a par-\nticular state sand following a specific policy π. It is defined as:\nV(s) = E\n[\nr(s0,a0,s1) + γr(s1,a1,s2) + γ2r(s2,a2,s3) + ···\n⏐⏐s0 = s,π\n]\n= E\n[\nr0 + γr1 + γ2r2 + ···\n⏐⏐s0 = s,π\n]\n= E\n[∞∑\nt=0\nγtrt\n⏐⏐s0 = s,π\n]\n(4.15)\nwhere γ ∈[0,1] is the discount factor that adjusts the importance of future rewards, s0 = s\nindicates that the agent starts with the state s, and the expectation E is performed over all\npossible trajectories (i.e., state-action sequences). Similarly, an action-value function (or\n4.3 Human Preference Alignment: RLHF 175\nx0 x1 ... xm y1 ... yt−1\nPolicy (LLM)\ny1 y2 ... yt\nState st (x and y<t)\nAction at\nReward Model\nR( st, at )\nValue Functions\nV( st ) and Q( st, at )\nFeedback\nFig. 4.7: LLM as policy in reinforcement learning. At each step t, the LLM predicts a token yt given the model\ninput x and the previously-generated tokens y<t. This process can be framed as a reinforcement learning problem,\nwhere yt serves as the action, (x,y<t) as the state, and the predicted distribution Pr(yt|x,y<t) as the policy. Once\nyt is predicted, the LLM inputs both (x,y<t) and yt to the reward model, which evaluates how effectively the chosen\ntoken contributes to achieving the desired textual outcome. This evaluation generates reward scores which are used to\ncompute the value functions V(st) and Q(st,at). These functions then provide feedback to the LLM and guide the\npolicy training.\nQ-value function) measures the expected return starting from a state staking an action a\nand thereafter following a policy π, given by\nQ(s,a) = E\n[∞∑\nt=0\nγtrt\n⏐⏐s0 = s,a0 = a,π\n]\n(4.16)\nwhere a0 = aindicates that the action taken at the initial state is a.\nThe goal of reinforcement learning is to learn a policy that maximizes the cumulative re-\nward (or return) the agent receives over the long run. Given a state-action sequence τ =\n{(s1,a1),..., (sT,aT)}1, the cumulative reward over this sequence can be written as\nR(τ) =\nT∑\nt=1\nrt (4.17)\nThe expectation of this cumulative reward over a space of state-action sequences is given in\nthe form\nJ(θ) = E\nτ∼D\n[\nR(τ)\n⏐⏐πθ\n]\n=\n∑\nτ∈D\nPrθ(τ)R(τ)\n=\n∑\nτ∈D\nPrθ(τ)\nT∑\nt=1\nrt (4.18)\n1We assume the state-action sequence begins with s1 and a1, rather than s0 and a0, to align with the notation\ncommonly used in this chapter, where the prediction y typically starts from y1. Of course, it is also common to denote\na state-action sequence as {(s0,a0),..., (sT,aT)}or {(s0,a0),..., (sT−1,aT−1)}in the literature. But this variation\nin notation does not affect the discussion of the models presented here.\nThen the training objective is to maximize J(θ):\n˜θ = arg max\nθ\nJ(θ) (4.19)\nNow, we have a simple reinforcement learning approach: 1) we sample a number of state-\naction sequences; then, 2) we evaluate each sequence using the performance function; then, 3) we\nupdate the model to maximize this performance function. If we take Eq. (4.18) and use gradient\ndescent to optimize the policy, this approach would constitutes a form of policy gradient methods\n[Williams, 1992].\nThe model described in Eqs. (4.17-4.19) establishes a basic form of reinforcement learning,\nand many variants and improvements of this model have been developed. Before showing those\nmore sophisticated models, let us take a moment to interpret the objective function J(θ) from the\nperspective of policy gradient. In gradient descent, we need to compute the gradient of J(θ) with\nrespect to θ:\n∂J(θ)\n∂θ = ∂∑\nτ∈DPrθ(τ)R(τ)\n∂θ\n=\n∑\nτ∈D\n∂Prθ(τ)\n∂θ R(τ)\n=\n∑\nτ∈D\nPrθ(τ)∂Prθ(τ)/∂θ\nPrθ(τ) R(τ)\n=\n∑\nτ∈D\nPrθ(τ)∂log Prθ(τ)\n∂θ R(τ) (4.20)\nIn some cases, we will assume that every sequence in Dis equally probable (i.e., Prθ(τ) =\n1/|D|). In this case we can simplify Eq. (4.20) and need only consider the terms ∂log Prθ(τ)\n∂θ and\nR(τ):\n∂J(θ)\n∂θ = 1\nm\n∑\nτ∈D\n∂log Prθ(τ)\n∂θ R(τ) (4.21)\nOne advantage of this result is that R(τ) does not need to be differentiable, which means that we\ncan use any type of reward function in reinforcement learning.\nThe policy component, log πθ(at|st), determines the log-probability of taking action at given\nstate st, and it is parameterized by θ. The dynamics component, log Pr(st+1|st,at), represents\nthe log-probability of transitioning to state st+1 from state st after taking action at. In typical\nreinforcement learning settings, the dynamics are not directly influenced by the policy parameters\nθ, and thus, their derivatives are often zero. In this case, therefore, Eq. (4.22) can be simplified to:\n∂log Prθ(τ)\n∂θ = ∂\n∂θ\nT∑\nt=1\nlog πθ(at|st) (4.23)\nIn other words, we only concentrate on optimizing the policy without concerning ourselves with\nthe underlying dynamics.\nSubstituting Eq. (4.23) into Eq. (4.21), and expanding R(τ), we then obtain\n∂J(θ)\n∂θ = 1\n|D|\n∑\nτ∈D\n∂\n∂θ\n(T∑\nt=1\nlog πθ(at|st)\nT∑\nt=1\nrt\n)\n(4.24)\nWhile this policy gradient approach is straightforward, it suffers from the problem that the\nvariance of the estimated gradients can be very high, making the learning process noisy and inef-\nficient. One reason for this high variance problem is that rewards can vary greatly across different\nsteps or scenarios. Imagine that in a sequence of action decisions, the reward model tends to assign\nsmall rewards to good actions (e.g., Rt = 2) and large penalties to poor actions (e.g., Rt = −50).\n178 Alignment\nThis policy gradient model with a baseline can be given by\n∂J(θ)\n∂θ = 1\n|D|\n∑\nτ∈D\n∂\n∂θ\n(T∑\nt=1\nlog πθ(at|st)\n)(T∑\nt=1\nrt −b\n)\n= 1\n|D|\n∑\nτ∈D\n∂\n∂θ\n[T∑\nt=1\nlog πθ(at|st)\n( T∑\nk=1\nrk −b\n)]\n= 1\n|D|\n∑\nτ∈D\n∂\n∂θ\n[T∑\nt=1\nlog πθ(at|st)\n(t−1∑\nk=1\nrk +\nT∑\nk=t\nrk −b\n)]\n(4.25)\nHere we write ∑T\nk=1 rk as the sum of two terms ∑t−1\nk=1 rk and ∑T\nk=trk to distinguish between the\nrewards accrued before and after the action at time stept. Note that in Markov decision processes,\nthe future is independent of the past given the present. Therefore, the action taken at time step t\ncannot influence the rewards received beforet, or in other words, the rewards prior totare already\n“fixed” by the time the action attis chosen. The term ∑t−1\nk=1 rk does not contribute to the gradient\nand can be omitted, leading to a simplified version of Eq. (4.25)\n∂J(θ)\n∂θ = 1\n|D|\n∑\nτ∈D\n∂\n∂θ\n[T∑\nt=1\nlog πθ(at|st)\n(T∑\nk=t\nrk −b\n)]\n(4.26)\nAlso note that removing ∑T\nk=trk can further reduce the variance of the gradient.\nBy using the advantage function A(st,at), the gradient of J(θ) can be written in the form\n∂J(θ)\n∂θ = 1\n|D|\n∑\nτ∈D\n∂\n∂θ\n(T∑\nt=1\nlog πθ(at|st)A(st,at)\n)\n(4.28)\nThis optimization objective corresponds to the advantage actor-critic (A2C) method in re-\ninforcement learning [Mnih et al., 2016]. In this method, the actor aims at learning a policy. It\nupdates the policy parameters using Eq. (4.28) to help focus more on actions that are likely to\nimprove performance. The critic, on the other hand, updates its estimation of the value function,\nwhich is used to calculate the advantage function A(st,at), thus serving as the evaluator of the\n4.3 Human Preference Alignment: RLHF 179\npolicy being learned by the actor.\nUp to this point, we have spent considerable space discussing the basics of reinforcement\nlearning, especially on how to derive the optimization objective for the A2C method. However,\nreinforcement learning is a vast field, and many technical details cannot be covered here. The in-\nterested reader can refer to reinforcement learning books for more details [Sutton and Barto, 2018;\nSzepesvári, 2010]. Nevertheless, we now have the necessary knowledge to further discuss RLHF.\n4.3.2 Training Reward Models\nWe have shown that reward models play a very important role in the general reinforcement learn-\ning framework and form the basis for computing value functions. We now consider the problem\nof training these reward models.\nIn RLHF, a reward model is a neural network that maps a pair of input and output token\nsequences to a scalar. Given an input x and an output y, the reward can be expressed as\nr = Reward( x,y) (4.33)\nwhere Reward(·) is the reward model. rcan be interpreted as a measure of how well the outputy\naligns with the desired behavior given the inputx. As discussed in the previous subsection, bothx\n3The training loss for the value network (or critic network) in A2C is generally formulated as the mean squared\nerror between the computed return rt+ γV(st+1) and the predicted state value V(st). Suppose that the value network\nis parameterized by ω. The loss function is given by\nLv(ω) = 1\nM\n∑ (\nrt + γVω(st+1) −Vω(st)\n)2\n(4.32)\nwhere M is the number of training samples, for example, for a sequence of T tokens, we can set M = T.\nRecall from Section 4.3.1 that a state-action sequence or trajectoryτcan be evaluated by the utility\nfunction\nU(τ; θ) =\nT∑\nt=1\nlog πθ(at|st)A(st,at) (4.39)\nwhere A(st,at) is the advantage of taking the actionat given the statest. An estimate of A(st,at)\nis defined as the TD error rt + γV(st+1) −V(st), where the value function V(st) is trained with\nthe reward model.\nOne common improvement of policy gradient methods is to use importance sampling to\nrefine the estimation of U(τ; θ). This can be written as\nU(τ; θ) =\nT∑\nt=1\nπθ(at|st)\nπθref (at|st)A(st,at) (4.45)\nHere we replace the log-probability log πθ(at|st) with the ratio πθ(at|st)\nπθref (at|st) . θref denotes the pa-\nrameters of the previous policy (such as an initial model from which we start the training). So\nπθ(at|st)\nπθref (at|st) , also called the ratio function, can be interpreted as the log-probability ratio between\nthe current policy πθ and the previous policy πθref (call it the reference policy). By using the\nratio function we reweight the observed rewards based on the likelihood of the actions under the\ncurrent policy versus the reference policy. When πθ(at|st)\nπθref (at|st) >1, the action at is more favored by\nthe current policy compared to the reference policy. By contrast, when πθ(at|st)\nπθref (at|st) <1, the action\nat is less favored by the current policy4.\n4Consider a more general case where we wish to evaluate the policy using its expected reward (also see Eq. (4.18))\nJ(θ) = Eτ∼πθ\n[\nR(τ)\n]\n(4.46)\nHere τ ∼πθ means that the sequence τ is generated by the policyπθ. Alternatively, we can writeJ(θ) in another form\nJ(θ) = Eτ∼πθref\n[Prθ(τ)\nPrθref (τ)R(τ)\n]\n(4.47)\nIt is not difficult to find that the right-hand sides of these equations are essentially the same since\nEτ∼πθref\n[\nPrθ(τ)\nPrθref (τ) R(τ)\n]\n=\n∑\nτ Prθref (τ) Prθ(τ)\nPrθref (τ) R(τ) =\n∑\nτ Prθ(τ)R(τ) = Eτ∼πθ\n[\nR(τ)\n]\nNote that this equivalence holds only when the expectation is performed over the entire sequence space. In practice,\nhowever, we often only sample a relatively small number of sequences using a policy in policy learning. As a result,\nthe sampling method itself matters. Eq. (4.47) offers an interesting manner to separate the sampling and reward\ncomputation processes: we first use a baseline policy (with θref) to sample a number of sequences, and then use the\ntarget policy (with θ) to compute the expected reward. In this way, we separate the policy used for collecting the data,\nand the policy used for computing the gradient. This approach avoids the need to directly sample from the policy we are\nevaluating, which can be beneficial in cases where generating sequences from the target policy is expensive or difficult.\nA further improvement to the above model is to considertrust regions in optimization [Schul-\nman et al., 2015]. In reinforcement learning, a large update to the policy can lead to instability,\nwhere the agent may start performing worse after an update. A reasonable idea is to optimize the\nmodel in the trust region, which refers to a region around the current parameter estimate where\nthe model is well-behaved. One approach to incorporating trust regions is to impose a constraint\non the size of the policy update, ensuring that the current policy does not deviate too significantly\nfrom the reference policy. This can be achieved by adding a penalty based on some form of di-\nvergence between the current and reference policies to the objective function. A simple form of\nsuch a penalty is given by the difference in the log-probability of the sequenceτ under the current\npolicy versus the reference policy:\nPenalty = log πθ(τ) −log πθref (τ) (4.51)\nIn practice, this penalty can be approximated by considering only the policy probabilities and\nignoring the dynamics. This gives\nPenalty =\nT∑\nt=1\nlog πθ(at|st) −\nT∑\nt=1\nlog πθref (at|st) (4.52)\nBy including this penalty in the optimization objective, we encourage the current policy to remain\nclose to the reference policy, limiting very large updates that could destabilize learning.\n4.3 Human Preference Alignment: RLHF 185\nU(x,y; θ) = Uppo-clip(x,y; θ) −βPenalty (4.54)\nwhere\nUppo-clip(x,y; θ) =\nT∑\nt=1\nClip\n(πθ(yt|x,y<t)\nπθref (yt|x,y<t)\n)\nA(x,y<t,yt) (4.55)\nPenalty = log Pr θ(y|x) −log Prθref (y|x)\n=\nT∑\nt=1\nlog Prθ(yt|x,y<t) −\nT∑\nt=1\nlog Prθref (yt|x,y<t) (4.56)\nAlthough the notation here appears a bit tedious, the idea of PPO is simple: we develop an\nobjective by combining the clipped likelihood ratio of the target and reference policies with an\nadvantage function, and then impose a penalty that ensures policy updates are not too large. The\nPPO-based RLHF is illustrated in Figure 4.9.\n• Value Model or Value Function (Vω(·) where ωdenotes the parameters). The value func-\ntion receives reward scores from the reward model and is trained to predict the expected\nsum of rewards that can be obtained starting from a state. It is generally based on the same\narchitecture as the reward model.\nIn practice, these models need to be trained in a certain order. First, we need to initialize them\nusing some other models. For example, the reward model and the value model can be initialized\nwith a pre-trained LLM, while the reference model and the target model can be initialized with a\nmodel that has been instruction fine-tuned. Note that, at this point, the reference model is ready for\nuse and will not be further updated. Second, we need to collect human preference data and train the\nreward model on this data. Third, both the value model and the policy are trained simultaneously\nusing the reward model. At each position in an output token sequence, we update the value model\n186 Alignment\nReward Model\nTraining\nPolicy Training\nReward Model\nrϕ(x,y)\nTo Learn\nLLM Policy\nPrθ(yt|x,y<t)\nTo Learn\nValue Function\nVω(x,y<t)\nTo Learn\nRef Model\nPrθold (yt|x,y<t)\nFixed\nMinimizing the loss based on\nthe Bradley-Terry model\nmin\nϕ\n− 1\n|Dr|\n∑\n(x,ya,yb)∈Dr\nlog σ(rϕ(x,ya) −rϕ(x,yb))\nHuman preference data Dr = {(x,ya,yb)}\nInput-only data D= {x}\nx0 x1 ··· xm y1 ··· yt−1\nState (x,y<t)\nx1 x2 ··· y1 y2 ··· yt Action yt\n(sampled with Prθold )\nEvaluate the state-action pair using the advantage\nfunction or the TD error (based on the reward\nmodel and the value function)\nLLM Policy\nLLM Policy\nMinimizing the clipped PPO loss\nwith the penalty\nmin\nθ\n−\n∑\nx∈D,y∼Prθold (·|x)\n∑T\nt=1\n[\nClip\n(Prθ(yt|x,y<t)\nPrθold (yt|x,y<t)\n)\nAt−\nβ·\n(\nlog Prθ(yt|x,y<t)−\nlog Prθold (yt|x,y<t)\n)]\nValue Function\nMinimizing the MSE between the\ncomputed return and the predicted\nstate value\nmin\nω\n1\nM\n∑\nx∈D\n∑T\nt=1\n[\nrt+ γVω(x,y<t+1) −Vω(x,y<t)\n]2\n∗∗rt = r(x,y<t+1) denotes the reward received as step t.\n∗∗Atdenotes the advantage at step t, and can be defined as rt+ γVω(x,y<t+1) −Vω(x,y<t)\nFig. 4.9: Illustration of RLHF. The first step is to collect human preference data and train the reward model using this\ndata. Once the reward model is optimized, along with the reference model, we proceed to train both the policy and\nthe value function. At each prediction step, we compute the sum of the PPO-based loss and update the parameters of\nthe policy. This requires access to the reward model, the reference model, and the value function at hand. At the same\ntime, we update the parameters of the value function by minimizing the MSE loss.\n4.4 Improved Human Preference Alignment 187\n4.4 Improved Human Preference Alignment\nIn the previous section, we reviewed the basic concepts of reinforcement learning and the general\nframework of RLHF. In this section, we will discuss some refinements of RLHF and alternative\nmethods to achieve human preference alignment.\n4.4.1 Better Reward Modeling\nIn Section 4.3.2, we highlighted the task of learning from human preferences as well as the use\nof pairwise ranking loss for training reward models. Here we consider more methods for reward\nmodeling. Our discussion will be relatively general, and since the reward model is widely used in\nmany reinforcement learning problems, it will be easy for us to apply the methods discussed here\nto RLHF and related applications.\n4.4.1.1 Supervision Signals\nThe training of reward models can broadly be seen as a ranking problem, where the model learns\nto assign scores to outputs so that their order reflects the preferences indicated by humans. There\nare several methods to train a reward model from the perspective of ranking.\nIn addition to pairwise and listwise ranking, using pointwise methods to train reward models\noffers an alternative way to capture human preferences. Unlike methods that focus on the relative\nrankings between different outputs, pointwise methods treat each output independently. For ex-\nample, human experts might assign a score to an individual output, such as a rating on a five-point\nscale. The objective is to adjust the reward model so that its outputs align with these scores. A\nsimple way to achieve pointwise training is through regression techniques where the reward of\neach output is treated as a target variable. Let φ(x,y) be the score assigned to y given x by\nhumans. Pointwise reward models can be trained by minimizing a loss function, often based on\nmean squared error or other regression losses, between the predicted rewardr(x,y) and the actual\nhuman feedback φ(x,y). For example, the loss function could be\nLpoint = −E\n[\nφ(x,y) −r(x,y)\n]2 (4.62)\nWhile pointwise methods are conceptually simpler and can directly guide the reward model to\n4.4 Improved Human Preference Alignment 189\npredict scores, they might not always be the best choice in RLHF. A problem is that these methods\nmay struggle with high variance in human feedback, especially when different experts provide\ninconsistent scores for similar outputs. Because they focus on fitting to absolute scores rather than\nrelative differences, inconsistencies in scoring can lead to poor model performance. Moreover,\nfitting to specific scored outputs might discourage generalization, particularly given that training\ndata is often very limited in RLHF. In contrast, methods that consider relative preferences can\npromote the learning of more generalized patterns of success and failure. Nevertheless, there are\nscenarios where pointwise methods might still be suitable. For example, in tasks where training\ndata is abundant and the costs of obtaining accurate, consistent annotations are low, pointwise\nmethods can prove effective.\n4.4.1.2 Sparse Rewards vs. Dense Rewards\nAs discussed in Section 4.3, the rewards in RLHF are very sparse: they are observed only at the\nend of sequences, rather than continuously throughout the generation process. Dealing with sparse\nrewards has long been a concern in reinforcement learning, and has been one of the challenges in\nmany practical applications. For example, in robotics, it often needs to shape the reward function\nto ease optimization rather than relying solely on end-of-sequence rewards. Various methods\nhave been developed to address this issue. One common approach is reward shaping, where the\noriginal function is modified to include intermediate rewards, thereby providing more immediate\nfeedback. Also, one can adopt curriculum learning to sequentially structure tasks in a way that the\ncomplexity gradually increases. This can help models to master simpler tasks first, which prepares\nthem for more complex challenges as their skills develop. There are many such methods that can\nmitigate the impact of sparse rewards, such as Monte Carlo methods and intrinsic motivation. Most\nof these methods are general and the discussion of them can be found in the broader literature on\nreinforcement learning, such as Sutton and Barto [2018]’s book.\n190 Alignment\nrather some form of the accumulated rewards from t until the last time step. Such supervision\nsignals are dense over the sequence, because the reward obtained at the end of the sequence can\nbe transferred back to that time step, regardless of which time step it is. In other words, the sparse\nrewards are transformed into the dense supervision signals. Furthermore, from the perspective of\nreward shaping, Ng et al. [1999] show that the reward at tcan be defined as\nr′(st,at,st+1) = r(st,at,st+1) + f(st,at,st+1) (4.64)\nwhere r′(·) is the transformed reward function, r(·) is the original reward function, and f(·) is\nthe shaping reward function. To ensure the optimality of the policy under the transformed reward\nfunction, the shaping reward function can be given in the form\nf(st,at,st+1) = γΦ(st+1) −Φ(st) (4.65)\nwhere Φ(s) is called the potential value of the state s. If we define Φ(s) as the common value\nfunction as in Eq. (4.15) and substitute Eq. (4.65) into Eq. (4.64), we obtain\nr′(st,at,st+1) = r(st,at,st+1) + γV(st+1) −V(st) (4.66)\nIt is interesting to see that this function is exactly the same as the advantage function used in PPO.\n4.4 Improved Human Preference Alignment 193\nya ≻yb\nPreference\nData\nReward Model\nValue Function\nPolicy\ntraining with MLE Training\nwith PPO\n(a) RLHF (PPO)\nya ≻yb\nPreference\nData\nPolicy\ntraining with MLE\n(b) DPO\nFig. 4.10: Standard RLHF (PPO) vs. DPO. In RLHF, the human preference data is used to train a reward model, which\nis then employed in training the policy as well as the value function. In DPO, the use of human preference data is more\ndirect, and the policy is trained on this data without the need for reward model training.\n4.4.2 Direct Preference Optimization\nAlthough learning reward models is a standard step in reinforcement learning, it makes the entire\ntraining process much more complex than supervised training. Training a reliable reward model\nis itself not an easy task, and a poorly trained reward model can greatly affect the outcome of\npolicy learning. We now consider an alternative alignment method, called direct preference op-\ntimization (DPO), which simplifies the training framework by eliminating the need to explicitly\nmodel rewards [Rafailov et al., 2024]. This method directly optimizes the policy based on user\npreferences, rather than developing a separate reward model. As a result, we can achieve human\npreference alignment in a supervised learning-like fashion. Figure 4.10 shows a comparison of\nthe standard RLHF method and the DPO method.\nBefore deriving the DPO objective, let us first review the objective of policy training used in\nRLHF. As discussed in Section 4.3.3, the policy is typically trained by optimizing a loss function\n194 Alignment\nwith a penalty term. The DPO method assumes a simple loss function where the quality of the\noutput y given the input x is evaluated by the reward modelr(x,y). The training objective is thus\ngiven by\n˜θ = arg min\nθ\nEx∼DEy∼πθ(·|x)\n[\n−r(x,y)  \nloss\n+β(log πθ(y|x) −log πθref (y|x))  \npenalty\n]\n(4.73)\nNote that in this optimization problem, only the term πθ(y|x) depends on the target policy πθ(·).\nThe main advantage of DPO lies in its simplicity and efficiency. The DPO objective is very\nstraightforward — it directly optimizes for preference-based feedback, rather than relying on sep-\narately developed reward models. Moreover, DPO is generally more sample-efficient, as it learns\nfrom a fixed dataset without the need for the computationally expensive sampling process used\nin PPO. This makes DPO a popular method for human preference alignment, especially when\ndeveloping and applying reward models via reinforcement learning is challenging.\n[Coste et al., 2024] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensem-\nbles help mitigate overoptimization. In The Twelfth International Conference on Learning Representa-\ntions, 2024.\n[Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward\ntransformations: Theory and application to reward shaping. InProceedings of the Sixteenth International\nConference on Machine Learning, pages 278–287, 1999.\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduc-\ntion (2nd ed.). The MIT Press, 2018"
    },
    {
        "cluster_id": 70,
        "text": "2.1.5 Prompting LLMs\nWe have so far shown that LLMs can be used to perform various tasks by giving them appropriate\nprompts. There are no restrictions on these prompts, which can include any information we wish\nto ask or communicate with LLMs, such as natural language instructions and the context of con-\nversations. Since this approach requires no additional training or tuning, adapting LLMs becomes\nhighly efficient once they are developed. This somewhat influences the paradigms in NLP: we\nno longer need to develop specific systems for individual tasks but can instead use a single, well-\ntrained LLM to perform different tasks by prompting it. An appealing aspect of LLM prompting\narises as a result: users can easily have “customized” systems by designing their own prompts\nfor LLMs. Given the important role played by prompting in LLMs, prompting engineering has\nbecome a very active area of research in NLP.\nThe term prompt is used in many different ways in the literature. In this chapter, this term\nrefers to the entire input to LLMs, and so we use the terms prompt and model input interchange-\nably. Before discussing prompting further, let us first see a few examples where the prompts\nare more complex than those presented in the previous subsections. Note that this subsection is\nnot aimed at writing high-quality prompts but rather at highlighting some interesting issues in\nprompting LLMs.\nThese examples and previous ones have shown that appropriate responses can be generated\nvia prompts involving clear instructions and questions. However, when problem solving requires\nknowledge that is not explicitly specified, LLMs may make mistakes, even though the instructions\nare sufficiently clear and precise. A family of challenging tasks for LLMs involves arithmetic\nreasoning and commonsense reasoning. For example, we can ask an LLM to solve primary school\nmath problems presented in natural language.\nFurthermore, a special kind of COT, zero-shot COT, introduces instructions that provoke\nLLMs to think, without presenting any intermediate reasoning steps in the prompts [Kojima et al.,\n2022]. An interesting example is that by appending a simple sentence “Let’s think step by step.”\nto the end of a prompt, LLMs can generate reasoning steps to reach the final answer.\nAbove, we have presented examples to illustrate the fundamental in-context learning capa-\nbilities of prompting LLMs. This section, however, does not include more advanced prompting\ntechniques in order to keep the content concise and compact. More discussions on prompting can\nbe found in Chapter 3.\nhttps://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nCHAPTER 3\nPrompting\nIn the context of LLMs,prompting refers to the method of providing an LLM with a specific input\nor cue to generate a desired output or perform a task. For example, if we want the LLM to translate\na sentence from English to Chinese, we can prompt it like this\nTranslate the text from English to Chinese.\nTranslation:\nPrompting is crucial for LLMs because it directly influences how effectively these models under-\nstand and respond to user queries. A well-crafted prompt can guide an LLM to generate more\naccurate, relevant, and contextually appropriate responses. Furthermore, this process can be iter-\natively refined. By analyzing the responses of the LLM, users can adjust their prompts to align\nmore closely with their specific needs. Given the importance of prompting in applying LLMs,\nprompt design has become an essential skill for users and developers working with LLMs. This\nleads to an active research area, calledprompt engineering, in which we design effective prompts\nto make better use of LLMs and enhance their practical utility in real-world applications.\nAn important concept related to prompting is in-context learning. When prompting an LLM,\nwe can add new information to the context, such as demonstrations of problem-solving. This\nallows the LLM to learn from this context how to solve the problem. Here is an example of\nprompting LLMs with a few demonstrations of how to classify text based on sentiment polarity.\nIn this chapter, we focus on prompting techniques in LLMs. We begin by considering several\ninteresting prompt designs commonly used in prompt engineering. Then, we discuss a series of\n3.1 General Prompt Design 97\nrefinements to these methods. Finally, we explore approaches for automating prompt design.\n3.1 General Prompt Design\nThis section presents basic concepts in prompt design, along with examples of how to prompt\nLLMs for various NLP tasks. Since the effectiveness of prompting is highly dependent on the\nLLMs being used, prompts often vary across different LLMs, making it difficult to provide a\ncomprehensive list of prompts for all LLMs and downstream tasks. Therefore, this discussion is\nnot focused on any specific LLM. Instead, the goal is to provide guiding principles for prompt\ndesign.\nFor example, we can describe a conversation between two people, named John and David, and use\nthe LLM to continue the conversation. A template of such prompts is given by\nJohn: {∗utterance1∗}\nDavid: {∗utterance2∗}\nJohn: {∗utterance3∗}\nDavid: {∗utterance4∗}\nJohn: {∗utterance5∗}\nDavid: {∗utterance6∗}\nJohn: {∗utterance7∗}\nDavid:\nThe “name:content” format can be used to define the task that we want the LLM to perform.\n3.1.3 Prompt Engineering Strategies\nDesigning prompts is highly empirical. In general, there are many ways to prompt an LLM for\nperforming the same task, and we need to perform a number of trial-and-error runs to find a\nsatisfactory prompt. To write good prompts more efficiently, one can follow certain strategies.\nA common example is that well-developed LLMs have achieved impressive performance\nin mathematical reasoning tasks, which are considered challenging. In prompt engineering,\nthe “thinking” ability of LLMs needs to be activated through appropriate prompting, espe-\ncially for problems that require significant reasoning efforts. In many cases, an LLM that\nis instructed to “think” can produce completely different results compared with the same\nLLM that is instructed to perform the task straightforwardly. For example, Kojima et al.\n[2022] found that simply appending “Let’s think step by step” to the end of each prompt\ncan improve the performance of LLMs on several reasoning tasks. LLMs can be prompted\nto “think” in a number of ways. One method is to instruct LLMs to generate steps for rea-\nsoning about the problem before reaching the final answer. For example, consider a task of\nsolving mathematical problems. See below for a simple prompt for this task.\n{∗problem∗}\nAnother method to guide LLMs to “think” is through multiple rounds of interaction with\nLLMs. For example, as a first step, we can instruct LLMs to solve the problem directly\nYou will be provided with a math problem. Please solve the problem.\nProblem: {∗problem∗}\nSolution: {∗solution∗}\nThe prompts presented here are closely related to a long line of research on reasoning prob-\nlems in LLMs. It is impossible to provide a complete discussion of all related issues because\nthis topic covers a large family of methods. But we will see a relatively more detailed dis-\ncussion on how to improve prompting through more reasoning in Section 3.2.\n106 Prompting\n3.1.4 More Examples\nIn this subsection, we consider more examples of prompting LLMs to perform various NLP tasks.\nThe motivation here is not to give standard prompts for these tasks, but rather to use simple\nexamples to illustrate how LLMs can be prompted to deal with NLP problems.\n3.2 Advanced Prompting Methods\nSo far in this chapter, we have introduced the basic concepts related to LLM prompting and pre-\nsented a number of prompts for NLP tasks. We now consider several techniques for enhancing the\neffectiveness of prompting.\nFollowing the instruction “Let’s think step by step”, the LLM is prompted to generate detailed\nreasoning steps. As discussed in Kojima et al. [2022]’s work, prompting with such instructions\nmay result in LLMs generating only the reasoning steps without a clear conclusion. In this case,\na second round of prompting can be used to extract the answer from these reasoning steps. For\nexample, Kojima et al. [2022] create a second prompt which combines both the input and output\n3.2 Advanced Prompting Methods 117\nin the first round of prompting. Using this combined input, the LLM can continue its reasoning\nprocess and then generate the correct answer. Furthermore, it is possible to prompt LLMs to\nreason using instructions other than “Let’s think step by step”, such as “Let’s think logically” and\n“Please show me your thinking steps first”.\n3.2 Advanced Prompting Methods 127\nSince the focus of this chapter is on LLM prompting, we consider using LLMs to provide\nfeedback. To illustrate, let us consider an example of generating good responses to user questions:\n• First, we prompt an LLM to answer the input question, as usual.\n• Given a prompt, we use LLMs to generate a number of similar prompts.\n3.3 Learning to Prompt\nSo far in this chapter, we have considered several basic prompting strategies and various refine-\nments to them. However, all the prompts we have discussed were designed manually. This leads\nto a number of problems: First, designing high-quality prompts is inherently difficult and requires\nsubstantial manual effort. For example, extensive experimentation with different prompts is often\nneeded to identify the most effective ones. Since different LLMs may respond better to certain\n3.3 Learning to Prompt 139\ntypes of prompts, developing universally effective prompts can be even more resource-intensive.\nSecond, manual prompt design relies heavily on human expertise, which can limit the diversity\nof approaches and overlook potentially effective prompts that are not immediately obvious to hu-\nmans. Third, prompts created by humans can be complex and redundant, leading to longer inputs\nfor LLMs and higher computational costs.\n• How can we automate the process of designing and optimizing prompts for LLMs?\n• Are there other forms of representing prompts beyond strings, and how can we learn such\nrepresentations?\n• How can we make prompts more concise and compact, thereby reducing their complexity\nand length?\nNote that there are many settings in which we can investigate these issues. For example, we\nmight specify that prompts are developed specifically for a particular LLM, or that the develop-\nment is independent of the LLM used. These settings can lead to different methods and application\nscenarios, but these methods may overlap in some ways. In the following discussion, we will cover\nseveral different scenarios and discuss the connections between various methods.\nThis is a very general framework, and different prompt optimization systems can vary in their\ndesign of each component. A widely-used approach is to use LLMs as the basis to develop these\ncomponents. Initially, a few prompts are provided. Then, the following process is iterated until\na stopping criterion is met: 1) the prompts are evaluated on a validation set; 2) a candidate pool\nis maintained by keeping only the most promising prompts; and 3) new prompts are created by\nemploying LLMs to infer similar prompts from this candidate pool. One benefit of this approach is\nthat it allows us to use off-the-shelf LLMs to perform the tasks mentioned above without the need\nfor substantial system development. To achieve this, we can prompt or fine-tune LLMs to adapt\nthem to these tasks. Here we consider Zhou et al. [2023c]’s method for illustrating LLM-based\nprompt optimization. It involves the following steps.\n• Initialization. Let Crepresent the pool of the candidate prompts we intend to explore. The\nfirst step is to add initial prompts intoC. We can do this in several ways. A simple method is\nto create such prompts by hand for a given task. However, in many cases where humans have\nlimited knowledge about how to write effective prompts for the task, developing prompts\nbecomes challenging. In these cases, it is desirable to use LLMs to generate prompts.\nNote that, in our discussion here, prompts are simply seen as sequences of tokens, and the out-\nput of prompt optimization is such a sequence. However, in a strict sense, prompts have complex\nstructures and include different fields such as user input, instruction, and demonstration. While\nour discussed approaches are mostly general, much work in prompt optimization has focused on\nlearning better instructions for prompting. Specifically, the goal is to generate instructions that\neffectively guide LLMs based on a given task. Of course, the concept of prompt optimization\ncan also be extended to learning other parts of prompts. For example, there has been substantial\nresearch interest in learning to select or generate demonstrations in CoT [Liu et al., 2022; Rubin\net al., 2022; Zhang et al., 2023b]. One of the differences between learning instructions and learn-\ning demonstrations is that generating high-quality demonstrations using LLMs is relatively easy\nand the focus of learning demonstrations is typically on how to sample appropriate demonstra-\ntions from a pool of candidates. In contrast, the difficulty in learning instructions is partly because\npre-trained LLMs are not suited to predict the quality of instructions, and testing these instructions\non downstream tasks is computationally expensive. This makes the optimization methods costly\nto apply, and exploring a wide variety of instructions poses significant challenges.\n3.3.2 Soft Prompts\nAlthough developing natural language prompts, either manually or automatically, is a straight-\nforward and widely applied approach, it presents some problems. One problem is that natural\nlanguage prompts can be complex and lengthy, resulting in significant computational burdens\nwhen processed via LLMs. In many applications, users may need to perform a task repeatedly,\nand inputting the same long prompt into the LLMs a large number of times is clearly inefficient.\nAnother problem is that while prompts are typically represented as discrete token sequences (call\nthem hard prompts) in regular LLM input, the LLMs encode them as low-dimensional real-\nvalued vectors. This raises the question of whether there are more compact and efficient ways to\nrepresent prompts.\nIn this subsection, we introduce the concept of soft prompts, which can be viewed as hidden,\ndistributed representations of prompts. When prompting LLMs, we are concerned with commu-\nnicating tasks or questions to elicit the desired responses. We can define hard prompts as explicit,\npredefined text sequences that users input directly into LLMs to guide the responses. In contrast,\nwe can think of soft prompts as implicit, adaptable prompting patterns embedded within LLMs.\nWhile the above example shows that soft prompts can be generated by transforming hard\nprompts, there is not necessarily a direct correspondence between them. In fact, we do not even\nneed to interpret soft prompts using meaningful text. They are instead simply hidden states in\nLLMs and can be learned as standard parameters of the models through continuous optimiza-\ntion. Such a treatment allows us to explore prompting methods beyond text. As another benefit,\nsoft prompts provide dense, low-dimensional, and learnable representations for encoding how\nwe guide LLMs to generate specific outputs. The training and application of these representa-\ntions require significantly lower computational costs than those required for processing long hard\nprompts. This approach would be of great practical value in LLM inference applications where\nthe same prompt is repeatedly used.\n3.3 Learning to Prompt 145\nFull Context User Input+ Prt(y|c,z)\nc z y\nTeacher Model:\nSimplified Context User Input+ Prs(y|c′,z)\nc′ z y\nStudent Model:\nLoss\nFig. 3.4: Illustration of context distillation [Snell et al., 2022]. The teacher model is a standard LLM, which takes both\nthe context and the user input as model input and produces a prediction as model output. Then, we simplify the context\n(e.g., simplifying the instruction in prompting) and use the student model to make predictions based on the simplified\ncontext and the user input. The student model is trained by minimizing the loss between the predictions produced by\nthe two models.\nHere we have considered methods for inserting soft prompts in LLMs. But we skip the details\nof training these soft prompts and assume that the reader is familiar with the standard supervised\nlearning process, that is, maximizing the likelihood of the correct model output given the model\n3.3 Learning to Prompt 149\nLayer l−1\nLayer l\nLayer l+ 1\n··· ··· ··· ··· ··· ··· ···\np0 p1 e0 e1 e2 e3 e4\n··· ··· ··· ··· ··· ··· ···\nLoss Loss\nLook out ! 小心 !\ntrainable prompt\nembeddings\nUser Input LLM PredictionSoft Prompt\nFig. 3.6: Illustration of prompt tuning for a translation task (Look out! → 小心!). Instead of using fixed textual\nprompts, soft prompts are learnable embeddings that are added at the beginning of the embedding sequence. During\nfine-tuning, only these prompt embeddings are optimized to efficiently adapt the LLM to the given task. Once opti-\nmized, the prompt embeddings are used to instruct the LLM to perform the task as new data arrives.\ninput. In fact, learning soft prompts can be related to many issues in LLM fine-tuning. For exam-\nple, if we consider it as a context compression problem, we can apply the knowledge distillation\nmethods described previously. In Mu et al. [2024]’s work, prompts are compressed and repre-\nsented as a few pseudo tokens, which are appended to each input sequence. The embeddings of\nthese pseudo tokens are optimized to mimic the predictions of a standard-prompted model. In\nother words, the prompting knowledge is distilled from a teacher model into the pseudo tokens.\nWhile compressing full context into continuous representations is a straightforward approach\nto learning soft prompts, it requires a teacher model that can deal with long input sequences. In\nmany cases, however, the context is so long that applying an LLM is too costly or infeasible.\n152 Prompting\n3.3.3 Prompt Length Reduction\nWhile soft prompts provide dense, hidden representations, they are not directly interpretable. The\nlack of interpretability can be a significant barrier for users trying to understand how their inputs\ninfluence LLM outputs. Moreover, although soft prompts are efficient for fine-tuning and de-\nployment, they are inflexible and do not allow for easy adjustments without extensive fine-tuning\nor modification. This inflexibility can limit their utility in dynamic environments where prompt\nchanges are frequently needed.\n3.4 Summary\nIn this chapter, we have discussed a variety of issues related to LLM prompting. Our discussion\nhas focused mainly on two aspects:\n• How to design basic prompts to guide the predictions of LLMs and refine these prompts for\nmore effective and efficient problem-solving?\n• How to automate the design and representation of prompts?\nSolutions to these issues involve both general prompt designs and more advanced techniques, such\nas CoT and prompt learning, which have been explored extensively in recent research.\nIn NLP, prompting can be viewed as a technology that has evolved along with LLMs, and\nin a sense, it has opened the door to the practical application of these models in an impressive\nrange of problem domains. In fact, if we expand the concept of prompts to some extent, it can be\ntraced back to the early days of machine learning and NLP. For example, many NLP systems use\nhand-crafted features and templates to “prompt” specific tasks. Imagine developing a feature to\nindicate whether a text is formal or informal. We can feed this feature into a machine translation\nsystem to condition the translation on the type of the input text.\nPrompting approaches were first experimented with smaller models and later demonstrated\nimpressive capabilities with large models like GPT-3, which could generate high-quality text in\nresponse to simple prompts across various tasks. As prompting technology evolved, prompt en-\ngineering emerged as a critical area of research. As discussed in this chapter, it broadly involves\ndesigning effective prompts to maximize model performance, encompassing both hand-crafted\nand automatically generated prompts. More recent research has explored how to enhance the ef-\nfectiveness of prompting through techniques like few-shot learning, zero-shot learning, and CoT\nreasoning, enabling LLMs to work effectively across a wide range of scenarios. A general dis-\ncussion of prompting can be very broad, and we cannot cover all details in this chapter. For\nmore advanced techniques of prompting, the reader can refer to recent surveys. Topics include\nin-context learning [Li, 2023; Dong et al., 2022], CoT [Chu et al., 2023; Yu et al., 2023; Zhang\net al., 2023a], efficient prompting [Chang et al., 2024], and general prompt engineering [Liu et al.,\n2023c; Chen et al., 2023a].\n154 Prompting\nNote that although we would ideally like to develop general prompting methods without ad-\njusting model architectures and parameters, the results of prompting generally depend heavily\non the quality and size of the given LLMs. For stronger models, such as commercialized online\nLLMs, simple prompts may be sufficient to instruct these models to perform tasks correctly. In\nthis case, prompt engineering is relatively easy, though we still need certain efforts to make LLMs\nwork properly. By contrast, if the LLMs are not powerful enough, we may need to carefully design\nthe prompts to achieve the desired results. In many cases, fine-tuning is still necessary to adapt\nthe models to sophisticated prompting strategies"
    },
    {
        "cluster_id": 19,
        "text": "Where was it held?\nThe 2022 FIFA World Cup was held in Qatar.\nWhere will the 2028 Olympics be held?\nWe can simply input this question into an online search engine. It will then return the relevant\npieces of text found on the internet, for example,\n(Wikipedia)\nThe 2028 Summer Olympics, officially the Games of the XXXIV Olympiad and com-\nmonly known as Los Angeles 2028 or LA28, is an upcoming international multi-sport\nevent scheduled to take place from July 14-30, 2028, in the United States. ...\n(The Sporting News)\nIn 2028, Los Angeles will become the third city, following London and Paris respectively,\nto host three Olympics after hosting the Summer Games in 1932 and 1984. It will also\nbe the first time the United States has hosted an Olympic Games since the 2002 Winter\nGames in Salt Lake City. ...\nQuestion:\nWhere will the 2028 Olympics be held?\nRelevant Text 1:\nThe 2028 Summer Olympics, officially the Games of the XXXIV Olympiad and\ncommonly known as Los Angeles 2028 or LA28 ...\nRelevant Text 2:\nIn 2028, Los Angeles will become the third city, following London and Paris\nrespectively, to host three Olympics after ...\nThe 2028 Olympics will be held in Los Angeles.\nQuestion:\nWhere will the 2028 Olympics be held?\nRelevant Text 1:\nThe 2024 Summer Olympics, officially the Games of the XXXIII Olympiad and\nbranded as Paris 2024, were an international multi-sport event .."
    },
    {
        "cluster_id": 33,
        "text": "How many times has Argentina won the World Cup?\nArgentina has won the FIFA World Cup three times.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nDEMO You will be provided with a sentence in English. The task is\nto output the correct sentence.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nDEMO1 You will be provided with a sentence in English. The task is\nto output the correct sentence.\nDEMO2 You will be provided with a sentence in English. The task is\nto output the correct sentence.\nUSER You will be provided with a sentence in English. The task is\nto output the correct sentence.\nTranslate the following text from English to Spanish.\nEl rápido zorro marrón salta sobre el perro perezoso.\nRewrite this text in a formal tone.\nMake this text simpler.\n{∗text∗}\nCondense and simplify this text"
    },
    {
        "cluster_id": 23,
        "text": "...\n...\n...\n...\n.."
    },
    {
        "cluster_id": 25,
        "text": "Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. How many apples\ndoes Jack have left in the end?\nThe answer is 10.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. How many apples\ndoes Jack have left in the end?\nThe answer is 12.\nJack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. How many apples\ndoes Jack have left in the end?\nJack starts with 7 apples. He eats 2 apples for dinner, so we subtract 2 from 7,\nleaving him with 5 apples. His mom gives him 5 more apples, adding 5 to his\ncurrent total. Now he has 5 + 5 = 10 apples. The next day, Jack gives 3 apples to\nhis friend John. Subtracting 3 from 10, Jack is left with 7 apples. Therefore, Jack\nhas 7 apples left. So the answer is 7.\n2.1 A Brief Introduction to LLMs 55\nJack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more\napples. The next day, Jack gave 3 apples to his friend John. How many apples\ndoes Jack have left in the end?\nLet’s think step by step.\n1. Initial Quantity: Jack starts with 7 apples.\n2. After Dinner: He eats 2 apples, so 7 −2 = 5 apples remain.\n3. His Mom Gives More: His mom gives him 5 more apples, so 5 + 5 = 10\napples are now with him.\n4. Giving to John: The next day, Jack gives 3 apples to his friend John,\nso 10 −3 = 7 apples are left.\nIn the end, Jack has 7 apples left.\nDEMO Q: Jerry’s two daughters play softball on different teams. They each have\n8 games this season. Each team practices 4 hours for every game they\nplay. If each game lasts for 2 hours, how many hours will Jerry spend at\nthe field watching his daughters play and practice altogether?\nA: Jerry will spend 8 games * 2 hours per game = ≪8 ∗2 = 16 ≫16\nhours watching one daughter play her games. He will spend 16 ∗2 =≪\n16 ∗2 = 32 ≫32 hours watching both daughters play their games. He\nwill spend 8 games * 4 hours of practice = ≪8 ∗4 = 32 ≫32 hours\nwatching one daughter practice. He will spend32∗2 =≪32∗2 = 64 ≫\n64 hours watching both daughters practice. He will spend a total of 32\nhours watching games + 64 hours watching practice =≪32 + 64 =\n96 ≫96 hours. #### 96\nQ: Mary bought six apples from the store. From the apples she bought,\nfor each that Mary ate, she planted two trees from the remaining ones.\nHow many apples did Mary eat?\nA: She planted eight trees. This means she used half of that amount of\napples, which is 8 trees /2 trees/apple =≪8/2 = 4 ≫4 apples. That\nmeans that she planted four of the six apples she bought, leaving only 6\napples −4 apples =≪6 −4 = 2 ≫2 apples to be eaten. #### 2\nUSER Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. If Boris\ngives Beck 10 apples, how many fewer apples does Beck have than Boris\nnow?\nA: Boris starts with 100 apples and gives Beck 10 apples, leaving\nhim with 100 apples −10 apples =≪100 −10 = 90 ≫90 apples"
    },
    {
        "cluster_id": 24,
        "text": "Tom has 12 marbles. He wins 7 more marbles in a game with his friend but then\nloses 5 marbles the next day. His brother gives him another 3 marbles as a gift.\nHow many marbles does Tom have now?\nThe answer is 17.\n2. He wins 7 more marbles in a game, so we add those to his initial count: 12 marbles + 7\nmarbles = 19 marbles.\n3. The next day, Tom loses 5 marbles, so we subtract those from his current total: 19 marbles\n- 5 marbles = 14 marbles.\n4. His brother gives him 3 more marbles, so we add those to his current total: 14 marbles + 3\nmarbles = 17 marbles.\n5. After all these events, Tom has 17 marbles.\nTom has 12 marbles. He wins 7 more marbles in a game with his friend but then\nloses 5 marbles the next day. His brother gives him another 3 marbles as a gift.\nHow many marbles does Tom have now?\nTom initially has 12 marbles. After winning a game, he gains 7 more, increasing\nhis total to 19 marbles. He loses 5 marbles the next day, bringing his total down\nto 14 marbles. His brother gifts him 3 more marbles, increasing his total to 17\nmarbles. Therefore, Tom now has 17 marbles. So the answer is 17"
    },
    {
        "cluster_id": 68,
        "text": "2.2 Training at Scale\nAs a first step in developing LLMs, we need to train these models on large amounts of data.\nemergent abilities of LLMs. For example, Wei et al. [2022b] studied the scaling properties of\nLLMs across different model sizes and amounts of computational resources. Their work shows\nthat some abilities emerge when we scale the model size to certain level. The appearance of\nemergent abilities has demonstrated the role of scaled training in enhancing the performance of\nLLMs, and it has also, to some extent, motivated researchers to continuously attempt to train larger\nmodels. As larger and stronger LMs continue to appear, our understanding of the scaling laws\ncontinues to mature. This helps researchers predict the performance of LLMs during training and\nestimate the minimal computational resources required to achieve a given level of performance.\n5.3.4.4 Training-based Scaling\nAs well as considering inference-time scaling methods without training, we also wish to consider\nmethods that can improve intrinsic reasoning capabilities of LLMs by modifying their parameters\nthrough further training. While such training-based scaling methods typically require additional\ntraining cost and computational resources, they instill stronger reasoning skills directly into the\nmodel parameters, which in turn can lead to more effective and efficient reasoning performance.\nThe primary advantage of these training-based scaling methods is that they endow the LLM\nwith stronger inherent reasoning skills. This directly contributes to improved inference-time scal-\ning in several ways: it can lead to more efficient inference, as the LLM might require less extensive\nsearch or fewer generation samples to arrive at a correct solution. Moreover, the base quality of\ngenerated steps or solutions is higher. Therefore, a well-trained LLM might generalize its learned\nreasoning abilities to novel problems more effectively than an LLM relying solely on in-context\nlearning or training-free inference schemes"
    },
    {
        "cluster_id": 49,
        "text": "• Data Parallelism. This method is one of the most widely used parallelism methods for\ntraining neural networks. To illustrate, consider the simplest case where the standard delta\nrule is used in gradient descent\nθt+1 = θt −lr·∂Lθt(Dmini)\n∂θt\n(2.32)\n2.2 Training at Scale 61\nwhere the new parameters θt+1 is obtained by updating the latest parametersθt with a small\nstep lr in the direction of the negative loss gradient. ∂Lθt(Dmini)\n∂θt is the gradient of the loss\nwith respect to the parameters θt, and is computed on a minibatch of training sampleDmini.\nThen, we distribute these batches to N workers, each with a corresponding batch. Once\nthe data is distributed, these workers can work at the same time. The gradient of the entire\nminibatch is obtained by aggregating the gradients computed by the workers, like this\n∂Lθt(Dmini)\n∂θt\n= ∂Lθt(D1)\n∂θt  \nworker 1\n+ ∂Lθt(D2)\n∂θt  \nworker 2\n+ ··· + ∂Lθt(DN)\n∂θt  \nworker N\n(2.33)\nIn ideal cases where the workers coordinate well and the communication overhead is small,\ndata parallelism can achieve nearly an N-fold speed-up for training.\nThe ultimate goal of parallel processing is to achieve linear growth in efficiency, that is, the\nnumber of samples that can be processed per unit of time increases linearly with the number of\ndevices. However, distributed training is complicated, and influenced by many factors in addition\nto the parallelism method we choose. One problem, which is often associated with distributed\nsystems, is the cost of communication. We can think of a distributed system as a group of net-\nworked nodes. Each of these nodes can perform local computation or pass data to other nodes. If\n2.2 Training at Scale 63\nthere are a large number of such nodes, it will be expensive to distribute and collect data across\nthem. Sometimes, the time savings brought about by parallelism are offset by the communica-\ntion overhead of a large network. Another problem with large-scale distributed systems is that\nthe synchronization of nodes introduces additional costs. As is often the case, some nodes may\ntake longer to work, causing others to wait for the slowest ones. While we can use asynchronous\ntraining to handle heterogeneity in computational resources, this may lead to stale gradients and\nnon-guaranteed convergence. Moreover, as more nodes are added to the network, there is more\nchance to have crashed nodes during training. In this case, we need to ensure that the whole\nsystem is fault tolerant. In many practical settings, to increase scalability, one needs to take into\naccount additional issues, including architecture design, data transfer and computation overlap,\nload balancing, memory bandwidth and so on"
    },
    {
        "cluster_id": 50,
        "text": "Here we use Pr(y|x) to denote the text generation probability where x is the context and y is the\ngenerated text. There are broadly three types of long sequence modeling problems.\n• Text generation based on long context (i.e., x is a long sequence). For example, we\ngenerate a short summary for a very long text.\n• Long text generation (i.e., y is a long sequence). For example, we generate a long story\nbased on a few keywords.\n• Long text generation based on long context (i.e., both x and y are long sequences). For\nexample, we translate a long document from Chinese to English.\n3.1.4.3 Text Generation\nIn NLP, the termtext generation is used broadly to cover a wide range of different problems, many\nof which focus on generating text based on given text. These problems can be roughly categorized\ninto two classes: 1) continual writing based on the input text (call it text completion), and 2)\ntransformation of the input text into another text (call it text transformation). Here, we illustrate\nthe prompts for the former and consider the prompts for the latter in the following discussion.\n• Repetition Penalty. A repetition penalty discourages the model from generating repetitive\nor redundant text. The penalty function might measure the frequency of repeated tokens or\nphrases in the generated sequence and impose a penalty proportional to their occurrence"
    },
    {
        "cluster_id": 55,
        "text": "• The first explores efficient training methods and model architectures to learn self-attention\nmodels from long-sequence data.\nGiven the attention weights {αi,j}, we then compute the attention results using Eq. (2.40).\n2.3 Long Sequence Modeling 69\nAs a consequence, we can prune most of the attention weights and represent the attention model\nin a compressed form. To illustrate, consider the self-attention model\nAttqkv(Q,K,V) = α(Q,K)V (2.45)\nwhere the attention weight matrix α(Q,K) ∈Rm×m is obtained by\nα(Q,K) = Softmax( QKT\n√\nd\n+ Mask)\n=\n\n\nα0,0 0 0 ... 0\nα1,0 α1,1 0 ... 0\nα2,0 α2,1 α2,2 ... 0\n... ... ... ... ...\nIn the original version of self-attention\n[\nαi,0 ... αi,i\n]\nis assumed to be dense, that is, most of\nthe values are non-zero. In sparse attention, some of the entries of\n[\nαi,0 ... αi,i\n]\nare considered\nnon-zero, and the remaining entries are simply ignored in computation. Suppose G⊆{0,...,i }is\nthe set of indices of the non-zero entries. For language models, the output of the sparse attention\nmodel at position iis given by\nAttsparse(qi,K≤i,V≤i) =\n∑\nj∈G\nα′\ni,jvj (2.48)\nHere {α′\ni,j}are normalized over G. Hence their values are different from the original attention\nweights (in fact we have α′\ni,j >αi,j). The sparsity of the model is determined by how large Gis.\nSparse attention models differ in the way we define G. One simple approach is to define Gbased\non heuristically designed patterns. For example, a widely-used pattern involves having Gcover a\nwindow of tokens located near position i[Parmar et al., 2018].\nWhile sparse attention reduces the computation through the use of sparse operations, such\nmodels still have significant limitations as we must keep the entire KV cache (i.e., K≤i and V≤i)\n70 Generative Models\nduring inference. If the sequence is very long, storing this cache will become highly memory-\nintensive. To address this, we can consider a different form of attention models where the KV\ncache is not explicitly retained. Linear attention is one such approach [Katharopoulos et al.,\n2020]. It uses a kernel function ϕ(·) to project each query and key onto points q′\ni = ϕ(qi)\nand k′\ni = ϕ(ki), respectively. By removing the Softmax function under such transformations 10,\nthe form of the resulting attention model is given by\nAttqkv(qi,K≤i,V≤i) ≈ Attlinear(q′\ni,K′\n≤i,V≤i)\n= q′\niµi\nq′\niνi\n(2.49)\nwhere µi and νi are variables that are computed in the recurrent forms\nµi = µi−1 + k′T\ni vi (2.50)\nνi = νi−1 + k′T\ni (2.51)\nµi and νi can be seen as representations of the history up to position i. A benefit of this model is\nthat we need not keep all past queries and values. Instead only the latest representations µi and\nνi are used. So the computational cost of each step is a constant, and the model can be easily\nextended to deal with long sequences.\n2.3 Long Sequence Modeling 71\nqikiki−1ki−2···k1k0\nvivi−1vi−2···v1v0\nAttqkv(qi,K≤i,V≤i)\n(a) Standard Self-attention\nqikiki−1ki−2···k1k0\nvivi−1vi−2···v1v0\nAttqkv(qi,{k1,ki},{v1,vi})\n(b) Sparse Attention\nqikiki−1ki−2···k1k0\nvivi−1vi−2···v1v0\nνi = νi−1 + k′T\ni\nµi = µi−1 + k′T\ni vi\nνi\nµi⇒\n⇒\nAttlinear(qi,K≤i,V≤i) = q′\niµi\nq′\niνi\n(c) Linear Attention\nhihi−1hi−2hi−3···h1h0\ninputi\nhi = f(hi−1,inputi)\n(d) Recurrent Models\nFig. 2.5: Illustrations of self-attention, sparse attention, linear attention and recurrent models. Blue boxes = cached\nstates for producing the output at position i. f(·) = a recurrent cell.\n2.3 Long Sequence Modeling 73\nIn general, the cumulative average can be written using a recursive formula\nMemi = (ki,vi) + i·Memi−1\ni+ 1 (2.57)\nwhere Memi and Memi−1 denote the cumulative averages of the current and previous po-\nsitions, respectively. An advantage of this model is that we only need to store a single\nkey-value pair during inference, rather than storing all the key-value pairs. Note that the\nabove memory models are related to recurrent models, and more advanced techniques have\nbeen used to develop alternatives to self-attention mechanisms in Transformers [Ma et al.,\n2023].\n• We have already seen that both global and local contexts are useful and can be modeled\nusing attention models. This view motivates the extension to attention models for combining\nboth local and long-term memories [Ainslie et al., 2020; Zaheer et al., 2020; Gupta and\nBerant, 2020]. A simple but widely-used approach is to involve the first few tokens of\nthe sequence in attention, serving as global tokens. This approach is usually applied along\nwith other sparse attention models. An advantage of incorporating global tokens of the\nsequence is that it helps smooth the output distribution of the Softmax function used in\nattention weight computation, and thus stabilizes model performance when the context size\nis very large [Xiao et al., 2024]. One drawback, however, is that using a fixed-size global\nmemory may result in information loss. When dealing with long sequences, we need to\nenlarge the KV cache for sufficient representations of the context, but this in turn increases\nthe computational cost.\n[2021]. It linearly combines the two types of attention, given by\nAtt(qi,Mem,Memknn) = g ⊙Attlocal + (1 −g) ⊙Attknn (2.64)\nAttlocal = Att( qi,Mem) (2.65)\nAttknn = Att( qi,Memknn) (2.66)\nHere g ∈Rd is the coefficient vector, which can be the output of a learned gate.\nGiven the k-NN-based memory model described above, the remaining task is to determine\nwhich key-value pairs are retained in the datastore. For standard language modeling tasks, we\nconsider the previously seen tokens in a sequence as the context, so we can add the keys and\nvalues of all these tokens into the datastore. In this case, the resulting k-NN-based attention\nmodel is essentially equivalent to a sparse attention model [Gupta et al., 2021].\n2.3.5.1 Attention with Learnable Biases\nOne problem with Eq. (2.74) is that the embedding model treats each token independently and\ntherefore ignores the distance between different tokens. A common improvement to this model,\ncalled relative positional embedding, is to consider the pairwise relationship between tokens\n[Shaw et al., 2018]. The general idea behind this is to obtain the offset between any pair of posi-\ntions and incorporate it into the self-attention model. One of the simplest forms of self-attention\nwith relative positional embedding is given by\nAttqkv(qi,K≤i,V≤i) =\ni∑\nj=0\nα(i,j)vj (2.75)\nα(i,j) = Softmax( qikT\nj + PE(i,j)√\nd\n+ Mask(i,j)) (2.76)\nThe only difference between this model and the original self-attention model is that a bias term\nPE(i,j) is added to the query-key product in this new model. Intuitively, PE(i,j) can be inter-\npreted as a distance penalty for the pair of positionsiand j. As imoves away from j, the value of\n84 Generative Models\nPE(i,j) decreases.\n2.3.5.2 Attention with Non-learned Biases\nRelative positional embedding models are based on a set of learned biases for the query-key prod-\nuct in self-attention. An alternative approach is to give these biases fixed values via heuristics,\nrather than training them on a particular dataset. One benefit of this heuristics-based approach is\nthat it does not rely on a training process and thus can be directly applied to any sequences once\nthe biases are set.\nOne example of such an approach is Press et al. [2022]’s approach, called attention with\nlinear biases or ALiBi for short. In the ALiBi approach, the bias term is defined as the negative\nscaled query-key offset\nPE(i,j) = −β·(i−j)\n= β·(j−i) (2.82)\nwhere βis the scaling factor. Adding this term to the query-key product, we obtain a new form of\n16Note that, in Raffel et al. [2020]’s T5 model, the rescaling operation for the query-key product is removed. The\nattention weight α(i,j) is then given by\nα(i,j) = Softmax( qikT\nj + ub(i−j) + Mask(i,j)) (2.80)\n86 Generative Models\nq0kT\n0\nq1kT\n0 q1kT\n1\nq2kT\n0 q2kT\n1 q2kT\n2\nq3kT\n0 q3kT\n1 q3kT\n2 q3kT\n3\nq4kT\n0 q4kT\n1 q4kT\n2 q4kT\n3 q4kT\n4\nq5kT\n0 q5kT\n1 q5kT\n2 q5kT\n3 q5kT\n4 q5kT\n5\nq6kT\n0 q6kT\n1 q6kT\n2 q6kT\n3 q6kT\n4 q6kT\n5 q6kT\n6\nqikT\nj\nu0\nu1 u0\nu2 u1 u0\nu2 u2 u1 u0\nu3 u2 u2 u1 u0\nu3 u3 u2 u2 u1 u0\nu3 u3 u3 u2 u2 u1 u0\nBias (ub(i−j))\n+\n(a) The T5 bias (nb = 3 and distmax = 5)\nq0kT\n0\nq1kT\n0 q1kT\n1\nq2kT\n0 q2kT\n1 q2kT\n2\nq3kT\n0 q3kT\n1 q3kT\n2 q3kT\n3\nq4kT\n0 q4kT\n1 q4kT\n2 q4kT\n3 q4kT\n4\nq5kT\n0 q5kT\n1 q5kT\n2 q5kT\n3 q5kT\n4 q5kT\n5\nq6kT\n0 q6kT\n1 q6kT\n2 q6kT\n3 q6kT\n4 q6kT\n5 q6kT\n6\nqikT\nj\n0\n−1β 0\n−2β −1β 0\n−3β −2β −1β 0\n−4β −3β −2β −1β 0\n−5β −4β −3β −2β −β 0\n−6β −5β −4β −3β −2β −β 0\nBias (−β(i−j))\n+\n(b) The ALiBi bias\nFig. 2.11: Query-key products with biases (above = the T5 bias and below = the ALiBi bias). The color scale of the\nbiases ranges from light blue denoting small absolute values to deep blue denoting large absolute values.\nEach time, the model attends position i′to all previous positions, which results in 2i′vector\nproducts (i′ times for qi′KT and i′ times for the product of Softmax(qi′KT\n√\nd ) and V). Hence,\ngenerating a sequence of length lenhas a time complexity of O(L×len2) for the self-attention\nnetwork. Clearly, the inference of this model is slow for long sequences due to its quadratic\ntime complexity with respect to sequence length. Therefore, many improvements to Transformers\nand alternative models have focused on efficient methods that are faster than this quadratic time\ncomplexity, such as sparse attention mechanisms and linear-time models. A detailed discussion\nof efficient Transformers can be found in the previous chapters, and this section will focus on the\n5.1 Prefilling and Decoding 207\nstandard Transformer architecture.\n[Bulatov et al., 2022] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.\n[Dong et al., 2021] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you\nneed: Pure attention loses rank doubly exponentially with depth. InInternational Conference on Machine\nLearning, pages 2793–2803. PMLR, 2021.\n[Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam\nNeyshabur. Block-recurrent transformers. Advances in neural information processing systems , 35:\n5.4 Summary 255\n33248–33261, 2022.\nTransformers are rnns: Fast autoregressive transformers with linear attention. InInternational conference\non machine learning, pages 5156–5165. PMLR, 2020.\n[Lake and Baroni, 2018] Brenden Lake and Marco Baroni. Generalization without systematicity: On\nthe compositional skills of sequence-to-sequence recurrent networks. In International conference on\nmachine learning, pages 2873–2882. PMLR, 2018.\n[Ma et al., 2023] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig,\nJonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. InThe Eleventh\nInternational Conference on Learning Representations, 2023.\n[Mohtashami and Jaggi, 2024] Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context\nlength for transformers. Advances in Neural Information Processing Systems, 36, 2024.\n[Press et al., 2022] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with lin-\near biases enables input length extrapolation. In Proceedings of International Conference on Learning\nRepresentations, 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of Advances in\nNeural Information Processing Systems, volume 30, 2017.\n[Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C. Alberti,\nS. Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, L. Yang, and A. Ahmed. Big bird: Transformers\nfor longer sequences. Advances in neural information processing systems, 33:17283–17297, 2020"
    },
    {
        "cluster_id": 12,
        "text": "vi\n\n\n=\ni∑\nj=0\nαi,jvj (2.47)\nwhere K≤i =\n\n\nk0\n...\nki\n\nand V≤i =\n\n\nv0\n...\nki\n\nand V[i−nc+1,i] =\n\n\nvi−nc+1\n...\nvi\n\n. Sometimes we denote\nK[i−nc+1,i] by {ki−nc+1,..., ki}and V[i−nc+1,i] by {vi−nc+1,..., vi}for notation simplicity.\nStep 3: Detailed Calculation.\n5.3 Inference-time Scaling 239\nExpress (5 −4i) −2(3 + 6i) as a x\ncomplex number. Think step by step.\nNow we distribute the 2 to the terms\nin the parenthesis. 2(3 + 6i) = 6 + 12i.\nSo (5 −4i) −2(3 + 6i) is equivalent to\n(5 −4i) −(6 + 12i).\nNow we subtract the terms. x\n5 −4i−6 −12i= −1 −16i.\nThat’s the answer. x\nYou can also write it as −1 −16i. x\nAnswer −1 −16i"
    },
    {
        "cluster_id": 46,
        "text": "Recall from Section 2.1.1 that multi-head self-attention uses multiple sets of queries, keys, and\nvalues (each set is called a head), each performing the QKV attention mechanism as usual. This\ncan be expressed as\nOutput = Merge(head 1,..., headτ)Whead (2.70)\nwhere headj ∈Rdh is computed using the standard QKV attention function\nheadj = Attqkv(q[j]\ni ,K[j]\n≤i,V[j]\n≤i) (2.71)\nHere, q[j]\ni , K[j]\n≤i, and V[j]\n≤i are the query, keys, and values that are projected onto the j-th feature\nsub-space. So this model can be interpreted as performing attention on a group of feature sub-\nspaces in parallel (see Figure 2.8 (b)). The KV cache needs to retain the keys and values for all\nthese heads, that is, {(K[1]\n≤i,V[1]\n≤i),..., (K[τ]\n≤i,V[τ]\n≤i)}.\nOne refinement to the multi-head attention model, calledmulti-query attention (MQA), is to\nshare keys and values across heads, while allowing queries to be unique for each head [Shazeer,\n2019]. In MQA, there is a single set of keys and values (K≤i,V≤i). In addition, there are τ\nqueries {q[1]\ni ,..., q[τ]\ni }, each corresponding to a different head. For each head, we have\nheadj = Attqkv(q[j]\ni ,K≤i,V≤i) (2.72)\nFigure 2.8 (c) illustrates this model. By sharing keys and values, the size of the KV cache would\nbe O(L·dh ·m).\nSee Figure 2.8 (d) for an illustration. Let g(j) be the group id for the j-th head. The GQA model\ncan be expressed as\nheadj = Attqkv(q[j]\ni ,K[g(j)]\n≤i ,V[g(j)]\n≤i ) (2.73)\nThe size of the KV cache of GQA isO(L·ng·dh·m). One benefit of GQA is that we can trade-off\nbetween computational efficiency and model expressiveness by adjusting ng. When ng = τ, the\nmodel becomes the standard multi-head attention model. By contrast, when ng = 1, it becomes\nthe GQA model"
    },
    {
        "cluster_id": 6,
        "text": "• For buckets 0 to nb+1\n2 −1, each bucket corresponds to one offset, that is, bucket 0↔offset\n0, bucket 1 ↔offset 1, bucket 2 ↔offset 2, and so on. We express this as b(i −j) = i−j.\n• For buckets nb+1\n2 to nb, the size of each bucket increases logarithmically. For example, the\nbucket number for a given offset i−j ≥nb+1\n2 can be defined as\nb(i−j) = nb + 1\n2 + ⌊ log(i−j) −log(nb+1\n2 )\nlog(distmax) −log(nb+1\n2 ) ·nb + 1\n2 ⌋ (2.78)\nwhere the parameter distmax is typically set to a relatively large number to indicate the\nmaximum offset we may encounter.\n• When i−j >distmax, we place i−jin the last bucket. In other words, bucket nb contains\nall the offsets that are not assigned to the previous buckets.\nTogether, these can be expressed as the function\nb(i−j)\n=\n\n\n\ni−j 0 ≤i−j <nb+1\n2\nmin(nb,nb+1\n2 + ⌊\nlog(i−j)−log( nb+1\n2 )\nlog(distmax)−log( nb+1\n2 )\n·nb+1\n2 ⌋) i−j ≥nb+1\n2\n(2.79)\nFigure 2.10 shows an illustration of these buckets. We see that in the first half of the buckets,\neach bucket is associated with only one value of i−j, while in the second half, the bucket size\nincreases as i−j grows. The last bucket is designed to handle sequences of arbitrarily long\nlengths.\n2.3 Long Sequence Modeling 85\n0 1 2 3 ··· 14 15 16 17 18 ··· 32Bucket\nOffset\n(i−j)\n0 1 2 3 14 15 16 ∼20 21 ∼26 27 ∼33 802 ∼∞\nfixed bucket size logarithmically increased bucket size\nFig. 2.10: Illustration of distributing query-key offsets into buckets in the T5 model ( nb = 32 and distmax = 1024)"
    },
    {
        "cluster_id": 17,
        "text": "Although this formula involves a complicated expression, its equivalent form in thed-dimensional\nEuclidean space is relatively easy to understand. We can write it as\nRo(x,tθ) =\n[\nx1 x2 ... xd\n]\n\n\nRtθ1\nRtθ2\n...\nRtθd/2\n\n\n(2.92)\nwhere Rtθk =\n[\ncos tθk sin tθk\n−sin tθk cos tθk\n]\n. θ =\n[\nθ1,...,θ d/2\n]\nare the parameters for controlling the an-\ngles of rotations in different dimensions. Typically, θk is set to 10000−2(k−1)\nd , which is analogous\nto the setting in sinusoidal embeddings.\nRo(x,tθ) =\n\n\nx1\nx2\n...\nxd−1\nxd\n\n\nT\n⊙\n\n\ncos tθ1\ncos tθ1\n...\ncos tθd/2\ncos tθd/2\n\n\nT\n+\n\n\n−x2\nx1\n...\n−xd\nxd−1\n\n\nT\n⊙\n\n\nsin tθ1\nsin tθ1\n...\nRo(xi,iθ) can be cast in the form of a linear combination of two periodic functions (see Eq"
    },
    {
        "cluster_id": 57,
        "text": "When studying long-context LLMs, it is natural to wonder what mechanisms may explain the\nuse of long context in language modeling. Can we compress the representation of infinite context\ninto a relatively small-sized model? Are all context tokens useful for predicting next tokens? How\ndo LLMs prepare for token prediction when they see the context? Can we know in advance which\ncontextual information will be critical for prediction? General answers to all these questions\nare not obvious, but they inspire follow-on research of explainable models, and some interesting\nresults have been found. For example, Deletang et al. [2024] conducted extensive experiments\nto show that LLMs are powerful in-context compressors. Although viewing predictive models\nas compression models has long been studied in machine learning, it also provides insights into\nour understanding of the LLM scaling laws. Pal et al. [2023] and Wu et al. [2024] investigated\nwhether the features learned up to the current step, though not intentionally, are already sufficient\nfor predicting tokens at the following steps. Note that the need for long-context in language\nmodeling is highly dependent on the problem that we address. A related issue is where to apply\nLLMs and how to evaluate them. For example, in summarization tasks we may only need to distill\nand focus on a few key aspects of the text, while in retrieval-like tasks we need to “memorize”\nthe entire context so that the relevant information can be accessed. We will discuss the evaluation\nissue later in this subsection.\n2.3.6.3 Evaluating Long-context LLMs\nEvaluating long-context LLMs is important, but it is a new issue in NLP. The general idea is that,\nif we input a long context to an LLM, then we can check from the output of the LLM whether it\nunderstands the entire context and makes use of it in predicting following tokens. In conventional\nresearch of NLP, such evaluations are often aimed at examining the ability of NLP models in\nhandling long-range dependencies. However, the size of contexts used in recent LLMs is much\nlarger than that used in NLP systems a few years ago. This motivates researchers to develop new\nevaluation benchmarks and metrics for long-context LLMs.\nOne approach is to use the perplexity metric. However, in spite of its apparent simplicity, this\nmethod tends to reflect more on the LLMs’ ability to make use of local context rather than global\ncontext. It is therefore tempting to develop evaluation methods that are specific to long-context\nLLMs. Popular methods include various synthetic tasks where artificially generated or modified\ndata is used to evaluate specific capabilities of long-context LLMs. In needle-in-a-haystack18 and\npasskey retrieval tasks [Mohtashami and Jaggi, 2024; Chen et al., 2023c], for instance, LLMs are\nrequired to identify and extract a small, relevant piece of information from a large volume of given\ntext. The assumption here is that an LLM with sufficient memory should remember earlier parts\nof the text as it processes new information. This LLM can thus pick out the relevant details, which\nmight be sparse and hidden among much irrelevant information, from the text. Alternatively, in\ncopy memory tasks (or copy tasks for short), LLMs are used to repeat the input text or a specific\nsegment multiple times. These tasks were initially proposed to test the extent to which recurrent\nmodels can retain and recall previously seen tokens [Hochreiter and Schmidhuber, 1997; Arjovsky\net al., 2016], and have been adopted in evaluating recent LLMs [Bulatov et al., 2022; Gu and Dao,\n2023].\n18https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n94 Generative Models\nAnother approach to evaluating long-context LLMs is to test them on NLP tasks that involve\nvery long input sequences. Examples include long-document or multi-document summarization,\nlong-document question answering, code completion, and so on. A benefit of this approach is that\nit can align evaluations with user expectations.\nAlthough many methods have been developed, there is still no general way to evaluate long-\ncontext LLMs [Liu et al., 2024c]. One problem is that most of these methods focus on specific\naspects of LLMs, rather than their fundamental ability to model very long contexts. Even though\nan LLM can pick out the appropriate piece of text from the input, we cannot say that it truly un-\nderstands the entire context. Instead, it might just remember some important parts of the context,\nor even simply recall the answer via the model learned in pre-training. Moreover, the data used\nin many tasks is small-scale and relatively preliminary, leading to discrepancies between evalu-\nation results and actual application performance. A more interesting issue is that the results of\nLLMs are influenced by many other factors and experimental setups, for example, using different\nprompts can lead to very different outcomes. This makes evaluation even more challenging be-\ncause improvements may not solely result from better modeling of long contexts, and there is a\nrisk of overclaiming our results. Nevertheless, many open questions remain in the development\nand evaluation of long-context LLMs. For example, these models still suffer from limitations\nsuch as restricted context length and high latency. Studying these issues is likely to prove valuable\nfuture directions.\nThe strength of LLMs lies in their ability to break the constraints of training NLP models for\na limited number of specific tasks. Instead, LLMs learn from large amounts of text through the\nsimple task of token prediction — we predict the next token in a sentence given its prior tokens.\nThe importance of the inference problem in LLMs also lies in the fact that many application\nscenarios require processing extremely long sequences. Recent studies have found that injecting\nadditional prompts and contextual information, such as long chain-of-thought prompts, during\ninference can significantly improve the performance of LLMs. This provides a new approach to\nscaling LLMs: better results can be achieved by increasing the compute at inference time. For\ninstance, through inference-time scaling, OpenAI [2024]’s o1 and Deepseek [2025]’s R1 systems\nhave demonstrated impressive performance on complex reasoning and programming tasks. This,\nin turn, has encouraged the NLP field to focus more on the issue of efficient inference"
    },
    {
        "cluster_id": 66,
        "text": "3.1.2 In-context Learning\nLearning can occur during inference. In-context learning is one such method, where prompts\ninvolve demonstrations of problem-solving, and LLMs can learn from these demonstrations how\nto solve new problems. Since we do not update model parameters in this process, in-context\nlearning can be viewed as a way to efficiently activate and reorganize the knowledge learned in\npre-training without additional training or fine-tuning. This enables quick adaptation of LLMs to\nnew problems, pushing the boundaries of what pre-trained LLMs can achieve without task-specific\nadjustments.\nIn-context learning can be illustrated by comparing three methods: zero-shot learning, one-\nshot learning and few-shot learning. Zero-shot learning, as its name implies, does not involve a\ntraditional “learning” process. It instead directly applies LLMs to address new problems that were\nnot observed during training. In practice, we can repetitively adjust prompts to guide the LLMs in\ngenerating better responses, without demonstrating problem-solving steps or providing examples.\nDEMO 12 5 → (12 + 5)/(12 ×5) = 0.283\n3 1 → (3 + 1)/(3 ×1) = 1.33\n−9 4 → (−9 + 4)/(−9 ×4) = 0.138\n15 15 → (15 + 15)/(15 ×15) = 0.133\nUSER 19 73 →\nIn many practical applications, the effectiveness of in-context learning relies heavily on the\nquality of prompts and the fundamental abilities of pre-trained LLMs. On one hand, we need a\nsignificant prompt engineering effort to develop appropriate prompts that help LLMs learn more\neffectively from demonstrations. On the other hand, stronger LLMs can make better use of in-\ncontext learning for performing new tasks. For example, suppose we wish to use an LLM to\ntranslate words from Inuktitut to English. If the LLM lacks pre-training on Inuktitut data, its\nunderstanding of Inuktitut will be weak, and it will be difficult for the model to perform well in\ntranslation regardless of how we prompt it. In this case, we need to continue training the LLM\nwith more Inuktitut data, rather than trying to find better prompts"
    },
    {
        "cluster_id": 31,
        "text": "Input: She don’t like going to the park.\nInput: She don’t like going to the park.\nInput: Me and my friend goes to the gym every day.\nInput: She don’t like going to the park.\n... ... ... ... ... ... ... ... ... ... ... .."
    },
    {
        "cluster_id": 13,
        "text": "Step 1: Problem Interpretation.\nStep 2: Strategy Formulation.\nStep 4: Solution Review"
    },
    {
        "cluster_id": 3,
        "text": "Text:\nThe service at the restaurant was slower than expected, which was a bit frustrat-\ning.\nText:\nThe service at the restaurant was slower than expected, which was a bit frustrat-\ning.\nText:\nThe service at the restaurant was slower than expected, which was a bit frustrat-\ning.\nText:\nThe service at the restaurant was slower than expected, which was a bit frustrat-\ning"
    },
    {
        "cluster_id": 54,
        "text": "Instead, we start with the task of named entity recognition — a task that has long been a concern\nin NLP. Named entity recognition is a process that detects and classifies key information in text\ninto specific groups. These key pieces of information, known as named entities, typically include\nproper names and are categorized into distinct classes such as people, locations, organizations,\ndates, monetary values, and percentages. Consider first a simple example of extracting person\nnames from a text3.\ndetect the coreference and anaphoric links between entities from the text.\n[Brill, 1992] Eric Brill. A simple rule-based part of speech tagger. In Speech and Natural Language:\nProceedings of a Workshop Held at Harriman, New York, February 23-26, 1992, 1992.\n[Dolan and Brockett, 2005] Bill Dolan and Chris Brockett. Automatically constructing a corpus of senten-\ntial paraphrases. In Proceedings of Third International Workshop on Paraphrasing (IWP2005), 2005"
    },
    {
        "cluster_id": 32,
        "text": "You are provided with a conversation between a tourist and a taxi driver. Please\ncomplete the conversation.\nChatbot I’m sorry to hear that. Besides feeling tired, have you noticed any\nother symptoms?\nUser Yes, I’m also experiencing headaches frequently.\nChatbot How long have these symptoms been going on?\nUser About a week now.\nChatbot It might be good to check in with a healthcare professional. Would\nyou like help setting up an appointment?\nUser Yes, please. Can it be after work hours?\nChatbot Sure, I can arrange that. There are slots available next Wednesday\nand Thursday after 5 PM. Which day works better for you?\n...\nChatbot: I’m sorry to hear that. Besides feeling tired,\nhave you noticed any other symptoms?\nUser: Yes, I’m also experiencing headaches frequently.\nChatbot: How long have these symptoms been going on?\n...\nRequest:\nHello, I noticed that my order hasn’t arrived yet, though it was scheduled to\narrive several days ago. Could you please update me on its status? Thank you!\nResponse A:\nI’m very sorry for the delay and understand how disappointing this can be. We’re\ndoing our best to sort this out quickly for you.\nResponse B:\nHey, stuff happens! Your package will get there when it gets there, no need to\nstress"
    },
    {
        "cluster_id": 9,
        "text": "112 Prompting\nYouare a poet. Please write a traditional Chinese poem. Here is the requirement.\nChinese:\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公\n布的研究成果认为，大约从距今5800年开始，中华大地上各个区域相继出\n现较为明显的社会分化，进入文明起源的加速阶段。\nEnglish:\nA series of discoveries have cemented the significant role of the Hongshan cul-\nture in studies on the origins of Chinese civilization. The latest research findings\nsuggests that, starting from around 5800 years ago, many regions across China\nbegan to experience noticeable social differentiations, entering an accelerated\nphase in the origin of civilization.\nRefinement:\nA series of archaeological discoveries have cemented the significant role\nof the Hongshan culture in studies on the origins of Chinese civilization"
    },
    {
        "cluster_id": 20,
        "text": "This growth has led to numerous environmental issues, including elevated lev-\nels of air and water pollution, increased waste production, and strained public\nservices ...\nGenerate a response to the question: “What are some major environmental\nconcerns today?”\nResponse:\nMajor environmental concerns today include climate change, air and water\npollution, deforestation, loss of biodiversity, and ocean degradation.\nQuestion:\nWhat are some major environmental concerns today?\nResponse:\nMajor environmental concerns today include climate change, air and water\npollution, deforestation, loss of biodiversity, and ocean degradation.\nFeedback:\nThe response provided is accurate and highlights key environmental\nconcerns. However, it would benefit from more specific and detailed\ndescriptions of these concerns. Here are suggestions for improvement:\n1. Include specific examples of environmental issues.\n2. Add information about major pollutants.\nQuestion:\nWhat are some major environmental concerns today?\nResponse:\nMajor environmental concerns today include climate change, air and water\npollution, deforestation, loss of biodiversity, and ocean degradation.\nFeedback:\nThe response provided is accurate and highlights key environmental con-\ncerns. However, it would benefit from more specific and detailed descrip-\ntions of these concerns. Here are suggestions for improvement:\n1. Include specific examples of environmental issues.\n2. Add information about major pollutants.\nRefined Response:\nKey environmental concerns include climate change, evidenced by rising\nglobal temperatures and severe weather patterns; significant air and\nwater pollution in urban and industrial areas; rampant deforestation in\nregions like the Amazon, causing biodiversity loss; and ocean degradation,\nhighlighted by coral reef bleaching and widespread overfishing"
    },
    {
        "cluster_id": 27,
        "text": "Q: Please calculate the average of the numbers 2, 4, and 9.\nQ: Please calculate the average of the numbers 1, 3, 5, and 7.\nQ: Please calculate the average of the numbers 2, 4, and 9.\nQ: Please calculate the mean square of the numbers 1, 3, 5, and 7.\nQ: Please calculate the average of the numbers 2, 4, and 9.\nQ: Please calculate the average of the numbers 2, 4, and 9.\nWe have three numbers: 2, 4, and 9. Add these numbers together,\n2 + 4 + 9 = 15. Determine how many numbers there are, which in this case\nis three. The average is calculated by dividing the total sum by the number of\nelements. Completing the division gives 15/3 = 5. So the answer is 5"
    },
    {
        "cluster_id": 29,
        "text": "A: The answer is 6.\nA: The answer is 4.\nA: The answer is 7.\nSo the answer is (c).\nSo the answer is (d)"
    },
    {
        "cluster_id": 69,
        "text": "These structures include tree-based structures [Yao et al., 2024], graph-based structures [Besta\net al., 2024], and so on. By using these compact representations of reasoning paths, LLMs can\nexplore a wider range of decision-making paths, analogous to System 2 thinking5. Another line of\nresearch focuses on prompting LLMs with multi-round interactions. This involves decomposing\ncomplex problems into sub-problems, verifying and refining model outputs, employing model\nensembling, and so on. Note that these methods and the issues involved are not limited to CoT. In\nfact, they are often used as more general approaches to improving LLMs, while CoT can be seen\nas a way to test the capabilities of LLMs. We will see discussions of some of these issues in the\nfollowing subsections.\nHowever, many real-world problems require complex reasoning. One key characteristic of\nthese problems is that the reasoning steps may not be fixed. The reasoning path can vary for\ndifferent problems, and each step of reasoning may depend on the outcomes of prior steps. In\n3.2 Advanced Prompting Methods 121\nsuch cases, it is undesirable to use fixed sub-problem generation in advance. Instead, sub-problems\nshould be generated dynamically based on the input problem, and, if possible, generated on the\nfly during the reasoning process. This makes problem decomposition more challenging compared\nwith designing divide-and-conquer algorithms. Ideally, we would like to jointly design both the\nsystems for sub-problem generation and sub-problem solving. But a more practical and widely\nused approach is to adopt separate models for these tasks. A straightforward way to achieve this\nis to adapt an LLM for these tasks by either prompting or tuning the model.\nThe least-to-most method offers a basic approach to prompting LLMs to generate and solve\nsub-problems separately. We can improve it in several ways. One simple improvement is to apply\nvarious advanced prompting techniques, which do not require changes to the problem decom-\nposition framework. For example, we can incorporate CoT into the prompting to enhance the\nreasoning performance of sub-problem generation and solving.\na0 = S0(p0,{p≤n,a≤n}) (3.4)\nOne way to refine this model is to modify theG(·) function so that the model can dynamically\ngenerate answers. Instead of generating all sub-problems at one time, we can generate each of\nthem during problem-solving [Dua et al., 2022]. To do this, we can replace Eq. (3.2) with\npi = Gi(p0,{p<i,a<i}) (3.5)\nHence we obtain a sub-problem generation model that operates in a step-by-step manner. At each\nstep i, we first generate the sub-problem pi by prompting an LLM with the original problem p0\nand the problem-solving history {p<i,a<i}. We then generate the answer ai for this sub-problem\nusing the same or a different LLM, based on the same contextual information (see Eq. (3.3)). This\nmethod effectively expands the reasoning capacity of LLMs by allowing them to dynamically\ngenerate and solve sub-problems in intermediate reasoning steps. As a result, the reasoning paths\nare not fixed in advance, and the models can choose and adapt their reasoning strategies during\nproblem-solving.\nIterative methods can be applied to a range of LLM prompting problems. For example, in problem\ndecomposition, one can incorporate new sub-problems and their solutions into the context at each\nstep, and thus LLMs can progressively approach the solution of the original problem. On the other\nhand, iterative methods raise several issues that are absent in non-iterative methods, for example,\n130 Prompting\nerrors in earlier steps may negatively impact subsequent problem-solving, and determining when\nto stop iterating often requires additional engineering effort.\nAt a high level, methods for scaling the reasoning of LLMs can be categorized into two classes:\n• Training-free Methods. These methods aim to improve reasoning capabilities without\nrequiring any modification or retraining of the pre-trained parameters. Instead, they focus\non techniques applied during inference, such as sophisticated prompting strategies (e.g.,\nchain-of-thought) and algorithmic control over the reasoning process (e.g., search).\nSuch fine-grained control is particularly beneficial for complex reasoning problems where a\nsingle incorrect intermediate step can render the entire subsequent reasoning chain invalid. By\nevaluating or guiding the generation at each intermediate step, the LLM can explore the reasoning\nspace more effectively, potentially pruning unpromising paths early or allocating more resources\nto explore more plausible ones"
    },
    {
        "cluster_id": 8,
        "text": "• Introduction\nIntroduce AI, its relevance, and the importance of understanding its risks for youth.\n• Privacy Concerns\nDiscuss how AI might compromise personal privacy through interactions online.\n• Misinformation\nExplore AI’s role in spreading misinformation and influencing young people’s deci-\nsions.\n• Cyberbullying\nHighlight how AI tools can be utilized in cyberbullying and the impact on mental\nhealth.\n• Tips for Safe AI Use\nOffer guidelines for responsible AI usage and promote critical thinking.\n• Conclusion\nRecap main points and encourage proactive engagement with AI ethics.\nYou are provided with a text. Please determine whether it discusses the risks of\nAI.\n[Krakovna et al., 2020] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew\nRahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specifi-\ncation gaming: the flip side of ai ingenuity. https://deepmind.google/discover/blog/\nspecification-gaming-the-flip-side-of-ai-ingenuity , 2020.\n[Russell, 2019] Stuart Russell. Human Compatible: Artificial Intelligence and the Problem of Controls.\n[Wiener, 1960] Norbert Wiener. Some moral and technical consequences of automation: As machines\nlearn they may develop unforeseen strategies at rates that baffle their programmers. Science, 131(3410):\n1355–1358, 1960"
    },
    {
        "cluster_id": 16,
        "text": "A general framework for problem decomposition involves two elements.\n• Sub-problem Generation. This involves decomposing the input problem into a number of\nsub-problems.\n• Sub-problem Solving. This involves solving each sub-problem and deriving intermediate\nand final conclusions through reasoning.\nThese two issues can be modeled in different ways, leading to various problem decomposition\nmethods. One approach is to treat them as separate steps in a two-step process. For example,\nconsider the blog writing task described at the beginning of this subsection. In the first step, we\ndecompose the entire problem into sub-problems all at once (i.e., outline the blog). In the second\nstep, we solve the sub-problems either sequentially or in another order (i.e., fill in content for\neach section as needed). The final output of this process combines the results from solving each\nsub-problem. While this method is simple and straightforward, it assumes that the problem is\ncompositional, making it more suitable for tasks like writing and code generation.\nTASK Your task is to decompose a problem into several sub-problems. You will\nbe given a few examples to illustrate how to achieve this"
    },
    {
        "cluster_id": 21,
        "text": "USER Q: The environmental study conducted from 2015 to 2020 revealed that the\naverage temperature in the region increased by 2.3 degrees Celsius. What\nwas the duration of the environmental study?\nA: To answer the question “What was the duration of the environmental\nstudy?”, we need to know: “When did the environmental study start?”,\n“When did the environmental study end?”.\nSUB-PROB1 Q: When did the environmental study start?\nA: The environmental study started in 2015.\nSUB-PROB1 Q: When did the environmental study start?\nA: The environmental study started in 2015.\nSUB-PROB2 Q: When did the environmental study end?\nA: The environmental study ended in 2020.\nSUB-PROB1 Q: When did the environmental study start?\nA: The environmental study started in 2015.\nSUB-PROB2 Q: When did the environmental study end?\nA: The environmental study ended in 2020.\nFINAL Q: What was the duration of the environmental study?\nA: The duration of the environmental study was 5 years.\n3. Specify the regions most affected.\n3. Specify the regions most affected"
    },
    {
        "cluster_id": 22,
        "text": "For LLM prompting, it is also possible to improve performance by combining predictions\nbased on different prompts. Suppose we have an LLM and a collection of prompts that address\nthe same task. We can run this LLM with each of the prompts and then combine the predictions.\nFormally, let{x1,..., xK}be Kprompts for performing the same task. Given an LLMPr(·|·),\nwe can find the best prediction for each xi using ˆyi = arg maxyi Pr(yi|xi). These predictions\ncan be combined to form a “new” prediction:\nˆy = Combine( ˆy1,..., ˆyK) (3.6)\nHere Combine(·) is the combination model, which can be designed in several different ways. For\nexample, we can select the best prediction by voting or by identifying the one that overlaps the\nmost with others. Another method for model combination is to perform model averaging during\ntoken prediction. Let ˆyj be the predicted token at the j-th step for model combination. The\nprobability of predicting ˆyj is given by\nˆyj = arg max\nyj\nK∑\nk=1\nlog Pr(yj|xk,ˆy1,..., ˆyj−1) (3.7)\n3.2 Advanced Prompting Methods 131\nIn ensembling for LLM prompting, it is generally advantageous to use diverse prompts so that\nthe combination can capture a broader range of potential responses. This practice is common in\nensemble learning, as diversity helps average out biases and errors that may be specific to any\nsingle model or configuration. From the Bayesian viewpoint, we can treat the promptx as a latent\nvariable, given the problem of interest, p. This allows the predictive distribution of y given pto\nbe written as the distribution Pr(y|x) marginalized over all possible prompts\nPr(y|p) =\nZ\nPr(y|x) Pr(x|p)dx (3.8)\nThe integral computes the total probability of y by considering all possible values of x, weighted\nby their likelihoods given p. Here Pr(y|x) is given by the LLM, and Pr(x|p) is the prior distri-\nbution of prompts for the problem. This is a good model because the integral effectively accounts\nfor the uncertainty in the choice of x, ensuring that the final predictive distribution Pr(y|p) is\nrobust and encompasses all potential variations and biases in the prompts. However, computing\nthis integral directly can be computationally infeasible due to the potentially infinite space of x.\nThe next question is how to combine diverse model predictions. This leads us back to the\nmodel combination issue as described in Eq. (3.6). Here we consider theself-consistency method,\nwhich outputs not the prediction with the highest probability, but rather the one that best aligns\nwith other predictions [Wang et al., 2022a; 2023b]. First, an LLM is prompted with CoT as\nusual and generates multiple reasoning paths by sampling. Then, the frequency of each answer is\ncounted across these reasoning paths. The final output is the answer that has the highest count"
    },
    {
        "cluster_id": 0,
        "text": "Prediction 1 (Correct):\nQ: Three friends play a game where they flip a fair coin. Each friend flips the\ncoin once. What is the probability that exactly one of them flips heads?\nA: Let’s think step by step.\nEach coin flip is an independent event with a 50% chance of landing heads\n(H) and 50% tails (T). Consider the possible outcomes for three flips: HHH,\nHHT, HTH, HTT, THH, THT, TTH, TTT. Out of these, only HHH, HHT, HTH,\nand THH are relevant. We are only interested in scenarios with exactly one\nhead: HTT, THT, and TTH. There are 3 such outcomes out of 8 total, giving a\nprobability of 3/8.\nPrediction 2 (Correct):\nQ: Three friends play a game where they flip a fair coin. Each friend flips the\ncoin once. What is the probability that exactly one of them flips heads?\nA: Let’s think step by step.\nUsing a binomial probability formula, where p (success or heads) is 0.5, and\neach of the 3 trials (flips) is independent, we look for exactly one success\n(head). The formula for exactly k successes in n trials is given by the\nbinomial coefficient formula\n(n\nk\n)\npk(1 −p)n−k. Plugging in the values, we get(3\n1\n)\n(0.5)1(0.5)3−1 = 3 ·0.5 ·0.25 = 0.375.\nPrediction 3 (Wrong):\n3.2 Advanced Prompting Methods 133\nQ: Three friends play a game where they flip a fair coin. Each friend flips the\ncoin once. What is the probability that exactly one of them flips heads?\nA: Let’s think step by step.\nSince each coin flip has a 50% chance of landing heads and a 50% chance of\nlanding tails, and we are looking for the case where only one of the three flips\nresults in heads, we might initially think this is simply a matter of picking which\nflip is the heads. There are three flips, so one might incorrectly assume that the\nchance of picking one specific outcome like this would be 1 out of 3. Thus, they\nmight conclude that the probability of exactly one head is 1/3 = 33.3%.\nPredictions 1 and 2 correctly identify the three cases where exactly one head is flipped, both\nobtaining a probability of 37.5%. The reasoning in Prediction 3 fails to account for the total num-\nber of outcomes possible with three coin flips, thus giving a wrong answer of 33.3%. Therefore,\nwe select 37.5% as the final answer because it is the consensus"
    },
    {
        "cluster_id": 39,
        "text": "You are provided with several input-output pairs for a task. Please write\nan instruction for performing this task.\nSample 1: (Instruction,User-input,Output)\nSample 2: (Instruction,User-input,Output)\n···\nTask Pool\nSampling\nDraw a few instructions from the pool\nInstructiona\nInstructionb\nInstructionc\nTask Pool\nsampling\nInstruction\nGeneration\nPrompt the LLM to generate a new instruction based on\nthe drawn instructions.\nYou are provided several different instructions for performing\nsome tasks. Please generate an instruction based on these.\nTask 1: Instructiona\nTask 2: Instructionb\nTask 3: Instructionc\nNew Task: Instructionnew\nSample\nGeneration\nGiven the newly-generated instruction and a few\ninput-output samples, generate a new sample.\nYou are provided with a set of input-output samples tasks,\neach composed of an instruction, a user input, and an output.\nSample 1: Samplea\nSample 2: Sampleb\nNew Sample: Instructionnew User-inputnew Outputnew\nFiltering\nFilter out invalid and low-quality samples.\nTask 1: {instruction1}\nTask 2: {instruction2}\nTask 3: {instruction3}\nTask 4: {instruction4}\nNew Task:\n• Given the generated instruction, the LLM is then prompted to complete the sample by filling\nin the remaining input fields and generating the corresponding output. Below is a prompt\ntemplate.\nYou are provided with a set of input-output samples, each composed of\nan instruction, a user input, and an output. Please generate a new sample\nbased on these.\nSample 1: {instruction1}\nInput: {user-input1}\nOutput: {output1}\nSample 2: {instruction2}\nInput: {user-input2}\nOutput: {output2}\nNew Sample: {new-instruction}\n• This newly-generated sample is examined by some heuristic rules (such as filtering out\nsamples or instructions that are similar to those already in the pool). If it passes, the sample\nand instruction are added to the pool"
    },
    {
        "cluster_id": 52,
        "text": "An extension to the Bradley-Terry model for listwise ranking could involve a ranking mecha-\nnism that takes into account the entire list of outputs rather than just pairwise comparisons. One\nsuch model is the Plackett-Luce model, which generalizes the Bradley-Terry model to handle\nmultiple items in a ranking [Plackett, 1975]. In the Plackett-Luce model, for each item in a list,\nwe define a “worth” for this item that reflects its relative strength of being chosen over other items.\n[Burges et al., 2005] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton,\nand Greg Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international\nconference on Machine learning, pages 89–96, 2005.\n[Cao et al., 2007] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from\npairwise approach to listwise approach. In Proceedings of the 24th international conference on Machine\nlearning, pages 129–136, 2007.\n[Li, 2011] Hang Li. Learning to Rank for Information Retrieval and Natural Language Processing. Online\naccess: Morgan & Claypool Synthesis Collection Five. Morgan & Claypool Publishers, 2011. ISBN\n9781608457076.\n[Liu, 2009] Tie-Yan Liu. Learning to rank for information retrieval. Foundations and Trends® in Informa-\ntion Retrieval, 3(3):225–331, 2009"
    },
    {
        "cluster_id": 67,
        "text": "In this chapter, we will introduce basic concepts and algorithms of LLM inference, including\nprefilling-decoding frameworks, search (decoding) algorithms, and evaluation metrics of infer-\nence performance. We will then present methods for improving the efficiency of LLM inference,\ncovering a range of techniques for speeding up the system and compressing the model. Finally,\nwe will discuss inference-time scaling, which is considered an important application of inference\noptimization.\n204 Inference\n5.1 Prefilling and Decoding\nIn this section, we present the prefilling-decoding framework, which is the most commonly used\nfor interpreting and implementing LLM inference processes. We first introduce the notation and\nbackground knowledge, and then describe the details of the framework, such as the decoding\nalgorithms for LLM inference.\n5.2.4 Remarks\nWe have considered many methods for improving the efficiency of LLMs in this and previous\nchapters. Although these approaches address different issues, most of them essentially explore\ntrade-offs between various performance factors. One important trade-off is between inference\nspeed and accuracy. For example, techniques like quantization, pruning, and knowledge dis-\ntillation can significantly reduce computational overhead and latency but may introduce minor\ndegradations in model performance. Conversely, preserving full precision or using larger models\nenhances accuracy but at the cost of slower inference and higher resource demands.\nBeyond speed, accuracy, and memory, several other dimensions also influence LLM inference\nefficiency. Some of these dimensions have been discussed in this chapter, while others have not.\nTechniques like model compression can improve energy efficiency, but usually with some\ndegradation in output quality or increase in latency. Energy constraints thus introduce an-\nother important dimension in optimizing LLM inference.\n5.4 Summary\nIn this chapter, we have discussed the inference issue for LLMs. We have presented the prefilling-\ndecoding framework and related decoding algorithms for LLM inference. Then, we have de-\nscribed several techniques for efficient inference. We have also discussed inference-time scaling,\nwhich has been considered one of the most important methods for improving LLM reasoning.\nBibliography\n[Agrawal et al., 2023] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gula-\nvani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chun-\nked prefills. arXiv preprint arXiv:2308.16369, 2023.\n[Patel et al., 2024] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed\nMaleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In\n2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA) , pages 118–\n132. IEEE, 2024"
    },
    {
        "cluster_id": 15,
        "text": "The prefilling and decoding processes are illustrated in Figure 5.3. Note that both these pro-\ncesses are autoregressive. However, as shown in Table 5.1, they differ in several aspects, which\nlead to very different implementations in practice.\none decoding step for\nx1 and x2.\none decoding step for\nx1 and x2, and prefilling\nfor x3.\none decoding step for\nx1, x2 and x3.\none decoding step for\nx1 and x3, and prefilling\nfor x4"
    },
    {
        "cluster_id": 36,
        "text": "playful\nare\nis\n1\n2\n3\n4\n5\ncats.\nplayful\nare\nis\n6\n7\n8\n9\n10\ncats.\nplayful\nare\nis\n11\n12\n13\n14\n15\ncats.\nplayful\nare\nis\n16\n17\n18\n19\n20\nPath: node 0 →node 3 →node 9 →node 11 →node 17\nOutput: cats are playful"
    },
    {
        "cluster_id": 37,
        "text": "are\nsick\n5\n6\n7\n8\n9\n① Expansion\ncute\non\nsick\nare.\nare\nsick\n5\n6\n7\n8\n9\n① Expansion\ncute\non\nsick\nare.\nare\nsick\n5\n6\n7\n8\n9\n① Expansion\ncute\non\nsick\nare.\nare\nsick\n5\n6\n7\n8\n9\n① Expansion\ncute\non\nsick\nare"
    },
    {
        "cluster_id": 18,
        "text": "6\n7\n9\n8\n5\nPr =.34\nPr =.32\nPr =.21\nPr =.12\nPr =.01\nok\nok\nok\npruned\npruned\n② Ranking\ncute\non\nsick\n6\n7\n9\n=⇒\n=⇒\n=⇒\n③ Output (K-best)\nbeam width (K) = 3\n(b) Beam search\nis\n4\non\ncute.\n6\n7\n9\n8\n5\nPr =.34\nPr =.32\nPr =.21\nPr =.12\nPr =.01\nok\nok\nok\npruned\npruned\n② Ranking\ncute\non\nsick\n6\n7\n9\n=⇒\n=⇒\n=⇒\nPr =.39\nPr =.36\nPr =.25\npruned\nok\npruned\n③ Selection & Sampling\non\n7\n=⇒\n④ Output\nselect top-khypotheses (k= 3),\nrenormalize their proababilities,\nand select one via sampling.\n(c) Top-kSampling\nis\n4\non\ncute"
    },
    {
        "cluster_id": 2,
        "text": "• Throughput. It refers to the number of tokens or requests the model can process per second.\n• Time to First Token (TTFT). This metric measures the time it takes from the beginning of\na request being sent to the generation of the first token of the response. If data transmission\ndoes not consume too much time, then TTFT is mainly the time for prefilling and predicting\nthe first token.\n• Inter-token Latency(ITL). This metric refers to the time taken to generate each subsequent\ntoken after the first one. It reflects the efficiency of the decoding process.\n• Tokens Per Second (TPS). This metric quantifies the number of tokens that the model can\ngenerate per second"
    },
    {
        "cluster_id": 48,
        "text": "5.2.2 Batching\nBatching in LLM inference refers to the process of processing multiple input sequences simultane-\nously as a group (called a batch) rather than one at a time. Because modern GPUs excel at parallel\nprocessing, batching allows them to compute multiple sequences in a single forward pass, keeping\nthe hardware fully occupied. Therefore, when serving LLMs at scale, batching is important for\n224 Inference\nprefilling decoding\n(a) batch size = 1\n11111pad 1 1 1 1\n2222padpad 2 2 2\n333padpadpad 3 3\n444444 4 4 4 4 4 4\nprefilling decoding\n(b) batch size = 4\n11111pad 1 1 1 1 1 1\n22222pad 2 2 2 2 2\n333333 3 3 3 3 3\n444444 4 4 4 4 4 4\nprefilling decoding\n(c) batch size = 4 (similar sequence lengths)\n11111pad 1 1 1 1\n222333 2 2 2\n555544 3 3 3 3\n666666 4 4 4\n5 5 5 5 5\n6 6 6 6 6 6\nprefilling decoding\nengine 1 engine 2\ntransfer the KV cache\n(d) disaggregation of prefilling and decoding\nFig. 5.8: Illustrations of basic batching methods. We use a 2D layout to illustrate the batch, where each square\nrepresents a token. Red squares indicate tokens in the prefilling stage, blue squares represent tokens in the decoding\nstage, green squares denote padding tokens, and gray squares correspond to meaningless tokens. Subfigures (a) and (b)\ncompare the cases where the batch size is 1 and 4, respectively. Subfigure (c) shows the strategy of grouping sequences\nwith similar lengths into the same batch. Subfigure (d) illustrates the disaggregation of prefilling and decoding. In\nthis approach, we can make better use of the parallelism of GPUs by concatenating multiple short sequences into a\nsingle long sequence for joint processing. This allows us to maximize the number of tokens processed in a batch while\nminimizing the number of padding tokens. However, as a trade-off, we need to copy the KV cache to the decoding\nengine and reorganize it after the prefilling phase, which introduces additional data transfer overhead.\nTo illustrate the idea of batching, Figure 5.8 (a-b) show simple examples with batch sizes of\n1 and 4, respectively. When using a batch size of 1 (i.e., without batching), the GPU processes\none input sequence at a time. Thus, the processing is sequential: the next sequence must wait for\nthe current computation to finish. By contrast, when using a batch size of 4, the GPU can process\nfour sequences simultaneously in a single forward pass. As the input sequences vary in length, we\nneed to standardize their length using padding techniques. Here we use left padding, which adds\ndummy tokens to the beginnings of short sequences, so all the sequences in the batch would have\nthe same length for prefilling. For decoding, tokens are generated simultaneously for all these\nsequences, and the generation process continues until the longest sequence reaches completion.\nAnother approach to implementing batching in LLMs is to disaggregate the prefilling and de-\ncoding processes [Wu et al., 2023a; Patel et al., 2024; Zhong et al., 2024]. For example, we can\nperform prefilling on one GPU, and perform decoding on another GPU. One advantage of disag-\ngregation is that we can rearrange the input sequences in the batch to better fill it, because there is\nno interference between prefilling and decoding. For example, we can concatenate multiple short\nsequences into a longer one, thus ensuring that the lengths of sequences in the batch are as con-\nsistent as possible, as illustrated in Figure 5.8 (d). In this way, we can maximize the throughput\nof the prefilling phase. However, as a trade-off, we need to transfer the KV cache to the devices\nperforming decoding, which also incurs extra communication overhead. Typically, this method\nrequires a high-bandwidth, low-latency network to achieve optimal performance.\nThis architecture is illustrated in Figure 5.9. Incorporating scheduling into batch processing\nprovides a flexible way to optimize both the system’s throughput and latency, thereby achieving a\nbetter balance between them. For example, the batching methods shown in Figure 5.8 (a) and (b)\ncan be considered one of the simplest scheduling strategies, called request-level scheduling. In\nthis strategy, once a batch is filled and sent to the engine, the processing of the entire batch cannot\n226 Inference\nScheduler Inference\nEngine\nbatch\nbatch (after processing)\nRequest Pool\nx1, x2, x3, ...\nbe interrupted. The scheduler waits for this batch to be processed before handling the next batch\n[Timonin et al., 2022].\nA more sophisticated scheduling strategy, called iteration-based scheduling, interacts with\nthe inference engine at each token prediction step rather than at the sequence level. This approach\nallows dynamic batch adjustment during inference, as illustrated in Figure 5.10. Such fine-grained\ncontrol lets the system prioritize critical tokens or sequences in real-time. For instance, if an urgent\nrequest arrives at some decoding step, the scheduler can add this request into the batch so that it\ncan be processed as early as possible. In the following subsections, we will discuss batching\nmethods based on iteration-based scheduling.\n5.2.2.2 Continuous Batching\nContinuous batching is an iteration-based scheduling method used in the Orca system [Yu et al.,\n2022]. In this method, an iteration refers to either the entire prefilling procedure or a single\ndecoding step. For example, given an input sequence x = x0...xm and an output sequence y =\ny1...yn, there are n+1 iterations in total: one for prefilling, and nfor generating the output tokens\n(one per token). During scheduling, the batch can be adjusted between iterations. For example,\nwe can either add a new input sequence to the batch, or remove a complete sequence from the\nbatch at some iteration, even if the batch processing is not yet finished.\nThe general process of continuous batching includes the following steps:\n• Initially, a batch is created with one or more input sequences, based on both the inference\nengine’s processing capacity and the current user requests. The batch is then fed into the\ninference engine.\n5.2 Efficient Inference Techniques 227\n1 1\n2\n3 3 3\n1 1\n2\n3 3 3 3\n2\n1\n1 1\n2\n3 3 3 3 3\n2 2\n1\n1 1\n2\n3 3 3 3 3 3\n2 2\n1\nBegin\nEnd\nRequests\nx1, x2, x3 arrived\nRequest\nx4 arrived\nx4 is added to\nthe next batch\nIteration 1\n(prefilling)\nIteration 2\nIteration 3\nIteration 4\n(a) Request-level Scheduling\n1 1\n2\n3 3 3\n1 1\n2\n3 3 3 3\n2\n1\n1 1\n2\n3 3 3\n4 4 4\n3 3\n2 2\n1\n1 1\n2\n3 3 3\n4 4 4\n3 3 3\n2 2\n1\n4\nBegin\nEnd\nRequests\nx1, x2, x3 arrived\nRequest\nx4 arrived\nprefilling for x4\none decoding step\nfor x1, x2, x3\nIteration 1\n(prefilling)\nIteration 2\nIteration 3\nIteration 4\nMore iterations\n(b) Iteration-level Scheduling\nFig. 5.10: Illustrations of request-level scheduling and iteration-based scheduling. In request-level scheduling, once\na batch is created and sent to the inference engine, we cannot adjust the batch. In other words, scheduling only\noccurs after the processing of a batch finishes. In iteration-level scheduling, we can perform scheduling during batch\nprocessing. For example, if a new request arrives at some point during inference, we can add it to the batch and continue\nprocessing.\n• The inference engine processes the batch iteration by iteration. After each iteration, the\nscheduler may adjust the batch in one of the following ways:\n– If a sequence in the batch completes generation (i.e., generates the end-of-sequence\nsymbol), that sequence is removed from the batch.\n– If a new user request arrives and the inference engine has additional processing capac-\nity, it is added to the batch.\n1 1 1\n2 2Scheduler\nx1, x2\narrived\n(a) Iteration 1\ninput\noutput\nbatch\n1 1 1\n2 2 2\n1\nScheduler\n(b) Iteration 2\ninput\noutput\nbatch\n1 1 1\n2 2 2 2\n1 1\nScheduler\n(c) Iteration 3\ninput\noutput\nbatch\n1 1 1\n2 2\n3 3\n2 2 2\n1 1 1\nScheduler\nx3 arrived\n(d) Iteration 4\ninput\noutput\nbatch\n1 1 1\n2 2\n3 3 3\n2 2 2 2\n1 1 1 1\ncomplete\nScheduler\ny2\n(e) Iteration 5\ninput\noutput\nbatch\n1 1 1\n4 4 4\n3 3 3 3\n1 1 1 1 1\ncontinue the second sequence\nin the batch with x4\nScheduler\nx4, x5\narrived\n(f) Iteration 6\ninput\noutput\nbatch\nFig. 5.11: Illustration of batch adjustment in continuous batching. Instead of fixing a batch of input sequences and\nprocessing them to completion (as in request-level batching), continuous batching dynamically updates the batch during\ninference. The system continuously accepts and adds new requests (e.g., x3 and x4) into the current batch as long as\nthere is available compute capacity.\nrequests, x1 and x2. These two sequences are packed into a batch and sent to the inference engine\nfor processing. After the engine completes two iterations, a new user request, x3, arrives. At this\npoint, the scheduler adjusts the batch by adding x3 to it. The inference engine then continues\nprocessing the updated batch. Note that the inference engine now processes different sequences\nin different ways: x1 and x2 proceed with the decoding process (i.e., predicting the next tokens),\nwhile x3 undergoes the prefilling process. After some time, the generation for x2 completes. As\nit happens, two more user requests, x4 and x5, arrive. The scheduler removes the completed\nsequence x2 from the batch and, considering the current load of the inference engine, adds x4 to\nthe batch. However, x5 must wait until another sequence in the batch finishes before it can be\nadded.\n5.2 Efficient Inference Techniques 229\nThe idea behind continuous batching is to keep the inference engine fully utilized by process-\ning as many sequences as possible, thereby maximizing computational resource usage. A key dif-\nference between continuous batching and standard batching (see Figure 5.8) lies in the fact that, in\ncontinuous batching, prefilling and decoding can occur simultaneously across different sequences,\nwhereas in standard batching, these two phases are performed sequentially for the entire batch. As\ndiscussed in Section 5.1.2, prefilling is considered a compute-bound process, while decoding is\nconsidered a memory-bound process. The intuition behind overlapping prefilling and decoding is\nto reduce idle times for both computation and data transfer. Consider two mini-batches: one for\nprefilling and one for decoding. While the prefilling mini-batch keeps the GPUs occupied, the\ndecoding mini-batch can perform memory transfers concurrently.\nAnother difference between continuous batching and standard batching is that continuous\nbatching is prefilling-prioritized, while standard batching is decoding-prioritized [Agrawal et al.,\n2024]. In continuous batching, once the inference engine has spare computational resources, the\nscheduler will add new requests to the batch. In other words, these newly added requests will be\nprocessed for prefilling as early as possible. This approach improves system throughput, but at the\ncost of increased latency, as the newly added requests extend the processing time of earlier ones.\n5.2.2.4 Chunked Prefilling\nWe have seen that, in iteration-level scheduling, prefilling and decoding for different sequences\ncan occur simultaneously. This can be seen as a prefilling-prioritized strategy which can maximize\nthe throughput. However, one such iteration can take a long time if the input sequence is very long\nand the prefilling process dominates the computation. In this case, decoding for other sequences\nhas to wait until the prefilling completes, leading to increased latency for generating output tokens.\nA simple way to reduce decoding latency is to make computations for different sequences in\nthe batch comparable. One such method is to divide sequences into chunks and perform prefilling\nchunk by chunk. This approach, often referred to as chunked prefilling, processes smaller portions\nof each sequence at a time, allowing the system to better balance the computational load across\nsequences [Agrawal et al., 2023]. By choosing an appropriate chunk size, we can ensure that when\nprefilling and decoding overlap for two sequences, their processing within the same iteration tends\nto take a similar amount of time. As a result, decoding idle time is reduced and overall throughput\nis improved.\nFigure 5.13 shows an illustration of chunked prefilling in a few iterations. In this example, the\nbatch contains two sequences. The whole prefilling process of the first sequence is divided into\nthree prefilling steps, giving rise to the chunks denotedP11, P12 and P13. Each chunk corresponds\nto one iteration and can thus overlap with one decoding step. In this way, during the prefilling of\nthe first sequence, we can perform three decoding steps, rather than only a single decoding step, as\nis the case in standard iteration-level scheduling. As a result, the idle time of the decoding process\nis reduced, and the output tokens can be generated earlier.\nChunked Prefilling improves decoding efficiency by overlapping prefilling and decoding, but\nat the cost of additional memory overhead and scheduling complexity. In standard prefilling, we\nprocess the whole input sequence once, building the KV cache in one go. By contrast, in chunked\nprefilling, each chunk needs a separate forward pass to compute its attention outputs and update\nthe KV cache. As a result, we need to maintain the KV cache of early chunks while processing\nlater chunks. This also compromises the parallelism of completing the prefilling for the entire\nsequence in a single pass. In practice, it is usually possible to balance throughput and latency by\nchoosing an appropriate chunk size.\n(a) Simple Iteration-level Scheduling\nP21\nP11\nD21\nP12\nD22\nP13\nD23\nD11\nD24\nD12\nD25\n···\n···Sequence 2\nSequence 1\nIter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 Iter. 6\nChunk 1 Chunk 2 Chunk 3\nThe second output token\ncan be predicted during\nprefilling for sequence 1.\n(b) Chunked Prefilling\nFig. 5.13: Comparison of simple iteration-based scheduling and chunked prefilling. Pxy denotes the y-th prefilling\nstep for sequence x, and Dxy denotes the y-th decoding step for sequence x. In simple iteration-based scheduling (or\nprefilling-prioritized scheduling), since prefilling is treated as a single iteration, D22 has to wait for the completion of\nthe prefilling of sequence 1. In chunked prefilling, the prefilling process can be divided into multiple steps. Thus, D22\ncan execute during prefilling for sequence 1 (i.e., during P12)"
    },
    {
        "cluster_id": 10,
        "text": "5.3.3 Output Ensembling\nIf we have multiple model outputs, it is often beneficial to combine them to mitigate the impact\nof individual model errors and synthesize a superior final output. Each model might capture\ndifferent aspects of the underlying data distribution or possess unique strengths and weaknesses.\nThe “scaling” from output ensembling comes at the cost of running multiple models or sam-\npling multiple outputs. This not only increases the latency of inference but also leads to the addi-\ntional complexity of managing multiple models. But the quality of outputs does not continue to\nimprove indefinitely as more models are added. In some cases, the benefits of output ensembling\nmay diminish as the number of component models in the ensemble exceeds a certain threshold.\nInstead, the benefits of ensembling are generally greater when the individual models are diverse\n(i.e., they make different errors), even if there are a relatively small number of component models.\nHowever, in addition to scaling up the quality, scaling can mean more. It can also signify scaling\nup the robustness (making the system less prone to errors and more reliable) and exploration (cov-\nering a wider range of potential solutions). In output ensembling, these dimensions are naturally\nintegrated. For instance, the very act of averaging or voting across different model outputs is a\n238 Inference\ndirect strategy to scale up robustness against individual model failures. Furthermore, by intention-\nally including varied models, ensembling increases the chances of discovering novel or superior\nsolutions. In this sense, scaling is not limited to making models larger or running them longer —\nit also means strategies for making inference more robust, exploratory, and adaptive"
    },
    {
        "cluster_id": 42,
        "text": "The search algorithm can efficiently generate a set of candidate solutions\nDc = {y1,..., yK} (5.38)\nThen, we can use a verifier, which evaluates each solution by the function V(y), to score the\ncandidates in Dc. The final output is the best candidate selected by the verifier\nˆy = arg max\ny∈Dc\nV(y) (5.39)\n240 Inference\nAlthough verifying the entire reasoning path is possible, a simpler alternative is to verify only\nthe final reasoning step. In this way the verifier function V(y) is simplified to depend solely on\nthe final answer contained within anr. This can be achieved in various ways, depending on the\nnature of the problem and the expected answer format.\n5.3.4.2 Step-level Search with Verifiers\nWhile the methods discussed above primarily focus on generating complete solutions before final\nselection, the search process can also be integrated more deeply into the step-by-step generation\nof the reasoning path itself. This leads to approaches that perform step-level search with verifiers,\nwhere guidance or pruning occurs at intermediate reasoning steps {a1,...,a nk}rather than only\nafter a full solution y is formed.\n242 Inference\nStep-level search with verifiers can also be modeled as a tree search problem. In this paradigm,\neach node (or state) corresponds to a partial reasoning path, a≤i = (a1,...,a i), representing the\nsequence of i reasoning steps taken so far (i.e., a path from the root node to the current node).\nThe core components of step-level search with verifiers are:\n• Node Representation. A node is a partial reasoning path a≤i = (a1,...,a i). The root node\nis an empty path, and terminal nodes are complete reasoning paths.\n• Verification. The verifier V(·) evaluates the quality of a newly generated step in the context\nof the current partial path a≤i = (a1,...,a i) and the original problem x. As with solution-\nlevel verification, step-level verifiers might output a numerical score, a categorical label,\nand textual feedback.\nThis step-by-step verification allows for dynamic adjustments to the reasoning process. If a\nstep ai+1 is deemed incorrect or unpromising by V(·), the search algorithm can backtrack and\nexplore alternative steps from a≤i, or even from an earlier node a≤i′ (where i′< i). Conversely,\nif a step is highly rated, resources can be focused on extending that path. See Figure 5.17 for an\nillustration of step-level search with verifiers.\nOne example of such a step-level verifier, particularly when using human feedback to assess\nintermediate progress, is the process reward model (PRM). A PRM is typically a separate lan-\nguage model trained to output a scalar reward for each reasoning stepai′within a partial path a≤i"
    },
    {
        "cluster_id": 4,
        "text": "[Akyürek et al., 2023] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.\n[Bach et al., 2022] Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V.\n[Brown et al., 1993] Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey\nWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\n[Burns et al., 2023] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold\nAschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu.\n[Chiang et al., 2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nLianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.\n[Chung et al., 2022] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gau-\nrav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov,\nEd Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n[Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\nlaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.\n[Dai et al., 2023] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei.\n[Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour,\nDJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, and Peter Shaw.\n[Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Luko-\nsiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain,\nDustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau,\nKamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemí Mercado, Nova\nDasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kada-\nvath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom\nHenighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\nSam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan.\n[Gou et al., 2024] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al.\n[Gunasekar et al., 2023] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil\nSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman\nKalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need.arXiv preprint arXiv:2306.11644, 2023.\n[Katharopoulos et al., 2020] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.\n[Liu et al., 2024] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437, 2024a.\n[Liu et al., 2023] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie\nTang. Gpt understands, too. AI Open, 2023b.\n[Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,\nSarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bod-\nhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.\n[Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\n[Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter.\n[Press et al., 2023] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\n[Pryzant et al., 2023] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng.\n[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.\n[Ramachandran et al., 2017] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation\nfunctions. arXiv preprint arXiv:1710.05941, 2017.\n[Schulman et al., 2015] John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel.\n[Schulman et al., 2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n[Sun et al., 2020] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\n[Team et al., 2024] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin,\nSurya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al.\n264 Inference\n[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\nLukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernan-\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Is-\nabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schel-\nten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 ,\n2023b.\n[Wang et al., 2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\n[Xia et al., 2024] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.\n[Yang et al., 2024] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\narXiv:2412.15115, 2024.\n[Yu et al., 2022] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.\n[Zhao et al., 2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen,\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen.\n[Zhou et al., 2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,\nAvia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer\nLevy. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023a"
    },
    {
        "cluster_id": 59,
        "text": "[Alabdulmohsin et al., 2022] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisit-\ning neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:\n22300–22312, 2022.\nUnsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pages 8440–8451, 2020.\n[Gordon et al., 2021] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws\nfor neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5915–5922, 2021.\n[Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine\ntranslation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 86–96, 2016"
    },
    {
        "cluster_id": 14,
        "text": "Advances in Neural Information Processing Systems, 35:11079–11091, 2022.\nNeural computation, 9(8):1735–1780, 1997.\nAdvances in Neural Information Processing Systems, 36, 2024.\nAdvances in Neural Information Processing Systems, 33:3008–3021, 2020"
    },
    {
        "cluster_id": 62,
        "text": "[Dong et al., 2019] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng\nGao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language\nunderstanding and generation. Advances in neural information processing systems, 32, 2019.\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\nDe Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for NLP. In Proceedings of the 36th International Conference on Machine Learning , pages\n2790–2799. PMLR, 2019.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving\nlanguage understanding by generative pre-training. OpenAI, 2018.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In International\nconference on machine learning, pages 8748–8763. PMLR, 2021"
    },
    {
        "cluster_id": 45,
        "text": "[Fan et al., 2019] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand\nwith structured dropout. In Proceedings of International Conference on Learning Representations, 2019.\n[Fedus et al., 2022] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to\ntrillion parameter models with simple and efficient sparsity.The Journal of Machine Learning Research,\n23(1):5232–5270, 2022.\n[Korthikanti et al., 2023] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee,\nMichael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in\nlarge transformer models. Proceedings of Machine Learning and Systems, 5, 2023.\n[V on Oswald et al., 2023] Johannes V on Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento,\nAlexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by\ngradient descent. In Proceedings of International Conference on Machine Learning , pages 35151–\n35174. PMLR, 2023"
    },
    {
        "cluster_id": 1,
        "text": "arXiv preprint arXiv:1902.09574, 2019.\narXiv preprint arXiv:1606.08415, 2016.\narXiv preprint arXiv:2205.12255, 2022.\narXiv preprint arXiv:2305.05920, 2023a"
    },
    {
        "cluster_id": 61,
        "text": "[Joshi et al., 2017] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–\n1611, 2017.\n5.4 Summary 259\n[Min et al., 2019] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop read-\ning comprehension through question decomposition and rescoring. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 6097–6109, 2019.\n[Williams et al., 2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge\ncorpus for sentence understanding through inference. InProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1112–1122, 2018.\n5.4 Summary 267\n[Zellers et al., 2018] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale\nadversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pages 93–104, 2018"
    },
    {
        "cluster_id": 53,
        "text": "[Jurafsky and Martin, 2008] Dan Jurafsky and James H. Martin. Speech and Language Processing (2nd\ned.). Prentice Hall, 2008.\n[Manning, 2022] Christopher D Manning. Human language understanding & reasoning. Daedalus, 151\n(2):127–138, 2022.\nIn Proceedings of the 33rd annual meeting of the association for computational linguistics , pages 189–\n196, 1995"
    },
    {
        "cluster_id": 44,
        "text": "[Kim et al., 2023] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan\nYan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia\nShao, and Amir Gholami. Full stack optimization of transformer inference: a survey. arXiv preprint\narXiv:2302.14017, 2023.\n[Pope et al., 2023] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.\n[Shazeer, 2020] Noam Shazeer. Glu variants improve transformer.arXiv preprint arXiv:2002.05202, 2020.\n[Tay et al., 2020] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A\nsurvey. CoRR, abs/2009.06732, 2020"
    }
]